2016-11-15 13:21:56 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:21:56 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:21:56 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:21:56 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:21:56 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:21:56 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:21:56 [scrapy] INFO: Spider opened
2016-11-15 13:21:56 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:22:00 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:22:00 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 22, 0, 515166),
 'item_scraped_count': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 21, 56, 902065)}
2016-11-15 13:22:00 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:29:12 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:29:12 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:29:12 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:29:12 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:29:12 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:29:12 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:29:12 [scrapy] INFO: Spider opened
2016-11-15 13:29:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:29:12 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:29:12 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 29, 12, 866671),
 'item_scraped_count': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 29, 12, 227550)}
2016-11-15 13:29:12 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:29:48 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:29:48 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:29:49 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:29:49 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:29:49 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:29:49 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:29:49 [scrapy] INFO: Spider opened
2016-11-15 13:29:49 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u63d0\u540d\u4eba\u53ca\u5019\u9009\u4eba\u58f0\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000862',
 'secName': u'\u94f6\u661f\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 11, 45, 2),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300549',
 'secName': u'\u4f18\u5fb7\u7cbe\u5bc6',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 52),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u300a\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u884c\u653f\u8bb8\u53ef\u9879\u76ee\u5ba1\u67e5\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u901a\u77e5\u4e66\u300b\u4e4b\u53cd\u9988\u610f\u89c1\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u56de\u590d',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u82cf\u5dde\u79d1\u73af\u73af\u4fdd\u79d1\u6280\u6709\u9650\u516c\u53f82014\u5e74-2016\u5e749\u6708\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u72ec\u7acb\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u9605\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u676d\u5dde\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u4e4b\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000633',
 'secName': u'*ST\u5408\u91d1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 53),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u4e2d\u4f26\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002655',
 'secName': u'\u5171\u8fbe\u7535\u58f0',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 47),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002262',
 'secName': u'\u6069\u534e\u836f\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 31),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u8fdb\u884c\u80a1\u7968\u8d28\u62bc\u5f0f\u56de\u8d2d\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002458',
 'secName': u'\u76ca\u751f\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 24),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u7ea6\u5b9a\u8d2d\u56de\u5f0f\u8bc1\u5238\u4ea4\u6613\u5230\u671f\u8d2d\u56de\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002471',
 'secName': u'\u4e2d\u8d85\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 9),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d77\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f82015\u5e74\u5ea6\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5174\u4e1a\u56fd\u9645\u4fe1\u6258\u6709\u9650\u516c\u53f8\u5174\u4e1a\u4fe1\u6258-\u4f17\u4fe1\u65c5\u6e381\u53f7\u5458\u5de5\u6301\u80a1\u96c6\u5408\u8d44\u91d1\u4fe1\u6258\u8ba1\u5212\u8d44\u91d1\u4fe1\u6258\u5408\u540c',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002707',
 'secName': u'\u4f17\u4fe1\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 11, 41),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u6c5f\u5929\u518c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u4fe1\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u6301\u7eed\u7763\u5bfc\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002729',
 'secName': u'\u597d\u5229\u6765',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 39),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u96c4\u53bf\u7ecf\u6d4e\u5f00\u53d1\u533a\u96c6\u4e2d\u4f9b\u70ed\u9879\u76ee\u83b7\u5f97\u6838\u51c6\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002616',
 'secName': u'\u957f\u9752\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 32),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300496',
 'secName': u'\u4e2d\u79d1\u521b\u8fbe',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 29),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u9009\u4e3e\u4ea7\u751f\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u83b7\u5f97\u201c\u56fd\u5bb6\u6280\u672f\u521b\u65b0\u793a\u8303\u4f01\u4e1a\u201d\u8ba4\u5b9a\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300066',
 'secName': u'\u4e09\u5ddd\u667a\u6167',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 19),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002528',
 'secName': u'\u82f1\u98de\u62d3',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u4e8b\u9879\u83b7\u5f97\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u6838\u51c6\u6279\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u7684\u4fee\u8ba2\u8bf4\u660e\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u6458\u8981',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\uff08\u4fee\u8ba2\u7a3f\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u8bf4\u660e\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u53d1\u884c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u6295\u8d44\u98ce\u9669\u7279\u522b\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e03\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u56db\u6b21\u4e34\u65f6\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5ef6\u671f\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535A',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5bf9\u5916\u6295\u8d44\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002646',
 'secName': u'\u9752\u9752\u7a1e\u9152',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e94\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5168\u8d44\u5b50\u516c\u53f8\u5b8c\u6210\u5de5\u5546\u53d8\u66f4\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u90e8\u5206\u6fc0\u52b1\u5bf9\u8c61\u540d\u5355',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u5341\u4e94\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5bf9\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u53d1\u8868\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u95ee\u8be2\u51fd\u56de\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002443',
 'secName': u'\u91d1\u6d32\u7ba1\u9053',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u610f\u5411\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u521d\u6b65\u8be2\u4ef7\u53ca\u63a8\u4ecb\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u63d0\u793a\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u56db\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2013\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u5173\u4e8e\u6838\u51c6\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u6279\u590d',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u975e\u7ecf\u5e38\u6027\u635f\u76ca\u7684\u4e13\u9879\u5ba1\u6838\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u56db\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u516d\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e94\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e09\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u610f\u89c1\u4e66\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u63a7\u80a1\u80a1\u4e1c\u53ca\u5b9e\u9645\u63a7\u5236\u4eba\u5bf9\u62db\u80a1\u8bf4\u660e\u4e66\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u8bbe\u7acb\u4ee5\u6765\u80a1\u672c\u6f14\u53d8\u60c5\u51b5\u7684\u8bf4\u660e\u53ca\u5176\u8463\u4e8b\u3001\u76d1\u4e8b\u3001\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5185\u90e8\u63a7\u5236\u9274\u8bc1\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u53f8\u7ae0\u7a0b\uff08\u8349\u6848\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u4ea4\u6240\u95ee\u8be2\u51fd\u30102016\u3011\u7b2c469\u53f7\u7684\u56de\u590d\u8bf4\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u56fd\u67ab\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e2d\u5c0f\u677f\u516c\u53f8\u7ba1\u7406\u90e8\u300a\u5173\u4e8e\u5bf9\u5e7f\u4e1c\u56fd\u76db\u91d1\u63a7\u96c6\u56e2\u80a1\u4efd\u6709\u9650\u516c\u53f8\u7684\u95ee\u8be2\u51fd\u300b\u76f8\u5173\u4e8b\u9879\u7684\u4e13\u9879\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u6743\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000820',
 'secName': u'\u91d1\u57ce\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6743\u76ca\u53d8\u52a8\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002316',
 'secName': u'\u952e\u6865\u901a\u8baf',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u8463\u4e8b\u4f1a\u4e8c\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u8f6c\u8ba9\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u76d1\u4e8b\u4f1a\u5341\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u8f6c\u8ba9\u63a7\u80a1\u5b50\u516c\u53f8\u80a1\u6743\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u548c\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002497',
 'secName': u'\u96c5\u5316\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u534e\u6cf0\u8054\u5408\u8bc1\u5238\u6709\u9650\u8d23\u4efb\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300556',
 'secName': u'\u4e1d\u8def\u89c6\u89c9',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u62402016\u5e74\u534a\u5e74\u62a5\u95ee\u8be2\u51fd\u7684\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u9879\u76ee\u7533\u8bf7\u6062\u590d\u5ba1\u67e5\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8bc9\u8bbc\u4e8b\u9879\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000571',
 'secName': u'\u65b0\u5927\u6d32\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000807',
 'secName': u'\u4e91\u94dd\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u5f00\u53d1\u884c2016\u5e74\u516c\u53f8\u503a\u5238\uff08\u7b2c\u4e00\u671f\uff09\u53d1\u884c\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000883',
 'secName': u'\u6e56\u5317\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000595',
 'secName': u'\u5b9d\u5854\u5b9e\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b80\u5f0f\u6743\u76ca\u53d8\u52a8\u62a5\u544a\u4e66',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u9009\u4e3e\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u4e4b\u6cd5\u5f8b\u610f\u89c1',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66\uff08\u6458\u8981\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u548c\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u7684\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u73af\u7403\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884cA\u80a1\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u53ca\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u4e4b\u4e13\u9879\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u516c\u544a\u4e66\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u957f\u57ce\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u91d1\u675c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u7684\u80a1\u7968\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e0a\u5e02\u4e4b\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7f51\u4e0a\u6447\u53f7\u4e2d\u7b7e\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300567',
 'secName': u'\u7cbe\u6d4b\u7535\u5b50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u201c11\u51ef\u8fea\u503a\u201d2016\u5e74\u503a\u5238\u4ed8\u606f\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000939',
 'secName': u'\u51ef\u8fea\u751f\u6001',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300559',
 'secName': u'\u4f73\u53d1\u5b89\u6cf0',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u5185\u5e55\u77e5\u60c5\u4eba\u4e70\u5356\u516c\u53f8\u80a1\u7968\u60c5\u51b5\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF" }
2016-11-15 13:29:52 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF" }
2016-11-15 13:29:52 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:29:52 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 29, 52, 739421),
 'log_count/ERROR': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 29, 49, 86544)}
2016-11-15 13:29:52 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:30:18 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:30:18 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:30:18 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:30:18 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:30:18 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:30:18 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:30:18 [scrapy] INFO: Spider opened
2016-11-15 13:30:18 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u63d0\u540d\u4eba\u53ca\u5019\u9009\u4eba\u58f0\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000862',
 'secName': u'\u94f6\u661f\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 11, 45, 2),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300549',
 'secName': u'\u4f18\u5fb7\u7cbe\u5bc6',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 52),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u300a\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u884c\u653f\u8bb8\u53ef\u9879\u76ee\u5ba1\u67e5\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u901a\u77e5\u4e66\u300b\u4e4b\u53cd\u9988\u610f\u89c1\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u56de\u590d',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u82cf\u5dde\u79d1\u73af\u73af\u4fdd\u79d1\u6280\u6709\u9650\u516c\u53f82014\u5e74-2016\u5e749\u6708\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u72ec\u7acb\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u9605\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u676d\u5dde\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u4e4b\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000633',
 'secName': u'*ST\u5408\u91d1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 53),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u4e2d\u4f26\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002655',
 'secName': u'\u5171\u8fbe\u7535\u58f0',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 47),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002262',
 'secName': u'\u6069\u534e\u836f\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 31),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u8fdb\u884c\u80a1\u7968\u8d28\u62bc\u5f0f\u56de\u8d2d\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002458',
 'secName': u'\u76ca\u751f\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 24),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u7ea6\u5b9a\u8d2d\u56de\u5f0f\u8bc1\u5238\u4ea4\u6613\u5230\u671f\u8d2d\u56de\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002471',
 'secName': u'\u4e2d\u8d85\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 9),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d77\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f82015\u5e74\u5ea6\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5174\u4e1a\u56fd\u9645\u4fe1\u6258\u6709\u9650\u516c\u53f8\u5174\u4e1a\u4fe1\u6258-\u4f17\u4fe1\u65c5\u6e381\u53f7\u5458\u5de5\u6301\u80a1\u96c6\u5408\u8d44\u91d1\u4fe1\u6258\u8ba1\u5212\u8d44\u91d1\u4fe1\u6258\u5408\u540c',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002707',
 'secName': u'\u4f17\u4fe1\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 11, 41),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u6c5f\u5929\u518c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u4fe1\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u6301\u7eed\u7763\u5bfc\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002729',
 'secName': u'\u597d\u5229\u6765',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 39),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u96c4\u53bf\u7ecf\u6d4e\u5f00\u53d1\u533a\u96c6\u4e2d\u4f9b\u70ed\u9879\u76ee\u83b7\u5f97\u6838\u51c6\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002616',
 'secName': u'\u957f\u9752\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 32),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300496',
 'secName': u'\u4e2d\u79d1\u521b\u8fbe',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 29),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u9009\u4e3e\u4ea7\u751f\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u83b7\u5f97\u201c\u56fd\u5bb6\u6280\u672f\u521b\u65b0\u793a\u8303\u4f01\u4e1a\u201d\u8ba4\u5b9a\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300066',
 'secName': u'\u4e09\u5ddd\u667a\u6167',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 19),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002528',
 'secName': u'\u82f1\u98de\u62d3',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u4e8b\u9879\u83b7\u5f97\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u6838\u51c6\u6279\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u7684\u4fee\u8ba2\u8bf4\u660e\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u6458\u8981',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\uff08\u4fee\u8ba2\u7a3f\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u8bf4\u660e\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u53d1\u884c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u6295\u8d44\u98ce\u9669\u7279\u522b\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e03\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u56db\u6b21\u4e34\u65f6\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5ef6\u671f\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535A',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5bf9\u5916\u6295\u8d44\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002646',
 'secName': u'\u9752\u9752\u7a1e\u9152',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e94\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5168\u8d44\u5b50\u516c\u53f8\u5b8c\u6210\u5de5\u5546\u53d8\u66f4\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u90e8\u5206\u6fc0\u52b1\u5bf9\u8c61\u540d\u5355',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u5341\u4e94\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5bf9\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u53d1\u8868\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u95ee\u8be2\u51fd\u56de\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002443',
 'secName': u'\u91d1\u6d32\u7ba1\u9053',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u610f\u5411\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u521d\u6b65\u8be2\u4ef7\u53ca\u63a8\u4ecb\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u63d0\u793a\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u56db\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'2013\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u5173\u4e8e\u6838\u51c6\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u6279\u590d',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u975e\u7ecf\u5e38\u6027\u635f\u76ca\u7684\u4e13\u9879\u5ba1\u6838\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u56db\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u516d\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e94\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e09\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u610f\u89c1\u4e66\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u63a7\u80a1\u80a1\u4e1c\u53ca\u5b9e\u9645\u63a7\u5236\u4eba\u5bf9\u62db\u80a1\u8bf4\u660e\u4e66\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u8bbe\u7acb\u4ee5\u6765\u80a1\u672c\u6f14\u53d8\u60c5\u51b5\u7684\u8bf4\u660e\u53ca\u5176\u8463\u4e8b\u3001\u76d1\u4e8b\u3001\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5185\u90e8\u63a7\u5236\u9274\u8bc1\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u53f8\u7ae0\u7a0b\uff08\u8349\u6848\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u4ea4\u6240\u95ee\u8be2\u51fd\u30102016\u3011\u7b2c469\u53f7\u7684\u56de\u590d\u8bf4\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u56fd\u67ab\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e2d\u5c0f\u677f\u516c\u53f8\u7ba1\u7406\u90e8\u300a\u5173\u4e8e\u5bf9\u5e7f\u4e1c\u56fd\u76db\u91d1\u63a7\u96c6\u56e2\u80a1\u4efd\u6709\u9650\u516c\u53f8\u7684\u95ee\u8be2\u51fd\u300b\u76f8\u5173\u4e8b\u9879\u7684\u4e13\u9879\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u6743\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000820',
 'secName': u'\u91d1\u57ce\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6743\u76ca\u53d8\u52a8\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002316',
 'secName': u'\u952e\u6865\u901a\u8baf',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u8463\u4e8b\u4f1a\u4e8c\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u8f6c\u8ba9\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF" }
2016-11-15 13:30:18 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u76d1\u4e8b\u4f1a\u5341\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u8f6c\u8ba9\u63a7\u80a1\u5b50\u516c\u53f8\u80a1\u6743\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u548c\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002497',
 'secName': u'\u96c5\u5316\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u534e\u6cf0\u8054\u5408\u8bc1\u5238\u6709\u9650\u8d23\u4efb\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300556',
 'secName': u'\u4e1d\u8def\u89c6\u89c9',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u62402016\u5e74\u534a\u5e74\u62a5\u95ee\u8be2\u51fd\u7684\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u9879\u76ee\u7533\u8bf7\u6062\u590d\u5ba1\u67e5\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8bc9\u8bbc\u4e8b\u9879\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000571',
 'secName': u'\u65b0\u5927\u6d32\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000807',
 'secName': u'\u4e91\u94dd\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u5f00\u53d1\u884c2016\u5e74\u516c\u53f8\u503a\u5238\uff08\u7b2c\u4e00\u671f\uff09\u53d1\u884c\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000883',
 'secName': u'\u6e56\u5317\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000595',
 'secName': u'\u5b9d\u5854\u5b9e\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b80\u5f0f\u6743\u76ca\u53d8\u52a8\u62a5\u544a\u4e66',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u9009\u4e3e\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u4e4b\u6cd5\u5f8b\u610f\u89c1',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66\uff08\u6458\u8981\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u548c\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u7684\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u73af\u7403\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884cA\u80a1\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u53ca\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u4e4b\u4e13\u9879\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u516c\u544a\u4e66\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u957f\u57ce\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u91d1\u675c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u7684\u80a1\u7968\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e0a\u5e02\u4e4b\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7f51\u4e0a\u6447\u53f7\u4e2d\u7b7e\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300567',
 'secName': u'\u7cbe\u6d4b\u7535\u5b50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u201c11\u51ef\u8fea\u503a\u201d2016\u5e74\u503a\u5238\u4ed8\u606f\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000939',
 'secName': u'\u51ef\u8fea\u751f\u6001',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300559',
 'secName': u'\u4f73\u53d1\u5b89\u6cf0',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u5185\u5e55\u77e5\u60c5\u4eba\u4e70\u5356\u516c\u53f8\u80a1\u7968\u60c5\u51b5\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF" }
2016-11-15 13:30:19 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF" }
2016-11-15 13:30:19 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:30:19 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 30, 19, 70414),
 'log_count/ERROR': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 30, 18, 501121)}
2016-11-15 13:30:19 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:31:47 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:31:47 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:31:47 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:31:47 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:31:47 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:31:47 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:31:47 [scrapy] INFO: Spider opened
2016-11-15 13:31:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u63d0\u540d\u4eba\u53ca\u5019\u9009\u4eba\u58f0\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000862',
 'secName': u'\u94f6\u661f\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 11, 45, 2),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300549',
 'secName': u'\u4f18\u5fb7\u7cbe\u5bc6',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 52),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u300a\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u884c\u653f\u8bb8\u53ef\u9879\u76ee\u5ba1\u67e5\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u901a\u77e5\u4e66\u300b\u4e4b\u53cd\u9988\u610f\u89c1\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u56de\u590d',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u82cf\u5dde\u79d1\u73af\u73af\u4fdd\u79d1\u6280\u6709\u9650\u516c\u53f82014\u5e74-2016\u5e749\u6708\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u72ec\u7acb\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u9605\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u676d\u5dde\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u4e4b\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000633',
 'secName': u'*ST\u5408\u91d1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 53),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u4e2d\u4f26\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002655',
 'secName': u'\u5171\u8fbe\u7535\u58f0',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 47),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002262',
 'secName': u'\u6069\u534e\u836f\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 31),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u8fdb\u884c\u80a1\u7968\u8d28\u62bc\u5f0f\u56de\u8d2d\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002458',
 'secName': u'\u76ca\u751f\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 24),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u7ea6\u5b9a\u8d2d\u56de\u5f0f\u8bc1\u5238\u4ea4\u6613\u5230\u671f\u8d2d\u56de\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002471',
 'secName': u'\u4e2d\u8d85\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 9),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d77\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f82015\u5e74\u5ea6\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5174\u4e1a\u56fd\u9645\u4fe1\u6258\u6709\u9650\u516c\u53f8\u5174\u4e1a\u4fe1\u6258-\u4f17\u4fe1\u65c5\u6e381\u53f7\u5458\u5de5\u6301\u80a1\u96c6\u5408\u8d44\u91d1\u4fe1\u6258\u8ba1\u5212\u8d44\u91d1\u4fe1\u6258\u5408\u540c',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002707',
 'secName': u'\u4f17\u4fe1\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 11, 41),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u6c5f\u5929\u518c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u4fe1\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u6301\u7eed\u7763\u5bfc\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002729',
 'secName': u'\u597d\u5229\u6765',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 39),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u96c4\u53bf\u7ecf\u6d4e\u5f00\u53d1\u533a\u96c6\u4e2d\u4f9b\u70ed\u9879\u76ee\u83b7\u5f97\u6838\u51c6\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002616',
 'secName': u'\u957f\u9752\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 32),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300496',
 'secName': u'\u4e2d\u79d1\u521b\u8fbe',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 29),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u9009\u4e3e\u4ea7\u751f\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u83b7\u5f97\u201c\u56fd\u5bb6\u6280\u672f\u521b\u65b0\u793a\u8303\u4f01\u4e1a\u201d\u8ba4\u5b9a\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300066',
 'secName': u'\u4e09\u5ddd\u667a\u6167',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 19),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002528',
 'secName': u'\u82f1\u98de\u62d3',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u4e8b\u9879\u83b7\u5f97\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u6838\u51c6\u6279\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u7684\u4fee\u8ba2\u8bf4\u660e\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u6458\u8981',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\uff08\u4fee\u8ba2\u7a3f\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u8bf4\u660e\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u53d1\u884c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u6295\u8d44\u98ce\u9669\u7279\u522b\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e03\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u56db\u6b21\u4e34\u65f6\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5ef6\u671f\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535A',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5bf9\u5916\u6295\u8d44\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002646',
 'secName': u'\u9752\u9752\u7a1e\u9152',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e94\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5168\u8d44\u5b50\u516c\u53f8\u5b8c\u6210\u5de5\u5546\u53d8\u66f4\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u90e8\u5206\u6fc0\u52b1\u5bf9\u8c61\u540d\u5355',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u5341\u4e94\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5bf9\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u53d1\u8868\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u95ee\u8be2\u51fd\u56de\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002443',
 'secName': u'\u91d1\u6d32\u7ba1\u9053',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u610f\u5411\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u521d\u6b65\u8be2\u4ef7\u53ca\u63a8\u4ecb\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u63d0\u793a\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u56db\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2013\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u5173\u4e8e\u6838\u51c6\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u6279\u590d',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u975e\u7ecf\u5e38\u6027\u635f\u76ca\u7684\u4e13\u9879\u5ba1\u6838\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u56db\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u516d\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e94\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e09\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u610f\u89c1\u4e66\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u63a7\u80a1\u80a1\u4e1c\u53ca\u5b9e\u9645\u63a7\u5236\u4eba\u5bf9\u62db\u80a1\u8bf4\u660e\u4e66\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u8bbe\u7acb\u4ee5\u6765\u80a1\u672c\u6f14\u53d8\u60c5\u51b5\u7684\u8bf4\u660e\u53ca\u5176\u8463\u4e8b\u3001\u76d1\u4e8b\u3001\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5185\u90e8\u63a7\u5236\u9274\u8bc1\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u53f8\u7ae0\u7a0b\uff08\u8349\u6848\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u4ea4\u6240\u95ee\u8be2\u51fd\u30102016\u3011\u7b2c469\u53f7\u7684\u56de\u590d\u8bf4\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u56fd\u67ab\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e2d\u5c0f\u677f\u516c\u53f8\u7ba1\u7406\u90e8\u300a\u5173\u4e8e\u5bf9\u5e7f\u4e1c\u56fd\u76db\u91d1\u63a7\u96c6\u56e2\u80a1\u4efd\u6709\u9650\u516c\u53f8\u7684\u95ee\u8be2\u51fd\u300b\u76f8\u5173\u4e8b\u9879\u7684\u4e13\u9879\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u6743\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000820',
 'secName': u'\u91d1\u57ce\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6743\u76ca\u53d8\u52a8\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002316',
 'secName': u'\u952e\u6865\u901a\u8baf',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u8463\u4e8b\u4f1a\u4e8c\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u8f6c\u8ba9\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u76d1\u4e8b\u4f1a\u5341\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u8f6c\u8ba9\u63a7\u80a1\u5b50\u516c\u53f8\u80a1\u6743\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u548c\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002497',
 'secName': u'\u96c5\u5316\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u534e\u6cf0\u8054\u5408\u8bc1\u5238\u6709\u9650\u8d23\u4efb\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300556',
 'secName': u'\u4e1d\u8def\u89c6\u89c9',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u62402016\u5e74\u534a\u5e74\u62a5\u95ee\u8be2\u51fd\u7684\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u9879\u76ee\u7533\u8bf7\u6062\u590d\u5ba1\u67e5\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8bc9\u8bbc\u4e8b\u9879\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000571',
 'secName': u'\u65b0\u5927\u6d32\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000807',
 'secName': u'\u4e91\u94dd\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u5f00\u53d1\u884c2016\u5e74\u516c\u53f8\u503a\u5238\uff08\u7b2c\u4e00\u671f\uff09\u53d1\u884c\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000883',
 'secName': u'\u6e56\u5317\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000595',
 'secName': u'\u5b9d\u5854\u5b9e\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b80\u5f0f\u6743\u76ca\u53d8\u52a8\u62a5\u544a\u4e66',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u9009\u4e3e\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u4e4b\u6cd5\u5f8b\u610f\u89c1',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66\uff08\u6458\u8981\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u548c\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u7684\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u73af\u7403\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884cA\u80a1\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u53ca\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u4e4b\u4e13\u9879\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u516c\u544a\u4e66\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u957f\u57ce\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u91d1\u675c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u7684\u80a1\u7968\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e0a\u5e02\u4e4b\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7f51\u4e0a\u6447\u53f7\u4e2d\u7b7e\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300567',
 'secName': u'\u7cbe\u6d4b\u7535\u5b50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u201c11\u51ef\u8fea\u503a\u201d2016\u5e74\u503a\u5238\u4ed8\u606f\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000939',
 'secName': u'\u51ef\u8fea\u751f\u6001',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300559',
 'secName': u'\u4f73\u53d1\u5b89\u6cf0',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u5185\u5e55\u77e5\u60c5\u4eba\u4e70\u5356\u516c\u53f8\u80a1\u7968\u60c5\u51b5\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF" }
2016-11-15 13:31:48 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF" }
2016-11-15 13:31:48 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:31:48 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 31, 48, 410913),
 'log_count/ERROR': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 31, 47, 752673)}
2016-11-15 13:31:48 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:32:55 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:32:55 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:32:55 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:32:55 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:32:55 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:32:55 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:32:55 [scrapy] INFO: Spider opened
2016-11-15 13:32:55 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:32:58 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:32:58 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 507,
 'downloader/request_count': 1,
 'downloader/request_method_count/POST': 1,
 'downloader/response_bytes': 84757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 32, 58, 874333),
 'item_scraped_count': 140,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2016, 11, 15, 5, 32, 55, 255817)}
2016-11-15 13:32:58 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:34:26 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:34:26 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:34:26 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:34:26 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:34:26 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:34:26 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:34:26 [scrapy] INFO: Spider opened
2016-11-15 13:34:26 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u63d0\u540d\u4eba\u53ca\u5019\u9009\u4eba\u58f0\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000862',
 'secName': u'\u94f6\u661f\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 11, 45, 2),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829492.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300549',
 'secName': u'\u4f18\u5fb7\u7cbe\u5bc6',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 52),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829472.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u300a\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u884c\u653f\u8bb8\u53ef\u9879\u76ee\u5ba1\u67e5\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u901a\u77e5\u4e66\u300b\u4e4b\u53cd\u9988\u610f\u89c1\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829493.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u56de\u590d',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829498.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u82cf\u5dde\u79d1\u73af\u73af\u4fdd\u79d1\u6280\u6709\u9650\u516c\u53f82014\u5e74-2016\u5e749\u6708\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 15),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829494.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u7533\u8bf7\u6587\u4ef6\u4e8c\u6b21\u53cd\u9988\u610f\u89c1\u4e4b\u72ec\u7acb\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829497.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u9605\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829495.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u676d\u5dde\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u4e4b\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000925',
 'secName': u'\u4f17\u5408\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 42, 16),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829496.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000633',
 'secName': u'*ST\u5408\u91d1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 53),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829491.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u4e2d\u4f26\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002655',
 'secName': u'\u5171\u8fbe\u7535\u58f0',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 47),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829486.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829490.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829489.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e00\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002681',
 'secName': u'\u594b\u8fbe\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 40),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829488.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002262',
 'secName': u'\u6069\u534e\u836f\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 31),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829487.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u8fdb\u884c\u80a1\u7968\u8d28\u62bc\u5f0f\u56de\u8d2d\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002458',
 'secName': u'\u76ca\u751f\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 24),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829485.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u7ea6\u5b9a\u8d2d\u56de\u5f0f\u8bc1\u5238\u4ea4\u6613\u5230\u671f\u8d2d\u56de\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002471',
 'secName': u'\u4e2d\u8d85\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 9),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829484.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d77\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f82015\u5e74\u5ea6\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829479.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002479',
 'secName': u'\u5bcc\u6625\u73af\u4fdd',
 'time': datetime.datetime(2016, 11, 15, 11, 41, 3),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829478.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5174\u4e1a\u56fd\u9645\u4fe1\u6258\u6709\u9650\u516c\u53f8\u5174\u4e1a\u4fe1\u6258-\u4f17\u4fe1\u65c5\u6e381\u53f7\u5458\u5de5\u6301\u80a1\u96c6\u5408\u8d44\u91d1\u4fe1\u6258\u8ba1\u5212\u8d44\u91d1\u4fe1\u6258\u5408\u540c',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002707',
 'secName': u'\u4f17\u4fe1\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 11, 41),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829483.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829480.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u6d59\u6c5f\u5929\u518c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829482.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u4fe1\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u623f\u5730\u4ea7\u4e1a\u52a1\u4e4b\u4e13\u9879\u6838\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002244',
 'secName': u'\u6ee8\u6c5f\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 51),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829481.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u6301\u7eed\u7763\u5bfc\u5b9a\u671f\u73b0\u573a\u68c0\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002729',
 'secName': u'\u597d\u5229\u6765',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 39),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829477.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u96c4\u53bf\u7ecf\u6d4e\u5f00\u53d1\u533a\u96c6\u4e2d\u4f9b\u70ed\u9879\u76ee\u83b7\u5f97\u6838\u51c6\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002616',
 'secName': u'\u957f\u9752\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 32),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829476.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300496',
 'secName': u'\u4e2d\u79d1\u521b\u8fbe',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 29),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829475.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829473.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u9009\u4e3e\u4ea7\u751f\u7b2c\u4e09\u5c4a\u76d1\u4e8b\u4f1a\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300501',
 'secName': u'\u6d77\u987a\u65b0\u6750',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 20),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829474.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u83b7\u5f97\u201c\u56fd\u5bb6\u6280\u672f\u521b\u65b0\u793a\u8303\u4f01\u4e1a\u201d\u8ba4\u5b9a\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'300066',
 'secName': u'\u4e09\u5ddd\u667a\u6167',
 'time': datetime.datetime(2016, 11, 15, 11, 40, 19),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829471.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002528',
 'secName': u'\u82f1\u98de\u62d3',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829014.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u4e8b\u9879\u83b7\u5f97\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u6838\u51c6\u6279\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829003.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u7684\u4fee\u8ba2\u8bf4\u660e\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829006.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\u6458\u8981',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829005.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u53d1\u884c\u80a1\u4efd\u53ca\u652f\u4ed8\u73b0\u91d1\u8d2d\u4e70\u8d44\u4ea7\u5e76\u52df\u96c6\u914d\u5957\u8d44\u91d1\u66a8\u5173\u8054\u4ea4\u6613\u62a5\u544a\u4e66\uff08\u4fee\u8ba2\u7a3f\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'300331',
 'secName': u'\u82cf\u5927\u7ef4\u683c',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829004.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829001.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002492',
 'secName': u'\u6052\u57fa\u8fbe\u946b',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202829002.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u8bf4\u660e\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828979.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u53d1\u884c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828977.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u6295\u8d44\u98ce\u9669\u7279\u522b\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300569',
 'secName': u'\u5929\u80fd\u91cd\u5de5',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828978.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e03\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u56db\u6b21\u4e34\u65f6\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828912.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5ef6\u671f\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000037',
 'secName': u'*ST\u5357\u7535A',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828911.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5bf9\u5916\u6295\u8d44\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002646',
 'secName': u'\u9752\u9752\u7a1e\u9152',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828910.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e94\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828895.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828898.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u5168\u8d44\u5b50\u516c\u53f8\u5b8c\u6210\u5de5\u5546\u53d8\u66f4\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828894.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828896.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u90e8\u5206\u6fc0\u52b1\u5bf9\u8c61\u540d\u5355',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828900.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u56db\u5c4a\u76d1\u4e8b\u4f1a\u7b2c\u5341\u4e94\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828897.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u5411\u6fc0\u52b1\u5bf9\u8c61\u6388\u4e882015\u5e74\u7b2c\u4e8c\u671f\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u9884\u7559\u9650\u5236\u6027\u80a1\u7968\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828901.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5bf9\u7b2c\u56db\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u516d\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u53d1\u8868\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002334',
 'secName': u'\u82f1\u5a01\u817e',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828899.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u95ee\u8be2\u51fd\u56de\u590d\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002443',
 'secName': u'\u91d1\u6d32\u7ba1\u9053',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828875.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u62db\u80a1\u610f\u5411\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828848.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u521d\u6b65\u8be2\u4ef7\u53ca\u63a8\u4ecb\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828818.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u63d0\u793a\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828817.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u56db\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828846.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828845.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e00\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828844.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828843.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e8c\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828842.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828841.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e00\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u5341\u4e8c\u6b21\u4f1a\u8bae\u51b3\u8bae',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828840.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828823.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2015\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828822.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828821.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2014\u5e74\u7b2c\u4e00\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828820.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2013\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828819.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e2d\u56fd\u8bc1\u76d1\u4f1a\u5173\u4e8e\u6838\u51c6\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u6279\u590d',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828816.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u975e\u7ecf\u5e38\u6027\u635f\u76ca\u7684\u4e13\u9879\u5ba1\u6838\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828847.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828839.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u62db\u5546\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u53d1\u884c\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828838.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u56db\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828836.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u516d\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828835.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e94\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828834.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e8c\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828833.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u6cd5\u5f8b\u610f\u89c1\u4e66\uff08\u4e09\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828832.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u610f\u89c1\u4e66\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828831.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u8865\u5145\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a\uff08\u4e00\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828830.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828829.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56fd\u6d69\u5f8b\u5e08\uff08\u6df1\u5733\uff09\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u4eba\u6c11\u5e01\u666e\u901a\u80a1\uff08A\u80a1\uff09\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u5f8b\u5e08\u5de5\u4f5c\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828828.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u63a7\u80a1\u80a1\u4e1c\u53ca\u5b9e\u9645\u63a7\u5236\u4eba\u5bf9\u62db\u80a1\u8bf4\u660e\u4e66\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828827.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u516c\u53f8\u8bbe\u7acb\u4ee5\u6765\u80a1\u672c\u6f14\u53d8\u60c5\u51b5\u7684\u8bf4\u660e\u53ca\u5176\u8463\u4e8b\u3001\u76d1\u4e8b\u3001\u9ad8\u7ea7\u7ba1\u7406\u4eba\u5458\u7684\u786e\u8ba4\u610f\u89c1',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828826.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5185\u90e8\u63a7\u5236\u9274\u8bc1\u62a5\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828825.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u53f8\u7ae0\u7a0b\uff08\u8349\u6848\uff09',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828824.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'300570',
 'secName': u'\u592a\u8fb0\u5149',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828837.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u4ea4\u6240\u95ee\u8be2\u51fd\u30102016\u3011\u7b2c469\u53f7\u7684\u56de\u590d\u8bf4\u660e',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828810.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u56fd\u67ab\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e2d\u5c0f\u677f\u516c\u53f8\u7ba1\u7406\u90e8\u300a\u5173\u4e8e\u5bf9\u5e7f\u4e1c\u56fd\u76db\u91d1\u63a7\u96c6\u56e2\u80a1\u4efd\u6709\u9650\u516c\u53f8\u7684\u95ee\u8be2\u51fd\u300b\u76f8\u5173\u4e8b\u9879\u7684\u4e13\u9879\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002670',
 'secName': u'\u56fd\u76db\u91d1\u63a7',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828809.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u6743\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000820',
 'secName': u'\u91d1\u57ce\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828799.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6743\u76ca\u53d8\u52a8\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002316',
 'secName': u'\u952e\u6865\u901a\u8baf',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828733.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u8463\u4e8b\u4f1a\u4e8c\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828678.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u53ec\u5f002016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u901a\u77e5',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828680.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828687.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u8f6c\u8ba9\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828686.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u5b50\u516c\u53f8\u8f6c\u8ba9\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f8100%\u80a1\u6743\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828685.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e5d\u5c4a\u76d1\u4e8b\u4f1a\u5341\u4e5d\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u76d1\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828679.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u4fe1\u5229\u6c47\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828684.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u62d3\u5174\u6770\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828683.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u56db\u5ddd\u534e\u4ea8\u6570\u7801\u901a\u8baf\u6709\u9650\u516c\u53f82016\u5e748\u670831\u65e5\u5ba1\u8ba1\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828682.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u8f6c\u8ba9\u63a7\u80a1\u5b50\u516c\u53f8\u80a1\u6743\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'000584',
 'secName': u'\u53cb\u5229\u63a7\u80a1',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828681.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u63a7\u80a1\u80a1\u4e1c\u90e8\u5206\u80a1\u4efd\u89e3\u9664\u8d28\u62bc\u548c\u8d28\u62bc\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002497',
 'secName': u'\u96c5\u5316\u96c6\u56e2',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828676.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u8463\u4e8b\u4f1a\u516c\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828645.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828647.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u534e\u6cf0\u8054\u5408\u8bc1\u5238\u6709\u9650\u8d23\u4efb\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u4e0e\u4e13\u4e1a\u673a\u6784\u5408\u4f5c\u6295\u8d44\u66a8\u5173\u8054\u4ea4\u6613\u7684\u6838\u67e5\u610f\u89c1',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828648.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u72ec\u7acb\u8463\u4e8b\u5173\u4e8e\u7b2c\u4e09\u5c4a\u8463\u4e8b\u4f1a\u7b2c\u4e09\u5341\u4e03\u6b21\u4f1a\u8bae\u76f8\u5173\u4e8b\u9879\u7684\u72ec\u7acb\u610f\u89c1',
 'announcementTypeName': u'\u4e0a\u5e02\u516c\u53f8\u5236\u5ea6',
 'secCode': u'002373',
 'secName': u'\u5343\u65b9\u79d1\u6280',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828646.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300556',
 'secName': u'\u4e1d\u8def\u89c6\u89c9',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828644.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828638.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8d2d\u4e70\u7406\u8d22\u4ea7\u54c1\u7684\u516c\u544a',
 'announcementTypeName': u'\u4ea4\u6613',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828637.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u62402016\u5e74\u534a\u5e74\u62a5\u95ee\u8be2\u51fd\u7684\u56de\u590d\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002336',
 'secName': u'*ST\u4eba\u4e50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202827541.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828634.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u9879\u76ee\u7533\u8bf7\u6062\u590d\u5ba1\u67e5\u7684\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828636.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u8bc9\u8bbc\u4e8b\u9879\u7684\u8fdb\u5c55\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828633.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e03\u6b21\uff08\u4e34\u65f6\uff09\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002240',
 'secName': u'\u5a01\u534e\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828635.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u505c\u724c\u8fdb\u5c55\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'000571',
 'secName': u'\u65b0\u5927\u6d32\uff21',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828572.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000807',
 'secName': u'\u4e91\u94dd\u80a1\u4efd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828571.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u516c\u5f00\u53d1\u884c2016\u5e74\u516c\u53f8\u503a\u5238\uff08\u7b2c\u4e00\u671f\uff09\u53d1\u884c\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000883',
 'secName': u'\u6e56\u5317\u80fd\u6e90',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828563.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828562.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u56db\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000918',
 'secName': u'\u5609\u51ef\u57ce',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828561.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u7684\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'000595',
 'secName': u'\u5b9d\u5854\u5b9e\u4e1a',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828558.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828557.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u516d\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'000967',
 'secName': u'\u76c8\u5cf0\u73af\u5883',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828556.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301\u516c\u53f8\u80a1\u4efd\u7684\u516c\u544a',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828549.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u7b80\u5f0f\u6743\u76ca\u53d8\u52a8\u62a5\u544a\u4e66',
 'announcementTypeName': u'\u80a1\u6743\u53d8\u52a8',
 'secCode': u'002097',
 'secName': u'\u5c71\u6cb3\u667a\u80fd',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828550.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828522.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e\u9009\u4e3e\u804c\u5de5\u4ee3\u8868\u76d1\u4e8b\u7684\u516c\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828521.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u4e4b\u6cd5\u5f8b\u610f\u89c1',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002033',
 'secName': u'\u4e3d\u6c5f\u65c5\u6e38',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828523.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66\uff08\u6458\u8981\uff09',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828519.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u60c5\u51b5\u62a5\u544a\u4e66\u66a8\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u589e\u53d1',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828520.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u548c\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u7684\u62a5\u544a',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828517.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u73af\u7403\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884cA\u80a1\u80a1\u7968\u53d1\u884c\u8fc7\u7a0b\u53ca\u8ba4\u8d2d\u5bf9\u8c61\u5408\u89c4\u6027\u4e4b\u4e13\u9879\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828516.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u4e1c\u5434\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u7684\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u4e2d\u4ecb\u673a\u6784\u62a5\u544a',
 'secCode': u'002150',
 'secName': u'\u901a\u6da6\u88c5\u5907',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828518.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e0a\u5e02\u516c\u544a\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828467.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u516c\u544a\u4e66\u63d0\u793a\u6027\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828468.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u957f\u57ce\u8bc1\u5238\u80a1\u4efd\u6709\u9650\u516c\u53f8\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u4e4b\u4e0a\u5e02\u4fdd\u8350\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828469.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5317\u4eac\u5e02\u91d1\u675c\u5f8b\u5e08\u4e8b\u52a1\u6240\u5173\u4e8e\u516c\u53f8\u9996\u6b21\u516c\u5f00\u53d1\u884c\u7684\u80a1\u7968\u4e8e\u6df1\u5733\u8bc1\u5238\u4ea4\u6613\u6240\u4e0a\u5e02\u4e4b\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300562',
 'secName': u'\u4e50\u5fc3\u533b\u7597',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828466.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u80a1\u7968\u5e76\u5728\u521b\u4e1a\u677f\u4e0a\u5e02\u7f51\u4e0a\u6447\u53f7\u4e2d\u7b7e\u7ed3\u679c\u516c\u544a',
 'announcementTypeName': u'\u9996\u6b21\u516c\u5f00\u53d1\u884c\u53ca\u4e0a\u5e02',
 'secCode': u'300567',
 'secName': u'\u7cbe\u6d4b\u7535\u5b50',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828465.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u201c11\u51ef\u8fea\u503a\u201d2016\u5e74\u503a\u5238\u4ed8\u606f\u516c\u544a',
 'announcementTypeName': u'\u503a\u5238\u516c\u544a',
 'secCode': u'000939',
 'secName': u'\u51ef\u8fea\u751f\u6001',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828464.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u80a1\u7968\u4ea4\u6613\u5f02\u5e38\u6ce2\u52a8\u516c\u544a',
 'announcementTypeName': u'\u6f84\u6e05\u3001\u98ce\u9669\u63d0\u793a\u3001\u4e1a\u7ee9\u9884\u544a\u4e8b\u9879',
 'secCode': u'300559',
 'secName': u'\u4f73\u53d1\u5b89\u6cf0',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828463.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828458.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'\u5173\u4e8e2016\u5e74\u9650\u5236\u6027\u80a1\u7968\u6fc0\u52b1\u8ba1\u5212\u5185\u5e55\u77e5\u60c5\u4eba\u4e70\u5356\u516c\u53f8\u80a1\u7968\u60c5\u51b5\u81ea\u67e5\u62a5\u544a',
 'announcementTypeName': u'\u5176\u5b83\u91cd\u5927\u4e8b\u9879',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828457.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e8c\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002195',
 'secName': u'\u4e8c\u4e09\u56db\u4e94',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828459.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u51b3\u8bae\u516c\u544a',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828455.PDF" }
2016-11-15 13:34:27 [scrapy] ERROR: Error processing {'announcementTitle': u'2016\u5e74\u7b2c\u4e09\u6b21\u4e34\u65f6\u80a1\u4e1c\u5927\u4f1a\u7684\u6cd5\u5f8b\u610f\u89c1\u4e66',
 'announcementTypeName': u'\u80a1\u4e1c\u5927\u4f1a',
 'secCode': u'002678',
 'secName': u'\u73e0\u6c5f\u94a2\u7434',
 'time': datetime.datetime(2016, 11, 15, 0, 0),
 'url': u'http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: announcements.cninfo index: url_1 dup key: { : "http://www.cninfo.com.cn/finalpage/2016-11-15/1202828456.PDF" }
2016-11-15 13:34:31 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:34:31 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 5539,
 'downloader/request_count': 9,
 'downloader/request_method_count/POST': 9,
 'downloader/response_bytes': 607192,
 'downloader/response_count': 9,
 'downloader/response_status_count/200': 9,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 34, 31, 336887),
 'item_scraped_count': 868,
 'log_count/ERROR': 140,
 'log_count/INFO': 7,
 'request_depth_max': 8,
 'response_received_count': 9,
 'scheduler/dequeued': 9,
 'scheduler/dequeued/memory': 9,
 'scheduler/enqueued': 9,
 'scheduler/enqueued/memory': 9,
 'start_time': datetime.datetime(2016, 11, 15, 5, 34, 26, 297590)}
2016-11-15 13:34:31 [scrapy] INFO: Spider closed (finished)
2016-11-15 13:51:14 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:51:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:51:14 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:51:14 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:51:14 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:51:14 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:51:14 [scrapy] INFO: Spider opened
2016-11-15 13:51:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:51:41 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-15 13:51:41 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-15 13:51:41 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-15 13:51:41 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-15 13:51:41 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-15 13:51:41 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-15 13:51:41 [scrapy] INFO: Spider opened
2016-11-15 13:51:41 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-15 13:51:43 [scrapy] INFO: Closing spider (finished)
2016-11-15 13:51:43 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3009,
 'downloader/request_count': 5,
 'downloader/request_method_count/POST': 5,
 'downloader/response_bytes': 386728,
 'downloader/response_count': 5,
 'downloader/response_status_count/200': 5,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 15, 5, 51, 43, 412208),
 'item_scraped_count': 636,
 'log_count/INFO': 7,
 'request_depth_max': 4,
 'response_received_count': 5,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'start_time': datetime.datetime(2016, 11, 15, 5, 51, 41, 225908)}
2016-11-15 13:51:43 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:20:07 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:20:07 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:20:07 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:20:07 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:20:07 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:20:07 [twisted] CRITICAL: Unhandled error in Deferred:
2016-11-21 14:20:07 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 1260, in _inlineCallbacks
    result = g.send(result)
  File "/Library/Python/2.7/site-packages/scrapy/crawler.py", line 90, in crawl
    six.reraise(*exc_info)
  File "/Library/Python/2.7/site-packages/scrapy/crawler.py", line 72, in crawl
    self.engine = self._create_engine()
  File "/Library/Python/2.7/site-packages/scrapy/crawler.py", line 97, in _create_engine
    return ExecutionEngine(self, lambda _: self.stop())
  File "/Library/Python/2.7/site-packages/scrapy/core/engine.py", line 69, in __init__
    self.scraper = Scraper(crawler)
  File "/Library/Python/2.7/site-packages/scrapy/core/scraper.py", line 71, in __init__
    self.itemproc = itemproc_cls.from_crawler(crawler)
  File "/Library/Python/2.7/site-packages/scrapy/middleware.py", line 58, in from_crawler
    return cls.from_settings(crawler.settings, crawler)
  File "/Library/Python/2.7/site-packages/scrapy/middleware.py", line 34, in from_settings
    mwcls = load_object(clspath)
  File "/Library/Python/2.7/site-packages/scrapy/utils/misc.py", line 49, in load_object
    raise NameError("Module '%s' doesn't define any object named '%s'" % (module, name))
NameError: Module 'news.pipelines' doesn't define any object named 'MongoDBPipeline'
2016-11-21 14:20:58 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:20:58 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:20:58 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:20:58 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:20:58 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:20:58 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:20:58 [scrapy] INFO: Spider opened
2016-11-21 14:20:58 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:21:02 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 25, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:21:02 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:21:02 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 11580,
 'downloader/request_count': 40,
 'downloader/request_method_count/GET': 40,
 'downloader/response_bytes': 382518,
 'downloader/response_count': 40,
 'downloader/response_status_count/200': 40,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 21, 2, 693948),
 'item_scraped_count': 38,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 40,
 'scheduler/dequeued': 40,
 'scheduler/dequeued/memory': 40,
 'scheduler/enqueued': 40,
 'scheduler/enqueued/memory': 40,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 6, 20, 58, 716059)}
2016-11-21 14:21:02 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:37:30 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:37:30 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'BOT_NAME': 'news'}
2016-11-21 14:37:30 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:37:30 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:37:30 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:37:31 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:37:31 [scrapy] INFO: Spider opened
2016-11-21 14:37:31 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:37:31 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_5.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_3.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_6.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_9.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_8.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_4.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_2.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Redirecting (302) to <GET http://www.cs.com.cn/404error/index.html> from <GET http://www.cs.com.cn/ssgs/gsxw/index_10.html>
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/404error/index.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_7.html> (referer: None)
2016-11-21 14:37:34 [scrapy] DEBUG: Filtered duplicate request: <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:51',
 'title': u'\u795e\u601d\u7535\u5b50\u80a1\u4e1c\u51cf\u6301330\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:54',
 'title': u'\u56fd\u796f\u73af\u4fdd\u4e2d\u68072.6\u4ebf\u5143\u8499\u57ce\u6c61\u6c34\u5382\u7f51\u4e00\u4f53\u5316PPP\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:43',
 'title': u'\u6d77\u8fbe\u80a1\u4efd\u80a1\u4e1c\u51cf\u6301390\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:56',
 'title': u'\u6052\u6cf0\u827e\u666e\u62df\u6295\u8d448000\u4e07\u5143\u8bbe\u7acb\u5317\u4eac\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:48',
 'title': u'\u534e\u529b\u521b\u901a\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301590\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:39',
 'title': u'\u4e1c\u963f\u963f\u80f6\u4e09\u5927\u4e3b\u5bfc\u4ea7\u54c1\u5168\u7ebf\u63d0\u4ef7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:56',
 'title': u'\u6d77\u8054\u8baf\u7b2c\u4e8c\u5927\u80a1\u4e1c\u51cf\u6301500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:41',
 'title': u'11\u670817\u65e5\u5348\u95f4\u516c\u544a:\u5317\u5df4\u4f20\u5a92\u3001\u4f17\u4e1a\u8fbe\u7ec8\u6b62\u53c2\u4e0e\u683c\u529b\u7535\u5668\u5b9a\u589e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:54',
 'title': u'\u529b\u6e90\u4fe1\u606f\u83b7\u5f97\u653f\u5e9c\u8865\u52a9900\u4e07\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:48',
 'title': u'\u534e\u8679\u8ba1\u901a\u80a1\u4e1c\u8ba1\u5212\u51cf\u6301\u4e0d\u8d85\u8fc7168\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:19',
 'title': u'80\u540e\u8463\u4e8b\u957f\u5927\u9605\u5175\uff1a38\u4eba\u9760\u5bb6\u65cf\u4f20\u627f 4\u4eba\u767d\u624b\u8d77\u5bb6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:55',
 'title': u'\u96f7\u66fc\u80a1\u4efd\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301600\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:01',
 'title': u'\u76d1\u7ba1\u90e8\u95e8\u6478\u5e95\u623f\u4f01\u53c2\u80a1\u5730\u65b9\u94f6\u884c \u623f\u4f01\u878d\u8d44\u538b\u529b\u589e\u5927',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:01',
 'title': u'\u7279\u9510\u5fb7\u5168\u8d44\u5b50\u516c\u53f8\u7b7e\u8ba23.52\u4ebf\u5143EPC\u603b\u627f\u5305\u5408\u540c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:31',
 'title': u'\u91d1\u5cad\u77ff\u4e1a\u62df\u7ec8\u6b62\u7b79\u5212\u8d2d\u4e70\u8d44\u4ea7 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:07',
 'title': u'\u5357\u73bbA\u539f\u9ad8\u7ba1\u4e0e\u524d\u6d77\u4eba\u5bff\u7ea0\u7eb7\u518d\u8d77\u6ce2\u6f9c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:04',
 'title': u'11\u670818\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1\uff1a\u6c49\u9f0e\u5b87\u4f51\u62df13\u4ebf\u5143\u5e76\u8d2d\u6e38\u620f\u516c\u53f8 \u5e03\u5c40\u5a31\u4e50\u4ea7\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:00',
 'title': u'\u56db\u65b9\u7cbe\u521b\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301150\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:23',
 'title': u'\u6df1\u4ea4\u6240\u5411\u5357\u73bbA\u518d\u53d1\u5173\u6ce8\u51fd \u5357\u73bbA\u8463\u79d8\u53ca\u4e24\u72ec\u7acb\u8463\u4e8b\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 17:04',
 'title': u'11\u670817\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:31',
 'title': u'\u683c\u529b\u7535\u5668\u62df\u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:47',
 'title': u'\u4e1c\u963f\u963f\u80f6\u4e09\u5927\u4e3b\u5bfc\u4ea7\u54c1\u63d0\u4ef7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:35',
 'title': u'11\u670817\u65e5\u589e\u51cf\u6301\uff1a\u4ebf\u6676\u5149\u7535\u906d\u63a7\u80a1\u80a1\u4e1c\u51cf\u63011500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:34',
 'title': u'11\u670817\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1\uff1a\u4e2d\u7535\u5e7f\u901a\u62df\u5265\u79bb\u4f20\u7edf\u4e1a\u52a1 \u7f6e\u5165\u519b\u5de5\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:24',
 'title': u'\u683c\u529b\u7535\u5668\u7ec8\u6b62\u53d1\u884c\u80a1\u4efd\u6536\u8d2d\u73e0\u6d77\u94f6\u9686',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:34',
 'title': u'11\u670817\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a\u683c\u529b\u7535\u5668\u62df\u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:27',
 'title': u'\u673a\u5668\u4eba1.2\u4ebf\u80a1\u9650\u552e\u80a1\u89e3\u7981 1\u4ebf\u80a121\u65e5\u53ef\u4e0a\u5e02\u6d41\u901a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:26',
 'title': u'\u592a\u6781\u5b9e\u4e1a\u63a7\u80a1\u5b50\u516c\u53f8\u4e2d\u6807\u91d1\u989d\u903e10.5\u4ebf\u5143\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4ebf\u6676\u5149\u7535\u906d\u63a7\u80a1\u80a1\u4e1c\u51cf\u63011500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e91\u5357\u767d\u836f\uff1a\u96c6\u56e2\u6df7\u6539\u65b0\u5b9e\u65bd\u8def\u5f84\u5df2\u5f97\u5230\u5404\u65b9\u8ba4\u53ef',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:42',
 'title': u'\u7eff\u53f6\u96c6\u56e2\u5218\u6bbf\u6ce2\u56de\u5e94\uff1a\u4e50\u89c6\u6709\u96be \u4e2a\u4eba\u62c9\u5144\u5f1f\u4e00\u628a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u56fd\u52a8\u529b\u80a1\u4e1c\u589e\u6301\u8ba1\u5212\u5b9e\u65bd\u5b8c\u6bd5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:45',
 'title': u'\u5357\u73bbA\u56e0\u7ba1\u7406\u5c42\u52a8\u8361\u518d\u6536\u6df1\u4ea4\u6240\u5173\u6ce8\u51fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u9c81\u4ebf\u901a\u80a1\u4e1c\u9646\u91d1\u6d77\u51cf\u63012.35%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:34 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e7f\u751f\u5802\u63a7\u80a1\u80a1\u4e1c\u589e\u6301\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html'}
2016-11-21 14:37:34 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u6d77\u53d1\u5c55\u66f4\u540d\u201c\u4e2d\u8fdc\u6d77\u80fd\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e0c\u52aa\u5c14\u7ec8\u6b62\u7b79\u5212\u91cd\u5927\u4e8b\u9879 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:42',
 'title': u'\u5357\u73bb\u8463\u79d8\u53ca\u603b\u4f1a\u8ba1\u5e08\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e1c\u6770\u667a\u80fd\u80a1\u4e1c\u5883\u754c\u6295\u8d44\u51cf\u63011.42%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u8fbd\u5b81\u6210\u5927\u62df4.2\u4ebf\u5143\u51fa\u552e\u5bb6\u4e50\u798f\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u9c81\u6297\u533b\u836f\u505c\u724c\u7b79\u5212\u975e\u516c\u5f00\u53d1\u884c\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u7eff\u53f6\u5236\u836f\u6f84\u6e05 \u5e76\u65e0\u4e0e\u4e50\u89c6\u8ba2\u7acb\u4efb\u4f55\u6295\u8d44\u5b89\u6392',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u7535\u5e7f\u901a\u62df\u5265\u79bb\u4f20\u7edf\u4e1a\u52a1 \u7f6e\u5165\u519b\u5de5\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u897f\u85cf\u836f\u4e1a\u5b9a\u589e\u7533\u8bf7\u83b7\u8bc1\u76d1\u4f1a\u5ba1\u6838\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:47',
 'title': u'\u6842\u53d1\u7965\u3001\u51ef\u83b1\u82f118\u65e5\u4e2d\u5c0f\u677f\u4e0a\u5e02',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:28',
 'title': u'\u79d1\u529b\u8fdc\uff1a\u62df\u51fa\u8d4414\u4ebf\u5143\u5171\u540c\u8bbe\u7acb\u6df7\u5408\u52a8\u529b\u6280\u672f\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:01',
 'title': u'\u5929\u80fd\u91cd\u5de5\u4e2d\u7b7e\u73870.02613%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u82f1\u98de\u62d3\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u6bc5\u8fbe\u56e0\u6d89\u5acc\u4fe1\u62ab\u8fdd\u6cd5\u8fdd\u89c4\u906d\u7acb\u6848\u8c03\u67e5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u84dd\u5149\u53d1\u5c55\u906d\u80a1\u4e1c\u51cf\u63012300\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:45',
 'title': u'\u8fbd\u5b81\u6210\u5927\uff1a\u62df4.2\u4ebf\u5143\u51fa\u552e\u5bb6\u4e50\u798f\u5168\u90e8\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e7f\u7530\u96c6\u56e2\u505c\u724c\u7b79\u5212\u6536\u8d2d\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:01',
 'title': u'\u534e\u9e4f\u98de\u80a1\u4e1c\u51cf\u63011100\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:47',
 'title': u'\u4eba\u4e8b\u53d8\u52a8\u7ee7\u7eed \u5357\u73bbA\u8463\u79d8\u53ca\u4e24\u72ec\u7acb\u8463\u4e8b\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:08',
 'title': u'\u5eb7\u5f97\u65b0\uff1a\u62df\u63a8\u603b\u89c4\u6a216\u4ebf\u5143\u5458\u5de5\u6301\u80a1\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:09',
 'title': u'\u524d\u6d77\u4eba\u5bff\u5357\u73bbA\u8f9e\u804c\u9ad8\u7ba1\u5404\u6267\u4e00\u8bcd \u5b9d\u80fd\u7cfb\u9762\u4e34\u7ba1\u7406\u6311\u6218',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:10',
 'title': u'\u5230\u5e95\u662f\u8c01"\u73bb\u7483\u5fc3"\uff1f\u5357\u73bbA\u98ce\u6ce2\u591a\u65b9\u5404\u6267\u4e00\u8bcd\u6ce2\u6f9c\u4e0d\u65ad',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:30',
 'title': u'\u4e07\u79d1\u80a1\u6743\u4e89\u593a\u6218\u6108\u6f14\u6108\u70c8\uff1a\u6052\u5927\u7ee7\u7eed\u589e\u6301 \u4e2d\u7b56\u5bcc\u6c47\u201c\u62cd\u9a6c\u6740\u5165\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:32',
 'title': u'\u4e0a\u5e02\u516c\u53f8\u5e76\u8d2d\u95ef\u5173\u5b58\u4e09\u5927\u6740\u5668 \u63a7\u5236\u6743\u6210\u5173\u6ce8\u65b0\u7126\u70b9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:37',
 'title': u'\u5149\u660e\u5730\u4ea7\uff1a\u62df10\u4ebf\u5143\u8f6c\u8ba9\u6240\u6301\u90e8\u5206\u7269\u4e1a\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:31',
 'title': u'\u76d1\u7ba1\u201c\u96f7\u9706\u51fa\u51fb\u201d\u623f\u5730\u4ea7\u4fe1\u6258\u4e1a\u52a1 \u4fe1\u6258\u516c\u53f8\u4e25\u9635\u4ee5\u5f85',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:10',
 'title': u'\u5b9d\u80fd\u7cfb\u5168\u9762\u63a5\u7ba1\u5357\u73bb \u76d1\u7ba1\u90e8\u95e8\u8fde\u53d1\u5173\u6ce8\u51fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:00',
 'title': u'\u683c\u529b\u7535\u5668\uff1a\u7ec8\u6b62\u7b79\u5212\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u4e8b\u5b9c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:32',
 'title': u'\u5b89\u90a6\u51fa\u624b\uff01113\u4ebf\u5143\u4e3e\u724c\u4e2d\u56fd\u5efa\u7b51\u56fe\u4e2a\u5565\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:43',
 'title': u'\u6b4c\u534e\u6709\u7ebf\u4e09\u5b63\u5ea6\u4e1a\u7ee9\u8868\u73b0\u62a2\u773c \u65b0\u5a92\u4f53\u8f6c\u578b\u6548\u679c\u663e\u73b0',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:44',
 'title': u'28\u4ebf\u9a70\u63f4\u8d5b\u7ef4\u7834\u4ea7\u91cd\u6574 \u6613\u6210\u65b0\u80fd\u8bd5\u6c34\u53e6\u7c7b\u201c\u503a\u8f6c\u80a1\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:42',
 'title': u'\u529b\u5e06\u80a1\u4efd\u62df1.7\u4ebf\u5143\u5265\u79bb\u5b89\u8bda\u4fdd\u9669\u80a1\u4efd \u6536\u7f29\u91d1\u878d\u201c\u8f93\u8840\u201d\u65b0\u80fd\u6e90',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:47',
 'title': u'\u201c\u95e8\u53e3\u7684\u91ce\u86ee\u4eba\u201d\u518d\u73b0 \u6da8\u505c\u7684\u5357\u73bbA\u80cc\u540e\u8fd8\u6709\u8fd9\u4e9b\u65e0\u4e3b\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:43',
 'title': u'\u91cd\u6574\u8ba1\u5212\u83b7\u901a\u8fc7 *ST\u4e91\u7ef4\u4fdd\u58f3\u9700\u626d\u8f6c\u8fd110\u4ebf\u5de8\u4e8f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:56',
 'title': u'\u906d\u9047\u5f3a\u52bf\u5165\u4e3b\u201c\u540e\u9057\u75c7\u201d \u5916\u6765\u8d44\u672c\u5982\u4f55\u6597\u800c\u4e0d\u7834',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:57',
 'title': u'\u6d77\u6da6\u5149\u4f0f\u4ea7\u4e1a\u57fa\u91d1\u8ba1\u5212\u88ab\u95ee\u8be2 \u671d\u4ee4\u5915\u6539\u4e14\u8bbe\u4e0d\u5e73\u7b49\u6761\u7ea6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:57',
 'title': u'\u9996\u6279\u5e74\u62a5\u201c\u9884\u9001\u8f6c\u201d\u65b9\u6848\u51fa\u7089',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 16:26',
 'title': u'11\u670818\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a\uff1a\u4e07\u79d1A\uff1a\u6536\u8d2d\u524d\u6d77\u56fd\u9645\u65b9\u6848\u5c1a\u672a\u8fbe\u6210\u5171\u8bc6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:47',
 'title': u'A\u80a1\u201c\u58f3\u201d\u751f\u610f\u98ce\u751f\u6c34\u8d77 \u5404\u8def\u8d44\u672c\u7eb7\u5230\u201c\u7897\u91cc\u6765\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:48',
 'title': u'11\u5bb6\u516c\u53f8\u8054\u624b\u7b79\u5efa\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:04',
 'title': u'\u4e2d\u56fd\u6052\u5927\u589e\u6301\u4e07\u79d1\u81f39.45% \u4e07\u79d1\u91cd\u7ec4\u65b9\u6848\u5c1a\u672a\u5c18\u57c3\u843d\u5b9a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:45',
 'title': u'\u957f\u57ce\u52a8\u6f2b\u51fa\u552e\u5723\u8fbe\u7126\u5316 \u62d6\u6b20\u8d27\u6b3e\u5ba2\u6237\u5fb7\u80dc\u96c6\u56e28\u6298\u63a5\u76d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:02',
 'title': u'\u6052\u5927\u80fd\u633d\u56de\u201c\u77ed\u7092\u201d\u4e8b\u4ef6\u4e2d\u53d7\u635f\u7684\u58f0\u8a89\u5417\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:56',
 'title': u'\u80a1\u5e02\u8d5a\u94b1\u6548\u5e94\u663e\u73b0 \u5458\u5de5\u6301\u80a1\u8ba1\u5212\u5bc6\u96c6\u5efa\u4ed3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:49',
 'title': u'\u534e\u661f\u521b\u4e1a\u7ec8\u6b62\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5b87\u901a\u5ba2\u8f66\u56de\u5e94\u4e0b\u8dcc\u539f\u56e0\uff1a\u6216\u56e0\u65b0\u80fd\u6e90\u6c7d\u8f66\u8865\u8d34\u53d6\u6d88\u4f20\u95fb',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:26',
 'title': u'\u4e0a\u6d77\u94f6\u884c\u4e0a\u5e02\u7b2c\u4e09\u5929\u5373\u6253\u5f00\u6da8\u505c\u677f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:51',
 'title': u'11\u670818\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u4e07\u79d1A\u5927\u6da86% \u518d\u521b\u5386\u53f2\u65b0\u9ad8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:57',
 'title': u'\u4e0d\u6b62\u5357\u73bbA \u8fd9\u4e9b\u516c\u53f8\u540c\u6837\u7206\u53d1\u4eba\u4e8b\u5730\u9707\uff01(\u9644\u540d\u5355)',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5fb7\u8d5b\u7535\u6c60\u900f\u9732\u516c\u53f8\u4e3a\u534e\u4e3aMate9\u63d0\u4f9b\u7535\u6c60\u4ea7\u54c1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u5409\u827e\u79d1\u6280\u7ec8\u6b62\u6536\u8d2d\u54c8\u8428\u514b\u65af\u5766\u70bc\u5316\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5eb7\u5f97\u65b0\uff1a\u4e0e\u534e\u4e3a\u7ec8\u6b62\u88f8\u773c3D\u4e1a\u52a1\u5408\u4f5c\u4f20\u95fb\u4e0d\u5b9e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:23',
 'title': u'11\u670818\u65e5\u5348\u95f4\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:22',
 'title': u'\u6842\u53d1\u7965\u4eca\u65e5\u4e0a\u5e02',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u8fce\u5b89\u90a6\u4e3e\u724c \u4e2d\u56fd\u5efa\u7b51\u4eca\u65e5\u9ad8\u5f00\u4f4e\u8d70',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:23',
 'title': u'\u817e\u90a6\u65c5\u6e38\u96c6\u56e2\u4e1a\u52a1\u5c06\u5411\u76ee\u7684\u5730\u5ef6\u4f38',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u6c49\u738b\u79d1\u6280\u62df\u63a8\u80a1\u6743\u6fc0\u52b1\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u666e\u90a6\u80a1\u4efd\u4e2d\u6807\u90d1\u5ddePPP\u9879\u76ee \u6709\u671b\u63d0\u632f\u5341\u5e74\u4e1a\u7ee9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:21',
 'title': u'\u6df1\u6e2f\u901a\u4e34\u8fd1 \u673a\u6784\u5bc6\u96c6\u8c03\u7814\u4e2d\u5c0f\u521b\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5c71\u4e1c\u534e\u9e4f\u91cd\u5927\u4e8b\u9879\u4e34\u505c \u9ad8\u9001\u8f6c\u662f\u8bef\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:14',
 'title': u'\u5ddd\u73af\u79d1\u6280\u5b9e\u63a7\u4eba\u610f\u89c1\u201c\u53cd\u590d\u201d\uff1f \u848b\u9752\u6625\u8463\u4e8b\u804c\u4f4d\u201c\u5f97\u800c\u590d\u5931\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html'}
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:35 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:49',
 'title': u'\u4fe1\u62ab\u4e25\u91cd\u5931\u5b9e*ST\u5de5\u65b0\u906d\u4e0a\u4ea4\u6240\u8c34\u8d23',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:48',
 'title': u'\u4e0a\u6d77\u94f6\u884c\u4eca\u65e5\u4e0a\u5e02 \u6279\u91cf\u9020\u5bcc\u8fd16000\u5458\u5de5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:24',
 'title': u'\u201c\u5927\u91d1\u4e3b\u201d\u63a8\u6ce2\u52a9\u6f9c\u58f3\u4f30\u503c\u534a\u5e74\u7ffb\u756a\uff1a100\u4ebf\u5143\u6210\u6807\u914d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u9c7c\u8dc3\u533b\u7597\u62df8.63\u4ebf\u5165\u4e3b\u4e2d\u4f18\u533b\u836f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:14',
 'title': u'\u5265\u79bb\u71c3\u6c14\u5177\u8d44\u4ea7\u81f4\u4e3b\u4e1a\u201c\u60ac\u7a7a\u201d \u4e07\u5bb6\u4e50\u91cd\u7ec4\u906d\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:08',
 'title': u'\u94a2\u6784\u5de5\u7a0b\u5265\u79bb\u4f4e\u6548\u4e1a\u52a1 \u62df\u51fa\u552e\u4e24\u5bb6\u5b50\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:58',
 'title': u'\u4e0a\u4ea4\u6240\u76d1\u7ba1\u95ee\u8be2\u51fb\u8981\u5bb3 \u6052\u5927\u7cfb\u660e\u786e\u4e3e\u724c\u6885\u96c1\u5409\u7965\u610f\u56fe',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:12',
 'title': u'\u8d3e\u8dc3\u4ead\u662f\u600e\u6837\u62ff\u4e0b\u957f\u6c5f\u5546\u5b66\u9662\u540c\u5b66\u76846\u4ebf\u6295\u8d44\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:29',
 'title': u'\u73c8\u4f1f\u80a1\u4efd\u53d1\u5e03\u56fa\u6001\u9502\u7535\u6c60\u4ea7\u54c1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u5feb\u4e50\u65f6\u4ee3\u4f30\u503c\u6210\u8c1c \u56fd\u76db\u91d1\u63a7\u6ea2\u4ef7\u6536\u8d2d5%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u4e0a\u6d77\u56fd\u8d44\u6539\u9769\u5927\u5c3a\u5ea6\u7a81\u7834 \u88c5\u5165\u4f18\u8d28\u8d44\u4ea7\u76d8\u6d3b\u201c\u58f3\u201d\u8d44\u6e90',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u5929\u76ee\u836f\u4e1a\u91cd\u7ec4\u516d\u8fde\u8d25 \u201c\u957f\u57ce\u7cfb\u201d\u8fdb\u9000\u7ef4\u8c37',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:16',
 'title': u'\u4eba\u6c11\u5e01\u51fb\u7a7f\u201c\u94c1\u5e95\u201d \u4e00\u5927\u6ce2\u4e0a\u5e02\u516c\u53f8\u635f\u5931\u8fc7\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 18:46',
 'title': u'\u4e50\u89c6\u63a7\u80a1\u64a4\u6362\u4e9a\u592a\u533a\u603b\u88c1 \u9999\u6e2f\u516c\u53f8\u88ab\u4f20\u4e24\u4e2a\u6708\u524d\u5df2\u5f00\u59cb\u88c1\u5458',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u623f\u4f01\u878d\u8d44\u5168\u7ebf\u6536\u7d27\uff1a\u53ea\u8981\u8fd8\u6ca1\u653e\u6b3e\u7684\u90fd\u505c\u4e0b\u6765',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:53',
 'title': u'11\u670816\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:17',
 'title': u'\u5357\u73bbA\u7ba1\u7406\u5c42\u52a8\u8361\u5f15\u53d1\u6df1\u4ea4\u6240\u5173\u6ce8 \u524d\u6d77\u4eba\u5bff\u7d27\u6025\u58f0\u660e\uff1a\u4ece\u672a\u5e72\u6d89\u65e5\u5e38\u7ecf\u8425',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:00',
 'title': u'\u5174\u6e90\u73af\u5883\u7b7e\u8ba2\u4f9b\u6392\u6c34\u5de5\u7a0bPPP\u9879\u76eePPP\u534f\u8bae',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:01',
 'title': u'\u534e\u5e73\u80a1\u4efd\u6301\u80a15%\u4ee5\u4e0a\u7684\u80a1\u4e1c\u51cf\u6301673.5\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u534e\u8c0a\u96c6\u56e2\u56fd\u6709\u80a1\u4efd\u65e0\u507f\u5212\u8f6c\u5b8c\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:01',
 'title': u'\u65b0\u5b81\u7269\u6d41\u63a7\u80a1\u80a1\u4e1c\u53ca\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u5206\u522b\u51cf\u6301300\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u4f20\u5316\u80a1\u4efd\u8bc1\u5238\u7b80\u79f0\u53d8\u66f4\u4e3a\u201c\u4f20\u5316\u667a\u8054\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 18:50',
 'title': u'\u5b9d\u5854\u5b9e\u4e1a\uff1a\u4ea4\u6613\u5f02\u5e38\u906d\u6df1\u4ea4\u6240\u95ee\u8be2 \u8981\u6c42\u8bf4\u660e\u662f\u5426\u5b58\u5728\u5185\u5e55\u4ea4\u6613',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u79d1\u8fbe\u80a1\u4efd16\u65e5\u590d\u724c \u91cd\u7ec4\u83b7\u8bc1\u76d1\u4f1a\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u4e1c\u65b9\u521b\u4e1a\u53c2\u80a1\u516c\u53f8IPO\u83b7\u6279',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'*ST\u73e0\u6c5f\u91cd\u7ec4\u83b7\u5317\u4eac\u56fd\u8d44\u59d4\u6279\u590d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u6df1\u8d5b\u683c16\u65e5\u590d\u724c \u91cd\u7ec4\u83b7\u8bc1\u76d1\u4f1a\u6709\u6761\u4ef6\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5eb7\u8010\u7279\u5b50\u516c\u53f8\u4e2a\u522b\u4eba\u5458\u6d89\u5acc\u804c\u52a1\u4fb5\u5360',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5367\u9f99\u5730\u4ea7\u62df\u7ec8\u6b62\u8de8\u754c\u6536\u8d2d\u6e38\u620f\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:15',
 'title': u'\u9646\u5bb6\u5634\u56e0\u4fe1\u606f\u62ab\u9732\u906d\u4e0a\u4ea4\u6240\u95ee\u8be2 \u51fa\u552e\u8d44\u4ea7\u6d89\u5acc\u4e0d\u5f53\u5173\u8054\u4ea4\u6613',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5929\u5eb7\u751f\u7269\u80a1\u4e1c\u4e2d\u65b0\u5efa\u62db\u5546\u62ab\u9732\u51cf\u6301\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u677e\u53d1\u80a1\u4efd\u63a7\u80a1\u80a1\u4e1c\u589e\u6301\u8ba1\u5212\u5b9e\u65bd\u5b8c\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u6d77\u9646\u91cd\u5de5\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u63015%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u9ec4\u5c71\u65c5\u6e38\u53c2\u80a1\u516c\u53f8IPO\u83b7\u6279',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5229\u4e9a\u5fb7\u5458\u5de5\u6301\u80a1\u8ba1\u5212\u4e70\u51651.1%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u79d1\u6797\u73af\u4fdd\u80a1\u6743\u8f6c\u8ba9\u5b8c\u6210 \u4e1c\u8bda\u745e\u4e1a\u6210\u5927\u80a1\u4e1c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:21',
 'title': u'\u5b81\u6ce2\u5bcc\u90a6\u8c03\u6574\u91cd\u7ec4\u65b9\u6848\u3000\u575a\u5b9a\u63a8\u8fdb\u6218\u7565\u8f6c\u578b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u9ad8\u4f1f\u8fbe\u80a1\u4e1c\u94f6\u8054\u79d1\u6280\u62ab\u9732\u51cf\u6301\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:40',
 'title': u'\u4e1c\u9633\u5149\u79d1\u56e0\u5173\u6d89\u91cd\u5927\u4e8b\u987916 \u65e5\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:15',
 'title': u'\u6c38\u6cf0\u80fd\u6e90\uff1a\u5408\u8d44\u5b50\u516c\u53f8\u62df\u5411\u5357\u9633\u7535\u5382\u589e\u8d4410\u4ebf\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:42',
 'title': u'\u4e2d\u822a\u4e09\u946b\u80a1\u4e1c\u97e9\u5e73\u5143\u51cf\u63011.87%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:18',
 'title': u'\u6c47\u91d1\u79d1\u628011\u670817\u65e5\u767b\u9646\u521b\u4e1a\u677f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:07',
 'title': u'\u5b81\u6ce2\u97f5\u5347\uff1a\u62df\u51fa\u8d443200\u4e07\u5143\u5171\u540c\u8bbe\u7acb\u97f5\u5347\u7535\u5b50\u516c\u53f8 \u6301\u80a140%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 21:18',
 'title': u'\u73c8\u4f1f\u80a1\u4efd\u53d1\u5e03\u5168\u7403\u9996\u4f8b\u56fa\u6001\u9502\u7535\u6c60\u4e0e\u5feb\u5145\u9502\u7535\u6c60',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:57',
 'title': u'\u5e74\u7ec8\u5957\u73b0"\u6697\u6d41\u6d8c" \u6076\u610f\u51cf\u6301"\u5957\u8def\u6df1"',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:50',
 'title': u'\u5357\u73bbA\u4e03\u540d\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:41',
 'title': u'\u4e2d\u56fd\u94c1\u5efa\u53ca\u5b50\u516c\u53f8\u7ec4\u8054\u5408\u4f53\u4e2d\u6807\u6295\u8d44\u989d\u7ea6229.19 \u4ebf\u5143PPP \u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:57',
 'title': u'\u9c7c\u8dc3\u533b\u7597\u62df8.6\u4ebf\u5143\u6536\u8d2d\u533b\u7528\u6d88\u6bd2\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:51',
 'title': u'\u4e1c\u822a\u6837\u672c\uff1a\u56fd\u8d44\u63a7\u5236\u4e0b\u591a\u4e3e\u63a8\u8fdb\u6df7\u6539',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:04',
 'title': u'\u5de5\u5927\u9ad8\u65b0\u53ca\u76f8\u5173\u9ad8\u7ba1\u88ab\u516c\u5f00\u8c34\u8d23',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:04',
 'title': u'\u7696\u6c5f\u7269\u6d41\uff1a\u5168\u8d44\u5b50\u516c\u53f8\u903e5\u4ebf\u5143\u6536\u8d2d\u63a7\u80a1\u80a1\u4e1c\u7535\u5382\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:53',
 'title': u'\u4e50\u89c6\u5c06\u83b7\u201c\u597d\u540c\u5b66\u201d6\u4ebf\u7f8e\u5143\u6295\u8d44',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:01',
 'title': u'\u6559\u80b2\u4ea7\u4e1a\u8d44\u6e90\u6574\u5408\u52a0\u901f\uff08\u9644\u80a1\uff09',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:05',
 'title': u'\u5ef6\u671f\u62ab\u9732\u91cd\u5927\u8bc9\u8bbc\u4e8b\u9879 \u5339\u51f8\u5339\u88ab\u4e0a\u4ea4\u6240\u8b66\u793a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:09',
 'title': u'\u6052\u5927\u9ad8\u4f4d\u63a5\u76d8\u4e07\u79d1\u7384\u673a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:16',
 'title': u'\u5357\u73bb7\u540d\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c \u5b9d\u80fd\u7cfb\u79f0\u5e76\u672a\u5e72\u6d89\u516c\u53f8\u7ecf\u8425',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:02',
 'title': u'\u6052\u5927\u7cfb\u79f0\u6218\u7565\u6295\u8d44\u6885\u96c1\u5409\u7965',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:14',
 'title': u'IPO\u53d1\u884c\u63d0\u901f \u5f71\u5b50\u80a1\u6709\u671b\u53d7\u76ca',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:06',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c\u7f57\u751f\u95e8\uff1a\u544a\u522b\u4fe1\u8be6\u8ff0\u7f18\u7531 \u5b9d\u80fd\u7cfb\u4e00\u4e00\u5426\u8ba4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html'}
2016-11-21 14:37:36 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:36 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:04',
 'title': u'\u683c\u529b\u7535\u5668\u6536\u8d2d\u73e0\u6d77\u94f6\u9686\u7ec8\u6b62',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:15',
 'title': u'\u4e24\u516c\u53f8\u6f84\u6e05\u65e0\u6295\u8d44\u4e50\u89c6\u8ba1\u5212 \u7cfb\u8463\u4e8b\u957f\u4e2a\u4eba\u6295\u8d44',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:40',
 'title': u'463\u5bb6\u516c\u53f8\u4e34\u8fd1\u88ab\u4e3e\u724c \u89e3\u5bc6\u4e09\u884c\u4e1a74\u53ea\u4e3e\u724c\u6f5c\u529b\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:26',
 'title': u'\u817e\u4fe1\u80a1\u4efd\u53ca\u5b9e\u63a7\u4eba\u6d89\u5acc\u5411\u6700\u9ad8\u9662\u539f\u526f\u9662\u957f\u4e4b\u5b50\u884c\u8d3f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:29',
 'title': u'\u201c\u5b9d\u80fd\u7cfb\u201d\u5bf9\u51b3\u5357\u73bb\u7ba1\u7406\u5c42 \u4e09\u5927\u7591\u70b9\u5f85\u89e3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:26',
 'title': u'\u83ab\u9ad8\u80a1\u4efd\u906d\u4e09\u5ea6\u4e3e\u724c \u63a7\u80a1\u80a1\u4e1c\u7b79\u5212\u5b9a\u589e\u53cd\u51fb\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:40',
 'title': u'\u4e0a\u6d77\u56fd\u4f01\u6539\u9769\u518d\u6380\u9ad8\u6f6e \u673a\u6784\u9884\u8ba16\u53ea\u9f99\u5934\u80a1\u76ee\u6807\u6da8\u5e45\u8d8550%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:28',
 'title': u'\u5357\u73bbA\u539f\u9ad8\u7ba1\u8bb2\u8ff0\u96c6\u4f53\u79bb\u804c\u539f\u56e0\uff1a\u524d\u6d77\u4eba\u5bff5\u4efd\u8bae\u6848\u4e5f\u662f\u5bfc\u706b\u7d22',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:45',
 'title': u'\u52df\u6295\u9879\u76ee\u8fdb\u5c55\u4e0d\u53ca\u9884\u671f \u5305\u94a2\u80a1\u4efd\u906d\u4e0a\u4ea4\u6240\u4e8c\u6b21\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:47',
 'title': u'\u77f3\u69b4\u96c6\u56e2\u81ea\u66dd\u589e\u6301\u52a8\u5411 \u9996\u94a2\u80a1\u4efd\u80a1\u4ef7\u5e94\u58f0\u800c\u6da8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'\u5468\u9e3f\u794e\u8c08360\u56de\u5f52\u56de\u5f52:\u786e\u4e0e\u56fd\u5bb6\u7f51\u7edc\u5b89\u5168\u6218\u7565\u6709\u5173',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'\u5927\u8fde\u4e0a\u5e02\u516c\u53f8\u91cd\u7ec4\u201c\u751f\u610f\u7ecf\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:51',
 'title': u'\u8bc4\u8bba:\u5357\u73bb\u4e8b\u4ef6\u6298\u5c04\u8d44\u672c\u4e0e\u804c\u4e1a\u7ecf\u7406\u4eba\u5171\u5904\u56f0\u5883',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u4e2d\u7535\u5e7f\u901a\u5265\u79bb\u539f\u4e1a\u52a1 \u63fd\u5165\u519b\u5de5\u7535\u5b50\u6807\u7684',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u79bb\u804c\u80cc\u540e \u6838\u5fc3\u6280\u672f\u53bbor\u7559',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u8001\u5458\u5de5\u8f9e\u804c\u98ce\u6ce2\u8d8a\u95f9\u8d8a\u5927 \u5357\u73bb\u5b9d\u80fd\u5404\u6267\u4e00\u8bcd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bb8\u9ad8\u7ba1\u8f9e\u804c \u524d\u6d77\u4eba\u5bff\uff1a\u80a1\u6743\u6fc0\u52b1\u53ea\u662f\u501f\u53e3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'PPP\u6982\u5ff5\u201c\u62db\u8702\u5f15\u8776\u201d \u673a\u6784\u7784\u4e0a\u5efa\u7b51\u88c5\u9970\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u4ee3\u7406\u8463\u4e8b\u957f\u9648\u7433\u9996\u53d1\u58f0\uff1a\u8981\u4fdd\u62a4\u597d\u5168\u4f53\u80a1\u4e1c\u6743\u76ca',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u4e09\u6cf0\u63a7\u80a1\u5b9a\u589e\u4ef7\u5012\u6302 \u89e3\u7981\u80a1\u4e1c\u201c\u5e72\u77aa\u773c\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bb\u5bab\u6597\u771f\u76f8\u53e3\u6c34\u6218\u4e2d\u6108\u53d1\u6a21\u7cca \u5f53\u4e8b\u53cc\u65b9\u5404\u6267\u4e00\u8bcd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u7f8e\u56fd\u4e50\u89c6\u6c7d\u8f66\u56de\u5e94\u505c\u4ea7:\u9879\u76ee\u4ece\u672a\u505c\u5de5 \u8d44\u91d1\u5c06\u5230\u4f4d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:54',
 'title': u'\u91d1\u660e\u7cbe\u673a\u6301\u7eed\u63a8\u8fdb\u201c\u667a\u80fd\u5236\u9020+\u5927\u5065\u5eb7\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u518d\u6536\u6df1\u4ea4\u6240\u5173\u6ce8\u51fd \u8981\u6c42\u5176\u5bf9\u5a92\u4f53\u62a5\u9053\u53ca\u6295\u8bc9\u4e3e\u62a5\u4e88\u4ee5\u6838\u67e5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u6668\u9e23\u7eb8\u4e1a\u8fde\u7eed\u591a\u6b21\u63d0\u4ef7 \u201c\u6d1b\u9633\u7eb8\u8d35\u201d\u535a\u5f08\u4eba\u6c11\u5e01\u8d2c\u503c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 11:01',
 'title': u'\u5357\u73bbA\u79bb\u804c\u9ad8\u7ba1\u79f0\u96c6\u4f53\u8df3\u69fd\u81f3\u65d7\u6ee8\u96c6\u56e2\u662f\u8c23\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u4e2d\u6c47\u5f71\u89c6\u9690\u7792\u5bf9\u8d4c \u4e09\u4e03\u4e92\u5a31\u65e7\u6807\u7684\u518d\u9047\u632b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u5317\u5df4\u4f20\u5a92\u3001\u4f17\u4e1a\u8fbe\u7ec8\u6b62\u53c2\u4e0e\u683c\u529b\u7535\u5668\u5b9a\u589e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u6885\u96c1\u5409\u7965\u5982\u4f55\u6f14\u7ece\u201c\u540e\u6052\u5927\u65f6\u4ee3\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u6d59\u5bcc\u63a7\u80a1\u4e2d\u68071.3\u4ebf\u5143\u8001\u631d\u6c34\u7535\u7ad9\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u5b9c\u660c\u4ea4\u8fd0\u5b50\u516c\u53f8\u4e24\u5904\u623f\u5c4b\u5c06\u88ab\u5f81\u6536',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 10:12',
 'title': u'11\u670817\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:23',
 'title': u'\u683c\u529b\u8463\u660e\u73e0\u9020\u8f66\u201c\u68a6\u788e\u201d \u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c\uff1a\u4e07\u79d1\u5267\u60c5\u7684\u9884\u6f14?',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u5357\u73bbA17\u65e5\u518d\u906d\u6df1\u4ea4\u6240\u95ee\u8be2 \u8981\u6c42\u8bf4\u660e\u524d\u6d77\u4eba\u5bff\u662f\u5426\u5e72\u9884\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:18',
 'title': u'\u201c\u5bfc\u706b\u7d22\u201d\u5357\u73bbA\u80a1\u6743\u8ba1\u5212\u7ec6\u8282\u62ab\u9732 6.2\u5143\u6fc0\u52b1\u4ef7\u4ec5\u4e3a\u5f53\u65f6\u5e02\u4ef7\u4e00\u534a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u5929\u9f99\u96c6\u56e2\uff1a2016\u5e74\u5ea6\u62df10\u8f6c15\u6d3e0.5 \u591a\u4f4d\u80a1\u4e1c\u62df\u5de8\u91cf\u51cf\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u4e50\u89c6\u9ad8\u5c42\u56de\u5e94\u201c\u5e9e\u6c0f\u9a97\u5c40\u8bf4\u201d \uff1a\u6b63\u5168\u529b\u4ee5\u8d74\u628a\u94b1\u8f6c\u5230\u7f8e\u56fd\u6c7d\u8f66\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:25',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u79bb\u804c\u5185\u5e55\uff1a\u59da\u632f\u534e\u6ca1\u63a5\u524d\u8463\u4e8b\u957f\u66fe\u5357\u7535\u8bdd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:58',
 'title': u'\u82cf\u5b81\u6613\u8d2d\u201c\u7b11\u503e\u57ce\u201d\u63ed\u5e55',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:57',
 'title': u'\u90d1\u7164\u673a\u56e0\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u4e8b\u9879\u5c06\u4e0a\u4f1a17\u65e5\u5f00\u5e02\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u8d44\u672c\u4e0e\u5b9e\u4e1a\u7684\u77db\u4e0e\u76fe\uff1a\u5357\u73bbA\u5f00\u542f\u524d\u6d77\u4eba\u5bff\u65f6\u4ee3 \u8d70\u5411\u5f15\u5173\u6ce8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:45',
 'title': u'\u6b65\u68ee\u80a1\u4efd\u7ec8\u6b62\u670d\u88c5\u52df\u6295\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html'}
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u5357\u73bbA\u98ce\u6ce2\uff1a\u4ece\u8001\u724c\u5408\u8d44\u4f01\u4e1a\u5230\u8d44\u672c\u730e\u7269\u4e4b\u65c5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:06',
 'title': u'\u4e2d\u901a\u56fd\u810911\u670818\u65e5\u7f51\u4e0a\u7533\u8d2d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u63a2\u8bbf\u5357\u73bbA\u6210\u90fd\u5206\u516c\u53f8\uff1a\u8463\u4e8b\u957f\u6302\u9774\u603b\u7ecf\u7406\u4ecd\u5728\u5c97',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u534e\u95fb\u4f20\u5a92\u7b4910\u5bb6\u516c\u53f8\u6d89\u80a1\u6743\u8f6c\u8ba9 3\u4ebf\u6e38\u8d44\u8ffd\u63676\u53ea\u6982\u5ff5\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u5411\u65e5\u8475\u6536\u8d2d\u5965\u80fd\u7535\u6e90\u589e\u503c\u8d8510\u500d\u906d\u95ee\u8be2 \u79f0\u5145\u7535\u6869\u524d\u666f\u5e7f\u9614',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u903e\u4e03\u6210\u4e0a\u5e02\u516c\u53f8\u5927\u80a1\u4e1c\u6301\u80a1\u4f4e\u4e8e50% \u63a7\u5236\u6743\u4e4b\u4e89\u6709\u201c\u8fc7\u6ee5\u201d\u4e4b\u5acc',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:50',
 'title': u'11\u5bb6\u4e0a\u5e02\u516c\u53f8\u62df\u53c2\u4e0e\u7b79\u529e\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u738b\u77f36\u5e74\u6765\u9996\u6b21\u53c2\u52a0\u5a92\u4f53\u4f1a \u5173\u4e8e\u5b9d\u4e07\u4e4b\u4e89\u8bf4\u4e86\u4e94\u4ef6\u91cd\u8981\u7684\u4e8b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:55',
 'title': u'\u5168\u90e8\u505c\u4ea7\uff01\u77f3\u836f\u96c6\u56e2\u534e\u5317\u5236\u836f\u7b49\u591a\u5bb6\u4e0a\u5e02\u836f\u4f01\u8eba\u67aa\u73af\u4fdd\u4ee4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 09:19',
 'title': u'\u300a\u6f58\u91d1\u83b2\u300b\u7275\u51fa\u5bf9\u8d4c\u534f\u8bae\uff1a\u51af\u5bfc\u4e3a6000\u4e07\u4e1a\u7ee9\u627f\u8bfa\u4e0d\u60dc\u6495\u7834\u8138',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u534e\u5851\u63a7\u80a1\u7529\u591a\u5e74\u201c\u5305\u88b1\u201d\uff1a\u63a7\u80a1\u5b50\u516c\u53f8\u7533\u8bf7\u7834\u4ea7\u6e05\u7b97',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u6f6e\u9000\u516c\u52df\u4e50\u7c89\u4f55\u53bb\u4f55\u4ece\uff1a\u91cd\u4ed3\u673a\u6784\u6295\u8d44\u8005\u6216\u906d\u91cd\u632b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u6c7d\u8f66200\u4ebf\u83ab\u5e72\u5c71\u5de5\u5382\u4e0b\u6708\u5960\u57fa\uff1a\u5b98\u5458\u79f0\u4e50\u89c6\u80af\u5b9a\u62ff\u5730',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u76d8\u70b95\u5927\u795e\u5947\u58f3\u516c\u53f8\uff1a\u62ff\u5230\u4e00\u4e2a\u5341\u591a\u5929\u5c31\u80fd\u8d5a\u51e0\u5341\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:44',
 'title': u'\u4e07\u8fbe\u9662\u7ebf\u6b32\u6253\u901a\u5168\u4ea7\u4e1a\u94fe \u767e\u4f59\u673a\u6784\u8c03\u7814',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:00',
 'title': u'11\u670821\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html" }
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u6d77\u80fd\u8fbe\u62df\u5b9a\u589e\u52df\u8d4410\u4ebf \u672c\u6b21\u5b9a\u589e\u5168\u90e8\u7531\u81ea\u5bb6\u4eba\u5305\u63fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u4e2d\u5b89\u6d88\u83b7\u65e5\u672c3.13\u4ebf\u5143\u5927\u5355 \u5c06\u6709\u5229\u4e8e\u8fdb\u4e00\u6b65\u62d3\u5bbd\u5883\u5916\u5e02\u573a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:10',
 'title': u'\u5c71\u4e1c\u534e\u9e4f\u516c\u544a\u9ad8\u9001\u8f6c\u7cfb\u8bef\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html'}
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u4e24\u9879\u76ee\u672a\u73b0\u65bd\u5de5\u8ff9\u8c61 \u6d89\u6295\u8d44\u989d600\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html" }
2016-11-21 14:37:37 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:37 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u80a1\u4ef7\u8fde\u521b\u65b0\u9ad8\u80cc\u540e \u91d1\u83b1\u7279\u62ff\u4ec0\u4e48\u6491\u8d77\u9ad8\u4f30\u503c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u6807\u7684\u8d44\u4ea7\u4e1a\u7ee9\u672a\u8fbe\u9884\u671f \u76db\u6d0b\u79d1\u6280\u7ec8\u6b62\u91cd\u7ec4\u906d\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:09',
 'title': u'\u56de\u590d\u6df1\u4ea4\u6240\u91cd\u7ec4\u95ee\u8be2\u51fd \u4e1c\u65b9\u7f51\u7edc\u610f\u5728\u589e\u5f3a\u5185\u5bb9\u63d0\u4f9b\u80fd\u529b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:09',
 'title': u'\u4e0a\u6d77\u5929\u73ae100%\u80a1\u67433.42\u4ebf\u5143\u6302\u724c\u8f6c\u8ba9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'8\u59294\u6b21 \u96f7\u67cf\u79d1\u6280\u63a7\u80a1\u80a1\u4e1c\u51cf\u6301\u4e0a\u763e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:22',
 'title': u'\u65b9\u6848\u4e24\u6b21\u88ab\u5426 \u5965\u7ef4\u901a\u4fe1\u7ec8\u6b62\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'\u745e\u548c\u80a1\u4efd\u6da8\u505c \u62df10\u8f6c25\u6d3e2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'\u897f\u90e8\u8d44\u6e90\u7b499\u516c\u53f8\u56e0\u91cd\u8981\u4e8b\u987921\u65e5\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:22',
 'title': u'11\u6708\u795e\u534e5500\u52a8\u529b\u7164\u518d\u4e0b\u8c035\u5143/\u5428',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 09:58',
 'title': u'\u745e\u4e30\u9ad8\u6750\u62df\u6536\u8d2d\u62df\u4e0a\u5e02\u516c\u53f8\u6c5f\u82cf\u548c\u65f6\u5229',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 10:50',
 'title': u'\u91d1\u5c71\u8f6f\u4ef6\u7b2c\u4e09\u5b63\u5ea6\u8425\u6536\u540c\u6bd4\u589e\u957f\u8fd1\u4e94\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 12:13',
 'title': u'\u901a\u5bcc\u5fae\u7535\u83b7\u5f97\u653f\u5e9c\u8865\u52a93080\u4e07\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 12:39',
 'title': u'\u4e09\u836f\u4f01\u62ab\u9732\u505c\u724c\u539f\u7531 \u5609\u5e94\u5236\u836f\u63a7\u80a1\u6743\u6216\u8f6c\u8ba9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 13:03',
 'title': u'\u5929\u745e\u4eea\u5668\u62df3.6\u4ebf\u6536\u8d2d\u4f53\u5916\u8bca\u65ad\u4ea7\u54c1\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 07:58',
 'title': u'11\u670821\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:05',
 'title': u'11\u670821\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 08:09',
 'title': u'11\u670821\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a*ST\u5357\u7535\u7ec8\u6b62\u7b79\u5212\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4 21\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html" }
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u591a\u673a\u6784\u7ec4\u56e2\u5f0f\u8fdb\u9a7b\u91cd\u7ec4\u6982\u5ff5\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 13:04',
 'title': u'11\u670821\u65e5\u5348\u95f4\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-21 12:33',
 'title': u'\u91d1\u57ce\u80a1\u4efd\u62df\u66f4\u540d\u795e\u96fe\u8282\u80fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Error processing {'content': u'<div class="Dte',
 'time': u'2016-11-18 08:11',
 'title': u'\u539f\u9ad8\u7ba1\u7ec6\u8ff01114\u8463\u4e8b\u4f1a\u5185\u60c5 \u5357\u73bbA\u4e71\u5c40\u7f18\u8d77\u65e9\u751f\u5acc\u9699\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html'}
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/pipelines.py", line 33, in process_item
    self.db[self.collection_name].insert(dict(item))
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 2212, in insert
    check_keys, manipulate, write_concern)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 535, in _insert
    check_keys, manipulate, write_concern, op_id, bypass_doc_val)
  File "/Library/Python/2.7/site-packages/pymongo/collection.py", line 517, in _insert_one
    _check_write_command_response([(0, result)])
  File "/Library/Python/2.7/site-packages/pymongo/helpers.py", line 309, in _check_write_command_response
    raise DuplicateKeyError(error.get("errmsg"), 11000, error)
DuplicateKeyError: E11000 duplicate key error collection: news.news index: url_1 dup key: { : "http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html" }
2016-11-21 14:37:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 34, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:10',
 'title': u'\u5bcc\u745e\u7279\u88c5\u7b7e\u7f72\u5408\u4f5c\u5f00\u53d1\u6846\u67b6\u534f\u8bae',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:45',
 'title': u'\u4e2d\u56fd\u8fdc\u6d0b\uff1a\u62df7234\u4e07\u5143\u6536\u8d2d\u4e2d\u8fdc\u5e0c\u814a\u7b49\u5883\u5916\u516c\u53f8\u90e8\u5206\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:10',
 'title': u'\u6a2a\u6cb3\u6a21\u5177\u6536\u5230796\u4e07\u5143\u653f\u5e9c\u8865\u8d34',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:13',
 'title': u'11\u670818\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a\u534e\u5cf0\u8d85\u7ea4\u5468\u4e94\u590d\u724c \u62df18\u4ebf\u5143\u6536\u8d2d\u79fb\u52a8\u652f\u4ed8\u670d\u52a1\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:13',
 'title': u'11\u670818\u65e5\u9ad8\u9001\u8f6c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:15',
 'title': u'11\u670818\u65e5\u91cd\u8981\u589e\u51cf\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:00',
 'title': u'\u5168\u7b51\u80a1\u4efd\uff1a\u7b7e\u8ba2\u5408\u4f5c\u6846\u67b6\u534f\u8bae \u6709\u671b\u83b7\u5f97\u5927\u91cf\u8ba2\u5355',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:05',
 'title': u'\u4e09\u529b\u58eb\u667a\u80fd\u88c5\u5907\u5c55\u4f1a\u60ca\u8273\u4eae\u76f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:03',
 'title': u'\u65b0\u6e56\u4e2d\u5b9d\uff1a\u62df7.1\u4ebf\u5143\u8f6c\u8ba9\u5b50\u516c\u53f8\u65b0\u6e56\u5b9d\u534e65%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:10',
 'title': u'\u65b0\u6e56\u4e2d\u5b9d\u5168\u8d44\u5b50\u516c\u53f8\u65a5\u903e14.69\u4ebf\u5143\u62ff\u5730',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:51',
 'title': u'\u6b65\u68ee\u80a1\u4efd\uff1a\u8fdb\u519b\u91d1\u878d\u79d1\u6280  \u6398\u91d1\u4e07\u4ebf\u5e02\u573a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:09',
 'title': u'\u4e2d\u79d1\u66d9\u5149\u80a1\u4e1c\u5929\u5bcc\u521b\u6295\u51cf\u63013.11%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:09',
 'title': u'\u6885\u6cf0\u8bfa\u62df3000\u4e07\u5143\u6536\u8d2d\u9f0e\u5143\u4fe1\u5e7f49%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:10',
 'title': u'\u5927\u5bcc\u79d1\u62802016\u5e74\u5ea6\u4e1a\u7ee9\u9884\u589e108%-137%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u6613\u6210\u65b0\u80fd\u62df\u903e28\u4ebf\u5143\u6536\u8d2d\u6c5f\u897f\u8d5b\u7ef4\u53ca\u65b0\u4f59\u8d5b\u7ef4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u7528\u53cb\u7f51\u7edc\u62df\u53d1\u8d77\u8bbe\u7acb\u5317\u4eac\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u70bc\u77f3\u6709\u8272\u505c\u724c\u7b79\u5212\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:19',
 'title': u'\u89e3\u8bfb\u4e50\u89c6\u7f51\u8d22\u62a5 \u4e50\u89c6\u6a21\u5f0f\u9700\u7834\u89e3\u201c\u4f9b\u8840\u4e0d\u8db3\u201d\u96be\u9898',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:19',
 'title': u'\u77f3\u69b4\u96c6\u56e2\u6536\u5173\u6ce8\u51fd\uff1a\u56e0\u5728\u975e\u6307\u5b9a\u5a92\u4f53\u4e0a\u53d1\u58f0',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u534e\u661f\u521b\u4e1a\u7ec8\u6b62\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u7ef4\u5c14\u522918\u65e5\u590d\u724c \u62df\u6536\u8d2d\u4e24\u5bb6\u73af\u4fdd\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u4e07\u79d1A\uff1a\u6536\u8d2d\u524d\u6d77\u56fd\u9645\u65b9\u6848\u5c1a\u672a\u8fbe\u6210\u5171\u8bc6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u5929\u836f\u80a1\u4efd\u62df11.59\u4ebf\u5143\u6536\u8d2d\u91d1\u8000\u836f\u4e1a62%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html'}
2016-11-21 14:37:38 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:37:38 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u534e\u5cf0\u8d85\u7ea4\u5468\u4e94\u590d\u724c \u62df18\u4ebf\u5143\u6536\u8d2d\u79fb\u52a8\u652f\u4ed8\u670d\u52a1\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html'}
2016-11-21 14:37:41 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:37:41 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:56',
 'title': u'\u5929\u9f99\u96c6\u56e2\uff1a2016\u5e74\u5ea6\u62df10\u8f6c15',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html'}
2016-11-21 14:37:41 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:37:41 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 82916,
 'downloader/request_count': 277,
 'downloader/request_method_count/GET': 277,
 'downloader/response_bytes': 2394141,
 'downloader/response_count': 277,
 'downloader/response_status_count/200': 276,
 'downloader/response_status_count/302': 1,
 'dupefilter/filtered': 85,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 37, 41, 183232),
 'item_scraped_count': 227,
 'log_count/DEBUG': 506,
 'log_count/ERROR': 39,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 276,
 'scheduler/dequeued': 277,
 'scheduler/dequeued/memory': 277,
 'scheduler/enqueued': 277,
 'scheduler/enqueued/memory': 277,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 6, 37, 31, 29078)}
2016-11-21 14:37:41 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:42:14 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:42:14 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'BOT_NAME': 'news'}
2016-11-21 14:42:14 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:42:14 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:42:14 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:42:14 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:42:14 [scrapy] INFO: Spider opened
2016-11-21 14:42:14 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:42:14 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_1.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_2.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_5.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_4.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_3.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_7.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Filtered duplicate request: <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098371.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_6.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_8.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098432.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098431.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/index_9.html> (referer: None)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098430.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098429.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098463.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098454.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098428.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098432.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:59',
 'title': u'\u5eb7\u8010\u727923.4\u4ebf\u5143\u8d2d\u4e70\u65d7\u8ba1\u667a\u80fd100%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098432.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098431.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:36',
 'title': u'\u54c8\u6295\u80a1\u4efd\uff1a\u5168\u8d44\u5b50\u516c\u53f8\u6c5f\u6d77\u8bc1\u5238\u62df5\u4ebf\u5143\u8bbe\u7acb\u5b50\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098431.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098433.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098498.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098497.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098466.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098430.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:44',
 'title': u'\u4e30\u5143\u80a1\u4efd\uff1a\u62df4.2\u4ebf\u5143\u5efa\u8bbe\u5e74\u4ea7\u4e07\u5428\u9502\u7535\u6c60\u6b63\u6781\u6750\u6599\u78f7\u9178\u94c1\u9502\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098430.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098429.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u5367\u9f99\u5730\u4ea7\u7ec8\u6b62\u6536\u8d2d\u58a8\u9e9f\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098429.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098463.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:37',
 'title': u'\u4e1c\u65b9\u7f51\u7edc\u62df35.36\u4ebf\u5143\u6536\u8d2d3\u5bb6\u5f71\u89c6\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098463.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098454.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:07',
 'title': u'\u5e7f\u6c47\u6c7d\u8f66\uff1a\u9644\u5c5e\u516c\u53f8\u62df66\u4ebf\u5143\u8bbe\u7acb\u5927\u8fde\u5b9d\u4fe1\u6c47\u8a89\u6c7d\u8f66\u6295\u8d44\u7ba1\u7406\u6709\u9650\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098454.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098428.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u534e\u4eea\u7535\u6c14\u906d\u63a7\u80a1\u80a1\u4e1c\u51cf\u63011030.39\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098428.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098433.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:01',
 'title': u'\u5929\u9f99\u5149\u7535\u8f6c\u8ba94\u5bb6\u5b50\u516c\u53f8\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098433.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098498.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:33',
 'title': u'\u7f8e\u5c14\u96c5\uff1a\u62df\u51fa\u8d442.55\u4ebf\u5143\u5171\u540c\u8bbe\u7acb\u4e2d\u690d\u91d1\u878d\u8d44\u4ea7\u4ea4\u6613\u4e2d\u5fc3 \u6301\u80a151%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098498.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:56',
 'title': u'\u80a1\u5e02\u8d5a\u94b1\u6548\u5e94\u663e\u73b0 \u5458\u5de5\u6301\u80a1\u8ba1\u5212\u5bc6\u96c6\u5efa\u4ed3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097799.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098497.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 21:18',
 'title': u'\u4e09\u6c5f\u8d2d\u7269\uff1a\u62df\u52df\u8d4415\u4ebf\u5143\u7528\u4e8e\u95e8\u5e97\u6e20\u9053\u6539\u9020\u53ca\u4ed3\u50a8\u7269\u6d41\u57fa\u5730\u5347\u7ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098497.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098466.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:39',
 'title': u'\u4e2d\u5b89\u6d88\u65e5\u672c\u5168\u8d44\u516c\u53f8\u83b73.13\u4ebf\u5143\u8ba2\u5355',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098466.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:02',
 'title': u'\u6052\u5927\u80fd\u633d\u56de\u201c\u77ed\u7092\u201d\u4e8b\u4ef6\u4e2d\u53d7\u635f\u7684\u58f0\u8a89\u5417\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097809.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:04',
 'title': u'\u4e2d\u56fd\u6052\u5927\u589e\u6301\u4e07\u79d1\u81f39.45% \u4e07\u79d1\u91cd\u7ec4\u65b9\u6848\u5c1a\u672a\u5c18\u57c3\u843d\u5b9a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097819.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:48',
 'title': u'11\u5bb6\u516c\u53f8\u8054\u624b\u7b79\u5efa\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097882.html'}
2016-11-21 14:42:14 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u8d44\u672c\u4e0e\u5b9e\u4e1a\u7684\u77db\u4e0e\u76fe\uff1a\u5357\u73bbA\u5f00\u542f\u524d\u6d77\u4eba\u5bff\u65f6\u4ee3 \u8d70\u5411\u5f15\u5173\u6ce8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099071.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u63a2\u8bbf\u5357\u73bbA\u6210\u90fd\u5206\u516c\u53f8\uff1a\u8463\u4e8b\u957f\u6302\u9774\u603b\u7ecf\u7406\u4ecd\u5728\u5c97',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099073.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u5357\u73bbA\u98ce\u6ce2\uff1a\u4ece\u8001\u724c\u5408\u8d44\u4f01\u4e1a\u5230\u8d44\u672c\u730e\u7269\u4e4b\u65c5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099070.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u5411\u65e5\u8475\u6536\u8d2d\u5965\u80fd\u7535\u6e90\u589e\u503c\u8d8510\u500d\u906d\u95ee\u8be2 \u79f0\u5145\u7535\u6869\u524d\u666f\u5e7f\u9614',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099112.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u903e\u4e03\u6210\u4e0a\u5e02\u516c\u53f8\u5927\u80a1\u4e1c\u6301\u80a1\u4f4e\u4e8e50% \u63a7\u5236\u6743\u4e4b\u4e89\u6709\u201c\u8fc7\u6ee5\u201d\u4e4b\u5acc',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099111.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:01',
 'title': u'\u76d1\u7ba1\u90e8\u95e8\u6478\u5e95\u623f\u4f01\u53c2\u80a1\u5730\u65b9\u94f6\u884c \u623f\u4f01\u878d\u8d44\u538b\u529b\u589e\u5927',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096692.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:47',
 'title': u'\u4e1c\u963f\u963f\u80f6\u4e09\u5927\u4e3b\u5bfc\u4ea7\u54c1\u63d0\u4ef7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097881.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:19',
 'title': u'80\u540e\u8463\u4e8b\u957f\u5927\u9605\u5175\uff1a38\u4eba\u9760\u5bb6\u65cf\u4f20\u627f 4\u4eba\u767d\u624b\u8d77\u5bb6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095704.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:15',
 'title': u'\u9646\u5bb6\u5634\u56e0\u4fe1\u606f\u62ab\u9732\u906d\u4e0a\u4ea4\u6240\u95ee\u8be2 \u51fa\u552e\u8d44\u4ea7\u6d89\u5acc\u4e0d\u5f53\u5173\u8054\u4ea4\u6613',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095711.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:01',
 'title': u'\u65b0\u5b81\u7269\u6d41\u63a7\u80a1\u80a1\u4e1c\u53ca\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u5206\u522b\u51cf\u6301300\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095375.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 18:46',
 'title': u'\u4e50\u89c6\u63a7\u80a1\u64a4\u6362\u4e9a\u592a\u533a\u603b\u88c1 \u9999\u6e2f\u516c\u53f8\u88ab\u4f20\u4e24\u4e2a\u6708\u524d\u5df2\u5f00\u59cb\u88c1\u5458',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095370.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:14',
 'title': u'\u5ddd\u73af\u79d1\u6280\u5b9e\u63a7\u4eba\u610f\u89c1\u201c\u53cd\u590d\u201d\uff1f \u848b\u9752\u6625\u8463\u4e8b\u804c\u4f4d\u201c\u5f97\u800c\u590d\u5931\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095708.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 18:50',
 'title': u'\u5b9d\u5854\u5b9e\u4e1a\uff1a\u4ea4\u6613\u5f02\u5e38\u906d\u6df1\u4ea4\u6240\u95ee\u8be2 \u8981\u6c42\u8bf4\u660e\u662f\u5426\u5b58\u5728\u5185\u5e55\u4ea4\u6613',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095371.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:21',
 'title': u'\u6df1\u6e2f\u901a\u4e34\u8fd1 \u673a\u6784\u5bc6\u96c6\u8c03\u7814\u4e2d\u5c0f\u521b\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095707.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u534e\u8c0a\u96c6\u56e2\u56fd\u6709\u80a1\u4efd\u65e0\u507f\u5212\u8f6c\u5b8c\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095378.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:00',
 'title': u'\u5174\u6e90\u73af\u5883\u7b7e\u8ba2\u4f9b\u6392\u6c34\u5de5\u7a0bPPP\u9879\u76eePPP\u534f\u8bae',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095372.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:01',
 'title': u'\u534e\u5e73\u80a1\u4efd\u6301\u80a15%\u4ee5\u4e0a\u7684\u80a1\u4e1c\u51cf\u6301673.5\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095373.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u4e1c\u65b9\u521b\u4e1a\u53c2\u80a1\u516c\u53f8IPO\u83b7\u6279',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095386.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u6df1\u8d5b\u683c16\u65e5\u590d\u724c \u91cd\u7ec4\u83b7\u8bc1\u76d1\u4f1a\u6709\u6761\u4ef6\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095387.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u79d1\u8fbe\u80a1\u4efd16\u65e5\u590d\u724c \u91cd\u7ec4\u83b7\u8bc1\u76d1\u4f1a\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095385.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u4f20\u5316\u80a1\u4efd\u8bc1\u5238\u7b80\u79f0\u53d8\u66f4\u4e3a\u201c\u4f20\u5316\u667a\u8054\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095390.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5929\u5eb7\u751f\u7269\u80a1\u4e1c\u4e2d\u65b0\u5efa\u62db\u5546\u62ab\u9732\u51cf\u6301\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095391.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u6d77\u9646\u91cd\u5de5\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u63015%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095395.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u79d1\u6797\u73af\u4fdd\u80a1\u6743\u8f6c\u8ba9\u5b8c\u6210 \u4e1c\u8bda\u745e\u4e1a\u6210\u5927\u80a1\u4e1c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095392.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5229\u4e9a\u5fb7\u5458\u5de5\u6301\u80a1\u8ba1\u5212\u4e70\u51651.1%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095397.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5eb7\u8010\u7279\u5b50\u516c\u53f8\u4e2a\u522b\u4eba\u5458\u6d89\u5acc\u804c\u52a1\u4fb5\u5360',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095394.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u5367\u9f99\u5730\u4ea7\u62df\u7ec8\u6b62\u8de8\u754c\u6536\u8d2d\u6e38\u620f\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095393.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u9ec4\u5c71\u65c5\u6e38\u53c2\u80a1\u516c\u53f8IPO\u83b7\u6279',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095399.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:21',
 'title': u'\u5b81\u6ce2\u5bcc\u90a6\u8c03\u6574\u91cd\u7ec4\u65b9\u6848\u3000\u575a\u5b9a\u63a8\u8fdb\u6218\u7565\u8f6c\u578b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095408.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'*ST\u73e0\u6c5f\u91cd\u7ec4\u83b7\u5317\u4eac\u56fd\u8d44\u59d4\u6279\u590d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095388.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:40',
 'title': u'\u4e1c\u9633\u5149\u79d1\u56e0\u5173\u6d89\u91cd\u5927\u4e8b\u987916 \u65e5\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095410.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u677e\u53d1\u80a1\u4efd\u63a7\u80a1\u80a1\u4e1c\u589e\u6301\u8ba1\u5212\u5b9e\u65bd\u5b8c\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095396.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:05',
 'title': u'\u9ad8\u4f1f\u8fbe\u80a1\u4e1c\u94f6\u8054\u79d1\u6280\u62ab\u9732\u51cf\u6301\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095398.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:41',
 'title': u'\u4e2d\u56fd\u94c1\u5efa\u53ca\u5b50\u516c\u53f8\u7ec4\u8054\u5408\u4f53\u4e2d\u6807\u6295\u8d44\u989d\u7ea6229.19 \u4ebf\u5143PPP \u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095411.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:07',
 'title': u'\u5b81\u6ce2\u97f5\u5347\uff1a\u62df\u51fa\u8d443200\u4e07\u5143\u5171\u540c\u8bbe\u7acb\u97f5\u5347\u7535\u5b50\u516c\u53f8 \u6301\u80a140%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095444.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:57',
 'title': u'\u9c7c\u8dc3\u533b\u7597\u62df8.6\u4ebf\u5143\u6536\u8d2d\u533b\u7528\u6d88\u6bd2\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095485.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 19:42',
 'title': u'\u4e2d\u822a\u4e09\u946b\u80a1\u4e1c\u97e9\u5e73\u5143\u51cf\u63011.87%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095413.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:18',
 'title': u'\u6c47\u91d1\u79d1\u628011\u670817\u65e5\u767b\u9646\u521b\u4e1a\u677f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095442.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 21:18',
 'title': u'\u73c8\u4f1f\u80a1\u4efd\u53d1\u5e03\u5168\u7403\u9996\u4f8b\u56fa\u6001\u9502\u7535\u6c60\u4e0e\u5feb\u5145\u9502\u7535\u6c60',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095500.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:50',
 'title': u'\u5357\u73bbA\u4e03\u540d\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095640.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:15',
 'title': u'\u6c38\u6cf0\u80fd\u6e90\uff1a\u5408\u8d44\u5b50\u516c\u53f8\u62df\u5411\u5357\u9633\u7535\u5382\u589e\u8d4410\u4ebf\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095443.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:57',
 'title': u'\u5e74\u7ec8\u5957\u73b0"\u6697\u6d41\u6d8c" \u6076\u610f\u51cf\u6301"\u5957\u8def\u6df1"',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095665.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:51',
 'title': u'\u4e1c\u822a\u6837\u672c\uff1a\u56fd\u8d44\u63a7\u5236\u4e0b\u591a\u4e3e\u63a8\u8fdb\u6df7\u6539',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095644.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-15 20:04',
 'title': u'\u7696\u6c5f\u7269\u6d41\uff1a\u5168\u8d44\u5b50\u516c\u53f8\u903e5\u4ebf\u5143\u6536\u8d2d\u63a7\u80a1\u80a1\u4e1c\u7535\u5382\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161115_5095445.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:09',
 'title': u'\u6052\u5927\u9ad8\u4f4d\u63a5\u76d8\u4e07\u79d1\u7384\u673a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095693.html'}
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_9.html)
2016-11-21 14:42:15 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098443.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 07:53',
 'title': u'\u4e50\u89c6\u5c06\u83b7\u201c\u597d\u540c\u5b66\u201d6\u4ebf\u7f8e\u5143\u6295\u8d44',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095648.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:01',
 'title': u'\u6559\u80b2\u4ea7\u4e1a\u8d44\u6e90\u6574\u5408\u52a0\u901f\uff08\u9644\u80a1\uff09',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095671.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:05',
 'title': u'\u5ef6\u671f\u62ab\u9732\u91cd\u5927\u8bc9\u8bbc\u4e8b\u9879 \u5339\u51f8\u5339\u88ab\u4e0a\u4ea4\u6240\u8b66\u793a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095675.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:04',
 'title': u'\u5de5\u5927\u9ad8\u65b0\u53ca\u76f8\u5173\u9ad8\u7ba1\u88ab\u516c\u5f00\u8c34\u8d23',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095672.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:16',
 'title': u'\u5357\u73bb7\u540d\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c \u5b9d\u80fd\u7cfb\u79f0\u5e76\u672a\u5e72\u6d89\u516c\u53f8\u7ecf\u8425',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095699.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:24',
 'title': u'\u201c\u5927\u91d1\u4e3b\u201d\u63a8\u6ce2\u52a9\u6f9c\u58f3\u4f30\u503c\u534a\u5e74\u7ffb\u756a\uff1a100\u4ebf\u5143\u6210\u6807\u914d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095715.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098443.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 20:03',
 'title': u'\u91d1\u9685\u5609\u4e1a\u5357\u4eac\u516c\u53f8\uff1a\u79ef\u6781\u5145\u5b9e\u571f\u5730\u50a8\u5907 \u5168\u5e74\u9884\u8ba1\u5b8c\u6210\u5408\u540c\u91d1\u989d30\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098443.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u591a\u673a\u6784\u7ec4\u56e2\u5f0f\u8fdb\u9a7b\u91cd\u7ec4\u6982\u5ff5\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095782.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:14',
 'title': u'\u5265\u79bb\u71c3\u6c14\u5177\u8d44\u4ea7\u81f4\u4e3b\u4e1a\u201c\u60ac\u7a7a\u201d \u4e07\u5bb6\u4e50\u91cd\u7ec4\u906d\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095712.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u9c7c\u8dc3\u533b\u7597\u62df8.63\u4ebf\u5165\u4e3b\u4e2d\u4f18\u533b\u836f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095785.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:49',
 'title': u'\u4fe1\u62ab\u4e25\u91cd\u5931\u5b9e*ST\u5de5\u65b0\u906d\u4e0a\u4ea4\u6240\u8c34\u8d23',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095765.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:16',
 'title': u'\u4eba\u6c11\u5e01\u51fb\u7a7f\u201c\u94c1\u5e95\u201d \u4e00\u5927\u6ce2\u4e0a\u5e02\u516c\u53f8\u635f\u5931\u8fc7\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095795.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:48',
 'title': u'\u4e0a\u6d77\u94f6\u884c\u4eca\u65e5\u4e0a\u5e02 \u6279\u91cf\u9020\u5bcc\u8fd16000\u5458\u5de5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095763.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:12',
 'title': u'\u8d3e\u8dc3\u4ead\u662f\u600e\u6837\u62ff\u4e0b\u957f\u6c5f\u5546\u5b66\u9662\u540c\u5b66\u76846\u4ebf\u6295\u8d44\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095797.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u623f\u4f01\u878d\u8d44\u5168\u7ebf\u6536\u7d27\uff1a\u53ea\u8981\u8fd8\u6ca1\u653e\u6b3e\u7684\u90fd\u505c\u4e0b\u6765',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095822.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u4e0a\u6d77\u56fd\u8d44\u6539\u9769\u5927\u5c3a\u5ea6\u7a81\u7834 \u88c5\u5165\u4f18\u8d28\u8d44\u4ea7\u76d8\u6d3b\u201c\u58f3\u201d\u8d44\u6e90',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095825.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095907.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095915.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095902.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:08',
 'title': u'\u94a2\u6784\u5de5\u7a0b\u5265\u79bb\u4f4e\u6548\u4e1a\u52a1 \u62df\u51fa\u552e\u4e24\u5bb6\u5b50\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095791.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:29',
 'title': u'\u73c8\u4f1f\u80a1\u4efd\u53d1\u5e03\u56fa\u6001\u9502\u7535\u6c60\u4ea7\u54c1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095819.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095917.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:00',
 'title': u'\u5929\u76ee\u836f\u4e1a\u91cd\u7ec4\u516d\u8fde\u8d25 \u201c\u957f\u57ce\u7cfb\u201d\u8fdb\u9000\u7ef4\u8c37',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095783.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 08:58',
 'title': u'\u4e0a\u4ea4\u6240\u76d1\u7ba1\u95ee\u8be2\u51fb\u8981\u5bb3 \u6052\u5927\u7cfb\u660e\u786e\u4e3e\u724c\u6885\u96c1\u5409\u7965\u610f\u56fe',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095778.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095913.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095907.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:33',
 'title': u'\u4e2d\u56fd\u4f53\u80b2\u603b\u5c40\u6b32\u51fa\u552e\u4e2d\u4f53\u4ea7\u4e1a \u4e07\u8fbe\u6216\u63a5\u68d2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095907.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095993.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095915.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:33',
 'title': u'\u524d\u6d77\u4eba\u5bff\u56de\u5e94\u5357\u73bb\u9ad8\u7ba1\u8f9e\u804c\uff1a\u672a\u5e72\u7ecf\u8425 \u591a\u6b21\u5584\u610f\u633d\u7559',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095915.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095995.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095902.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:34',
 'title': u'\u201c\u6700\u706b\u201d\u6e38\u8d44\u9ad8\u4f4d\u786c\u63a5\u65b0\u534e\u7f51 \u5f00\u677f\u65b0\u80a1\u706b\u4e2d\u53d6\u6817\u80dc\u7b97\u51e0\u4f55',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095902.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:17',
 'title': u'\u5357\u73bbA\u7ba1\u7406\u5c42\u52a8\u8361\u5f15\u53d1\u6df1\u4ea4\u6240\u5173\u6ce8 \u524d\u6d77\u4eba\u5bff\u7d27\u6025\u58f0\u660e\uff1a\u4ece\u672a\u5e72\u6d89\u65e5\u5e38\u7ecf\u8425',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095860.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:53',
 'title': u'11\u670816\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095838.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 09:13',
 'title': u'\u5feb\u4e50\u65f6\u4ee3\u4f30\u503c\u6210\u8c1c \u56fd\u76db\u91d1\u63a7\u6ea2\u4ef7\u6536\u8d2d5%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095824.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095917.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:33',
 'title': u'\u8d3e\u8dc3\u4ead\u597d\u540c\u5b666\u4ebf\u7f8e\u5143\u529b\u633a\u4e50\u89c6 \u79f0\u8fd9\u4e0d\u662f\u8d48\u707e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095917.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095919.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095913.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:33',
 'title': u'\u9996\u94a2\u8001\u5382\u533a\u5730\u4ef7\u4e0d\u65ad\u98d9\u6da8 \u5730\u4ea7\u5546\u79f0\u62bc\u5b9d\u4eac\u6d25\u5180\u4e00\u4f53\u5316',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095913.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095904.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095925.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095993.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 12:29',
 'title': u'\u4eac\u7cae\u5411*ST\u73e0\u6c5f\u63d0\u4f9b10\u4ebf\u5143\u8fdb\u884c\u6536\u8d2d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095993.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095994.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095996.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095995.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 12:30',
 'title': u'\u76ca\u751f\u80a1\u4efd\u505c\u724c\u7b79\u5212\u755c\u7267\u4e1a\u8d44\u4ea7\u6536\u8d2d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095995.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096082.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095919.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:33',
 'title': u'\u6052\u5927\u8bc1\u5b9e\u672a\u9ad8\u4f4d\u51cf\u6301\u6885\u96c1\u5409\u7965 \u738b\u7684\u5973\u4eba\u5230\u5e95\u88ab\u8c01\u5f03\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095919.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095904.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:34',
 'title': u'\u5b8f\u660c\u7535\u5b50\u4e0a\u5e02\u540e\u8425\u6536\u5f00\u5012\u8f66 4\u5e74\u4e70\u4e8662\u7b14\u7406\u8d22\u4ea7\u54c1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095904.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095925.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 10:37',
 'title': u'\u79d1\u8fbe\u80a1\u4efd\u4e8c\u6b21\u91cd\u7ec4\u83b7\u8bc1\u76d1\u4f1a\u65e0\u6761\u4ef6\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095925.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096071.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096182.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095994.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 12:29',
 'title': u'\u4e1c\u65b9\u5e02\u573a11%\u80a1\u6743\u65e0\u507f\u5212\u8f6c\u5434\u6c5f\u4e1c\u65b9\u56fd\u8d44',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095994.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096085.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095996.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 12:28',
 'title': u'11\u670816\u65e5\u5348\u95f4\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5095996.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096082.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 13:46',
 'title': u'\u4e0a\u6d77\u4fdd\u4ea4\u6240\u9996\u6279\u4ea7\u54c1\u4e0a\u7ebf \u6052\u751f\u7535\u5b50\u63d0\u4f9b\u6280\u672f\u652f\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096082.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096189.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096260.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_8.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096071.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 14:01',
 'title': u'\u77f3\u69b4\u96c6\u56e2\u56de\u5e94\u4e3e\u724c\u9996\u94a2\uff1a\u662f\u6218\u7565\u6295\u8d44\u8005 \u5df2\u589e\u6301\u81f37%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096071.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096182.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 15:21',
 'title': u'\u6052\u5927\u96c6\u56e2\u4ee3\u8868\uff1a\u770b\u597d\u6c34\u529b\u53d1\u7535\u884c\u4e1a\u53d1\u5c55\u524d\u666f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096182.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096085.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 13:48',
 'title': u'\u4e1c\u822a\u6210\u4e3a\u9996\u5bb6\u8363\u83b7\u4e9a\u592a\u5730\u533a\u5e74\u5ea6\u5353\u8d8a\u822a\u7a7a\u5927\u5956\u7684\u4e2d\u56fd\u822a\u7a7a\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096085.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:15',
 'title': u'\u4e24\u516c\u53f8\u6f84\u6e05\u65e0\u6295\u8d44\u4e50\u89c6\u8ba1\u5212 \u7cfb\u8463\u4e8b\u957f\u4e2a\u4eba\u6295\u8d44',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096717.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096189.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 15:35',
 'title': u'\u6052\u5927\u96c6\u56e2\u4ee3\u8868\uff1a\u9501\u5b9a12\u4e2a\u6708  \u5207\u5b9e\u7ef4\u62a4\u201c\u4e09\u516c\u201d\u79e9\u5e8f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096189.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:04',
 'title': u'\u683c\u529b\u7535\u5668\u6536\u8d2d\u73e0\u6d77\u94f6\u9686\u7ec8\u6b62',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096698.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096260.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 15:39',
 'title': u'\u6295\u670d\u4e2d\u5fc3\u4ee3\u8868\u63d0\u51fa\u4e09\u5927\u95ee\u9898',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096260.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:02',
 'title': u'\u6052\u5927\u7cfb\u79f0\u6218\u7565\u6295\u8d44\u6885\u96c1\u5409\u7965',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096696.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:06',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c\u7f57\u751f\u95e8\uff1a\u544a\u522b\u4fe1\u8be6\u8ff0\u7f18\u7531 \u5b9d\u80fd\u7cfb\u4e00\u4e00\u5426\u8ba4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096701.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:28',
 'title': u'\u5357\u73bbA\u539f\u9ad8\u7ba1\u8bb2\u8ff0\u96c6\u4f53\u79bb\u804c\u539f\u56e0\uff1a\u524d\u6d77\u4eba\u5bff5\u4efd\u8bae\u6848\u4e5f\u662f\u5bfc\u706b\u7d22',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096747.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:40',
 'title': u'463\u5bb6\u516c\u53f8\u4e34\u8fd1\u88ab\u4e3e\u724c \u89e3\u5bc6\u4e09\u884c\u4e1a74\u53ea\u4e3e\u724c\u6f5c\u529b\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096793.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:14',
 'title': u'IPO\u53d1\u884c\u63d0\u901f \u5f71\u5b50\u80a1\u6709\u671b\u53d7\u76ca',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096714.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:40',
 'title': u'\u4e0a\u6d77\u56fd\u4f01\u6539\u9769\u518d\u6380\u9ad8\u6f6e \u673a\u6784\u9884\u8ba16\u53ea\u9f99\u5934\u80a1\u76ee\u6807\u6da8\u5e45\u8d8550%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096794.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:23',
 'title': u'\u683c\u529b\u8463\u660e\u73e0\u9020\u8f66\u201c\u68a6\u788e\u201d \u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096736.html'}
2016-11-21 14:42:16 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:47',
 'title': u'\u77f3\u69b4\u96c6\u56e2\u81ea\u66dd\u589e\u6301\u52a8\u5411 \u9996\u94a2\u80a1\u4efd\u80a1\u4ef7\u5e94\u58f0\u800c\u6da8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096810.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:26',
 'title': u'\u817e\u4fe1\u80a1\u4efd\u53ca\u5b9e\u63a7\u4eba\u6d89\u5acc\u5411\u6700\u9ad8\u9662\u539f\u526f\u9662\u957f\u4e4b\u5b50\u884c\u8d3f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096740.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:29',
 'title': u'\u201c\u5b9d\u80fd\u7cfb\u201d\u5bf9\u51b3\u5357\u73bb\u7ba1\u7406\u5c42 \u4e09\u5927\u7591\u70b9\u5f85\u89e3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096750.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:26',
 'title': u'\u83ab\u9ad8\u80a1\u4efd\u906d\u4e09\u5ea6\u4e3e\u724c \u63a7\u80a1\u80a1\u4e1c\u7b79\u5212\u5b9a\u589e\u53cd\u51fb\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096741.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'\u5468\u9e3f\u794e\u8c08360\u56de\u5f52\u56de\u5f52:\u786e\u4e0e\u56fd\u5bb6\u7f51\u7edc\u5b89\u5168\u6218\u7565\u6709\u5173',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096814.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:51',
 'title': u'\u8bc4\u8bba:\u5357\u73bb\u4e8b\u4ef6\u6298\u5c04\u8d44\u672c\u4e0e\u804c\u4e1a\u7ecf\u7406\u4eba\u5171\u5904\u56f0\u5883',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096827.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:45',
 'title': u'\u52df\u6295\u9879\u76ee\u8fdb\u5c55\u4e0d\u53ca\u9884\u671f \u5305\u94a2\u80a1\u4efd\u906d\u4e0a\u4ea4\u6240\u4e8c\u6b21\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096801.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u4e2d\u7535\u5e7f\u901a\u5265\u79bb\u539f\u4e1a\u52a1 \u63fd\u5165\u519b\u5de5\u7535\u5b50\u6807\u7684',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096863.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'PPP\u6982\u5ff5\u201c\u62db\u8702\u5f15\u8776\u201d \u673a\u6784\u7784\u4e0a\u5efa\u7b51\u88c5\u9970\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096813.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:46',
 'title': u'\u5927\u8fde\u4e0a\u5e02\u516c\u53f8\u91cd\u7ec4\u201c\u751f\u610f\u7ecf\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096811.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u8001\u5458\u5de5\u8f9e\u804c\u98ce\u6ce2\u8d8a\u95f9\u8d8a\u5927 \u5357\u73bb\u5b9d\u80fd\u5404\u6267\u4e00\u8bcd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096859.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bb\u5bab\u6597\u771f\u76f8\u53e3\u6c34\u6218\u4e2d\u6108\u53d1\u6a21\u7cca \u5f53\u4e8b\u53cc\u65b9\u5404\u6267\u4e00\u8bcd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096858.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u8f9e\u804c\uff1a\u4e07\u79d1\u5267\u60c5\u7684\u9884\u6f14?',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096870.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bb8\u9ad8\u7ba1\u8f9e\u804c \u524d\u6d77\u4eba\u5bff\uff1a\u80a1\u6743\u6fc0\u52b1\u53ea\u662f\u501f\u53e3',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096862.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u7f8e\u56fd\u4e50\u89c6\u6c7d\u8f66\u56de\u5e94\u505c\u4ea7:\u9879\u76ee\u4ece\u672a\u505c\u5de5 \u8d44\u91d1\u5c06\u5230\u4f4d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096871.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u79bb\u804c\u80cc\u540e \u6838\u5fc3\u6280\u672f\u53bbor\u7559',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096864.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u4ee3\u7406\u8463\u4e8b\u957f\u9648\u7433\u9996\u53d1\u58f0\uff1a\u8981\u4fdd\u62a4\u597d\u5168\u4f53\u80a1\u4e1c\u6743\u76ca',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096861.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u4e09\u6cf0\u63a7\u80a1\u5b9a\u589e\u4ef7\u5012\u6302 \u89e3\u7981\u80a1\u4e1c\u201c\u5e72\u77aa\u773c\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096868.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:03',
 'title': u'\u5357\u73bbA\u518d\u6536\u6df1\u4ea4\u6240\u5173\u6ce8\u51fd \u8981\u6c42\u5176\u5bf9\u5a92\u4f53\u62a5\u9053\u53ca\u6295\u8bc9\u4e3e\u62a5\u4e88\u4ee5\u6838\u67e5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096867.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_6.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u4e2d\u6c47\u5f71\u89c6\u9690\u7792\u5bf9\u8d4c \u4e09\u4e03\u4e92\u5a31\u65e7\u6807\u7684\u518d\u9047\u632b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096869.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 08:54',
 'title': u'\u91d1\u660e\u7cbe\u673a\u6301\u7eed\u63a8\u8fdb\u201c\u667a\u80fd\u5236\u9020+\u5927\u5065\u5eb7\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096878.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:34',
 'title': u'11\u670817\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1\uff1a\u4e2d\u7535\u5e7f\u901a\u62df\u5265\u79bb\u4f20\u7edf\u4e1a\u52a1 \u7f6e\u5165\u519b\u5de5\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096363.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u6885\u96c1\u5409\u7965\u5982\u4f55\u6f14\u7ece\u201c\u540e\u6052\u5927\u65f6\u4ee3\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096873.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 09:01',
 'title': u'\u6668\u9e23\u7eb8\u4e1a\u8fde\u7eed\u591a\u6b21\u63d0\u4ef7 \u201c\u6d1b\u9633\u7eb8\u8d35\u201d\u535a\u5f08\u4eba\u6c11\u5e01\u8d2c\u503c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096872.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 10:12',
 'title': u'11\u670817\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096932.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:31',
 'title': u'\u683c\u529b\u7535\u5668\u62df\u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096362.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:23',
 'title': u'\u6df1\u4ea4\u6240\u5411\u5357\u73bbA\u518d\u53d1\u5173\u6ce8\u51fd \u5357\u73bbA\u8463\u79d8\u53ca\u4e24\u72ec\u7acb\u8463\u4e8b\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096366.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:31',
 'title': u'\u91d1\u5cad\u77ff\u4e1a\u62df\u7ec8\u6b62\u7b79\u5212\u8d2d\u4e70\u8d44\u4ea7 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096361.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 17:04',
 'title': u'11\u670817\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096318.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:26',
 'title': u'\u592a\u6781\u5b9e\u4e1a\u63a7\u80a1\u5b50\u516c\u53f8\u4e2d\u6807\u91d1\u989d\u903e10.5\u4ebf\u5143\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096398.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4ebf\u6676\u5149\u7535\u906d\u63a7\u80a1\u80a1\u4e1c\u51cf\u63011500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096410.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:35',
 'title': u'11\u670817\u65e5\u589e\u51cf\u6301\uff1a\u4ebf\u6676\u5149\u7535\u906d\u63a7\u80a1\u80a1\u4e1c\u51cf\u63011500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096381.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:24',
 'title': u'\u683c\u529b\u7535\u5668\u7ec8\u6b62\u53d1\u884c\u80a1\u4efd\u6536\u8d2d\u73e0\u6d77\u94f6\u9686',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096397.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:34',
 'title': u'11\u670817\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a\u683c\u529b\u7535\u5668\u62df\u7ec8\u6b62\u6536\u8d2d\u73e0\u6d77\u94f6\u9686 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096382.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:27',
 'title': u'\u673a\u5668\u4eba1.2\u4ebf\u80a1\u9650\u552e\u80a1\u89e3\u7981 1\u4ebf\u80a121\u65e5\u53ef\u4e0a\u5e02\u6d41\u901a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096399.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 18:45',
 'title': u'\u5357\u73bbA\u56e0\u7ba1\u7406\u5c42\u52a8\u8361\u518d\u6536\u6df1\u4ea4\u6240\u5173\u6ce8\u51fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096402.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e91\u5357\u767d\u836f\uff1a\u96c6\u56e2\u6df7\u6539\u65b0\u5b9e\u65bd\u8def\u5f84\u5df2\u5f97\u5230\u5404\u65b9\u8ba4\u53ef',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096411.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e0c\u52aa\u5c14\u7ec8\u6b62\u7b79\u5212\u91cd\u5927\u4e8b\u9879 17\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096420.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:42',
 'title': u'\u5357\u73bb\u8463\u79d8\u53ca\u603b\u4f1a\u8ba1\u5e08\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096414.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u9c81\u4ebf\u901a\u80a1\u4e1c\u9646\u91d1\u6d77\u51cf\u63012.35%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096416.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u56fd\u52a8\u529b\u80a1\u4e1c\u589e\u6301\u8ba1\u5212\u5b9e\u65bd\u5b8c\u6bd5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096412.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:42',
 'title': u'\u7eff\u53f6\u96c6\u56e2\u5218\u6bbf\u6ce2\u56de\u5e94\uff1a\u4e50\u89c6\u6709\u96be \u4e2a\u4eba\u62c9\u5144\u5f1f\u4e00\u628a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096415.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e1c\u6770\u667a\u80fd\u80a1\u4e1c\u5883\u754c\u6295\u8d44\u51cf\u63011.42%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096421.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u6d77\u53d1\u5c55\u66f4\u540d\u201c\u4e2d\u8fdc\u6d77\u80fd\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096418.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e7f\u751f\u5802\u63a7\u80a1\u80a1\u4e1c\u589e\u6301\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096417.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u82f1\u98de\u62d3\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096422.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u9c81\u6297\u533b\u836f\u505c\u724c\u7b79\u5212\u975e\u516c\u5f00\u53d1\u884c\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096427.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u7535\u5e7f\u901a\u62df\u5265\u79bb\u4f20\u7edf\u4e1a\u52a1 \u7f6e\u5165\u519b\u5de5\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096430.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u8fbd\u5b81\u6210\u5927\u62df4.2\u4ebf\u5143\u51fa\u552e\u5bb6\u4e50\u798f\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096423.html'}
2016-11-21 14:42:17 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:17 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u7eff\u53f6\u5236\u836f\u6f84\u6e05 \u5e76\u65e0\u4e0e\u4e50\u89c6\u8ba2\u7acb\u4efb\u4f55\u6295\u8d44\u5b89\u6392',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096424.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u897f\u85cf\u836f\u4e1a\u5b9a\u589e\u7533\u8bf7\u83b7\u8bc1\u76d1\u4f1a\u5ba1\u6838\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096425.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:01',
 'title': u'\u5929\u80fd\u91cd\u5de5\u4e2d\u7b7e\u73870.02613%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096465.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u84dd\u5149\u53d1\u5c55\u906d\u80a1\u4e1c\u51cf\u63012300\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096429.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:47',
 'title': u'\u6842\u53d1\u7965\u3001\u51ef\u83b1\u82f118\u65e5\u4e2d\u5c0f\u677f\u4e0a\u5e02',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096434.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u4e2d\u6bc5\u8fbe\u56e0\u6d89\u5acc\u4fe1\u62ab\u8fdd\u6cd5\u8fdd\u89c4\u906d\u7acb\u6848\u8c03\u67e5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096428.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:28',
 'title': u'\u79d1\u529b\u8fdc\uff1a\u62df\u51fa\u8d4414\u4ebf\u5143\u5171\u540c\u8bbe\u7acb\u6df7\u5408\u52a8\u529b\u6280\u672f\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096446.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:01',
 'title': u'\u534e\u9e4f\u98de\u80a1\u4e1c\u51cf\u63011100\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096464.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u534e\u95fb\u4f20\u5a92\u7b4910\u5bb6\u516c\u53f8\u6d89\u80a1\u6743\u8f6c\u8ba9 3\u4ebf\u6e38\u8d44\u8ffd\u63676\u53ea\u6982\u5ff5\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099113.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:45',
 'title': u'\u8fbd\u5b81\u6210\u5927\uff1a\u62df4.2\u4ebf\u5143\u51fa\u552e\u5bb6\u4e50\u798f\u5168\u90e8\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096487.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:37',
 'title': u'\u5149\u660e\u5730\u4ea7\uff1a\u62df10\u4ebf\u5143\u8f6c\u8ba9\u6240\u6301\u90e8\u5206\u7269\u4e1a\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096538.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:00',
 'title': u'\u683c\u529b\u7535\u5668\uff1a\u7ec8\u6b62\u7b79\u5212\u53d1\u884c\u80a1\u4efd\u8d2d\u4e70\u8d44\u4ea7\u4e8b\u5b9c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096497.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:54',
 'title': u'\u738b\u77f36\u5e74\u6765\u9996\u6b21\u53c2\u52a0\u5a92\u4f53\u4f1a \u5173\u4e8e\u5b9d\u4e07\u4e4b\u4e89\u8bf4\u4e86\u4e94\u4ef6\u91cd\u8981\u7684\u4e8b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099114.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 20:47',
 'title': u'\u4eba\u4e8b\u53d8\u52a8\u7ee7\u7eed \u5357\u73bbA\u8463\u79d8\u53ca\u4e24\u72ec\u7acb\u8463\u4e8b\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096484.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 21:08',
 'title': u'\u5eb7\u5f97\u65b0\uff1a\u62df\u63a8\u603b\u89c4\u6a216\u4ebf\u5143\u5458\u5de5\u6301\u80a1\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096500.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u534e\u5851\u63a7\u80a1\u7529\u591a\u5e74\u201c\u5305\u88b1\u201d\uff1a\u63a7\u80a1\u5b50\u516c\u53f8\u7533\u8bf7\u7834\u4ea7\u6e05\u7b97',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099121.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u6f6e\u9000\u516c\u52df\u4e50\u7c89\u4f55\u53bb\u4f55\u4ece\uff1a\u91cd\u4ed3\u673a\u6784\u6295\u8d44\u8005\u6216\u906d\u91cd\u632b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099123.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:55',
 'title': u'\u5168\u90e8\u505c\u4ea7\uff01\u77f3\u836f\u96c6\u56e2\u534e\u5317\u5236\u836f\u7b49\u591a\u5bb6\u4e0a\u5e02\u836f\u4f01\u8eba\u67aa\u73af\u4fdd\u4ee4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099115.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u76d8\u70b95\u5927\u795e\u5947\u58f3\u516c\u53f8\uff1a\u62ff\u5230\u4e00\u4e2a\u5341\u591a\u5929\u5c31\u80fd\u8d5a\u51e0\u5341\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099124.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 09:19',
 'title': u'\u300a\u6f58\u91d1\u83b2\u300b\u7275\u51fa\u5bf9\u8d4c\u534f\u8bae\uff1a\u51af\u5bfc\u4e3a6000\u4e07\u4e1a\u7ee9\u627f\u8bfa\u4e0d\u60dc\u6495\u7834\u8138',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099116.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u4e24\u9879\u76ee\u672a\u73b0\u65bd\u5de5\u8ff9\u8c61 \u6d89\u6295\u8d44\u989d600\u4ebf',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099125.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:52',
 'title': u'\u4e50\u89c6\u6c7d\u8f66200\u4ebf\u83ab\u5e72\u5c71\u5de5\u5382\u4e0b\u6708\u5960\u57fa\uff1a\u5b98\u5458\u79f0\u4e50\u89c6\u80af\u5b9a\u62ff\u5730',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099122.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u6d77\u80fd\u8fbe\u62df\u5b9a\u589e\u52df\u8d4410\u4ebf \u672c\u6b21\u5b9a\u589e\u5168\u90e8\u7531\u81ea\u5bb6\u4eba\u5305\u63fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099148.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u4e2d\u5b89\u6d88\u83b7\u65e5\u672c3.13\u4ebf\u5143\u5927\u5355 \u5c06\u6709\u5229\u4e8e\u8fdb\u4e00\u6b65\u62d3\u5bbd\u5883\u5916\u5e02\u573a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099154.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u80a1\u4ef7\u8fde\u521b\u65b0\u9ad8\u80cc\u540e \u91d1\u83b1\u7279\u62ff\u4ec0\u4e48\u6491\u8d77\u9ad8\u4f30\u503c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099155.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:44',
 'title': u'\u4e07\u8fbe\u9662\u7ebf\u6b32\u6253\u901a\u5168\u4ea7\u4e1a\u94fe \u767e\u4f59\u673a\u6784\u8c03\u7814',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099126.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:05',
 'title': u'\u6807\u7684\u8d44\u4ea7\u4e1a\u7ee9\u672a\u8fbe\u9884\u671f \u76db\u6d0b\u79d1\u6280\u7ec8\u6b62\u91cd\u7ec4\u906d\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099156.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:09',
 'title': u'\u56de\u590d\u6df1\u4ea4\u6240\u91cd\u7ec4\u95ee\u8be2\u51fd \u4e1c\u65b9\u7f51\u7edc\u610f\u5728\u589e\u5f3a\u5185\u5bb9\u63d0\u4f9b\u80fd\u529b',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099157.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:00',
 'title': u'11\u670821\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099146.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:09',
 'title': u'\u4e0a\u6d77\u5929\u73ae100%\u80a1\u67433.42\u4ebf\u5143\u6302\u724c\u8f6c\u8ba9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099158.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 09:58',
 'title': u'\u745e\u4e30\u9ad8\u6750\u62df\u6536\u8d2d\u62df\u4e0a\u5e02\u516c\u53f8\u6c5f\u82cf\u548c\u65f6\u5229',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099163.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'\u745e\u548c\u80a1\u4efd\u6da8\u505c \u62df10\u8f6c25\u6d3e2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099168.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'\u897f\u90e8\u8d44\u6e90\u7b499\u516c\u53f8\u56e0\u91cd\u8981\u4e8b\u987921\u65e5\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099169.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:21',
 'title': u'8\u59294\u6b21 \u96f7\u67cf\u79d1\u6280\u63a7\u80a1\u80a1\u4e1c\u51cf\u6301\u4e0a\u763e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099170.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:22',
 'title': u'\u65b9\u6848\u4e24\u6b21\u88ab\u5426 \u5965\u7ef4\u901a\u4fe1\u7ec8\u6b62\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099171.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:22',
 'title': u'11\u6708\u795e\u534e5500\u52a8\u529b\u7164\u518d\u4e0b\u8c035\u5143/\u5428',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099172.html'}
2016-11-21 14:42:18 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:18 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 10:50',
 'title': u'\u91d1\u5c71\u8f6f\u4ef6\u7b2c\u4e09\u5b63\u5ea6\u8425\u6536\u540c\u6bd4\u589e\u957f\u8fd1\u4e94\u6210',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099232.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 12:13',
 'title': u'\u901a\u5bcc\u5fae\u7535\u83b7\u5f97\u653f\u5e9c\u8865\u52a93080\u4e07\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099352.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 12:33',
 'title': u'\u91d1\u57ce\u80a1\u4efd\u62df\u66f4\u540d\u795e\u96fe\u8282\u80fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099358.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 12:39',
 'title': u'\u4e09\u836f\u4f01\u62ab\u9732\u505c\u724c\u539f\u7531 \u5609\u5e94\u5236\u836f\u63a7\u80a1\u6743\u6216\u8f6c\u8ba9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099360.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 13:03',
 'title': u'\u5929\u745e\u4eea\u5668\u62df3.6\u4ebf\u6536\u8d2d\u4f53\u5916\u8bca\u65ad\u4ea7\u54c1\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099365.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html> (referer: http://www.cs.com.cn/ssgs/gsxw/)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 13:04',
 'title': u'11\u670821\u65e5\u5348\u95f4\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5099368.html'}
2016-11-21 14:42:19 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 34, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:09',
 'title': u'11\u670821\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a*ST\u5357\u7535\u7ec8\u6b62\u7b79\u5212\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4 21\u65e5\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098999.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_3.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 07:58',
 'title': u'11\u670821\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098986.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-21 08:05',
 'title': u'11\u670821\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161121_5098996.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:11',
 'title': u'\u539f\u9ad8\u7ba1\u7ec6\u8ff01114\u8463\u4e8b\u4f1a\u5185\u60c5 \u5357\u73bbA\u4e71\u5c40\u7f18\u8d77\u65e9\u751f\u5acc\u9699\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097700.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:49',
 'title': u'\u534e\u661f\u521b\u4e1a\u7ec8\u6b62\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097884.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u534e\u5cf0\u8d85\u7ea4\u5468\u4e94\u590d\u724c \u62df18\u4ebf\u5143\u6536\u8d2d\u79fb\u52a8\u652f\u4ed8\u670d\u52a1\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097475.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_7.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:51',
 'title': u'11\u670818\u65e5\u6da8\u505c\u80a1\u63ed\u79d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097886.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:19',
 'title': u'\u89e3\u8bfb\u4e50\u89c6\u7f51\u8d22\u62a5 \u4e50\u89c6\u6a21\u5f0f\u9700\u7834\u89e3\u201c\u4f9b\u8840\u4e0d\u8db3\u201d\u96be\u9898',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097480.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u5929\u836f\u80a1\u4efd\u62df11.59\u4ebf\u5143\u6536\u8d2d\u91d1\u8000\u836f\u4e1a62%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097481.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:18',
 'title': u'\u201c\u5bfc\u706b\u7d22\u201d\u5357\u73bbA\u80a1\u6743\u8ba1\u5212\u7ec6\u8282\u62ab\u9732 6.2\u5143\u6fc0\u52b1\u4ef7\u4ec5\u4e3a\u5f53\u65f6\u5e02\u4ef7\u4e00\u534a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097485.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:25',
 'title': u'\u5357\u73bbA\u9ad8\u7ba1\u96c6\u4f53\u79bb\u804c\u5185\u5e55\uff1a\u59da\u632f\u534e\u6ca1\u63a5\u524d\u8463\u4e8b\u957f\u66fe\u5357\u7535\u8bdd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097497.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u4e50\u89c6\u9ad8\u5c42\u56de\u5e94\u201c\u5e9e\u6c0f\u9a97\u5c40\u8bf4\u201d \uff1a\u6b63\u5168\u529b\u4ee5\u8d74\u628a\u94b1\u8f6c\u5230\u7f8e\u56fd\u6c7d\u8f66\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097512.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-16 19:41',
 'title': u'\u5e7f\u7530\u96c6\u56e2\u505c\u724c\u7b79\u5212\u6536\u8d2d\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161116_5096431.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u534e\u661f\u521b\u4e1a\u7ec8\u6b62\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097476.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:56',
 'title': u'\u5929\u9f99\u96c6\u56e2\uff1a2016\u5e74\u5ea6\u62df10\u8f6c15',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097518.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u5357\u73bbA17\u65e5\u518d\u906d\u6df1\u4ea4\u6240\u95ee\u8be2 \u8981\u6c42\u8bf4\u660e\u524d\u6d77\u4eba\u5bff\u662f\u5426\u5e72\u9884\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097493.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 17:19',
 'title': u'\u77f3\u69b4\u96c6\u56e2\u6536\u5173\u6ce8\u51fd\uff1a\u56e0\u5728\u975e\u6307\u5b9a\u5a92\u4f53\u4e0a\u53d1\u58f0',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097479.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097558.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:24',
 'title': u'\u5929\u9f99\u96c6\u56e2\uff1a2016\u5e74\u5ea6\u62df10\u8f6c15\u6d3e0.5 \u591a\u4f4d\u80a1\u4e1c\u62df\u5de8\u91cf\u51cf\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097511.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097675.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097560.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:57',
 'title': u'\u90d1\u7164\u673a\u56e0\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4\u4e8b\u9879\u5c06\u4e0a\u4f1a17\u65e5\u5f00\u5e02\u8d77\u505c\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097519.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:50',
 'title': u'11\u5bb6\u4e0a\u5e02\u516c\u53f8\u62df\u53c2\u4e0e\u7b79\u529e\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097528.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:10',
 'title': u'\u5c71\u4e1c\u534e\u9e4f\u516c\u544a\u9ad8\u9001\u8f6c\u7cfb\u8bef\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097530.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:06',
 'title': u'\u4e2d\u901a\u56fd\u810911\u670818\u65e5\u7f51\u4e0a\u7533\u8d2d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097532.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:58',
 'title': u'\u82cf\u5b81\u6613\u8d2d\u201c\u7b11\u503e\u57ce\u201d\u63ed\u5e55',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097525.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097558.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:35',
 'title': u'\u65d7\u6ee8\u96c6\u56e2\u56e0\u5357\u73bbA\u4eba\u4e8b\u53d8\u52a8\u906d\u4e0a\u4ea4\u6240\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097558.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097565.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097678.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097675.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:03',
 'title': u'\u6c42\u89e3\u5357\u73bb\u4e71\u5c40\uff1a\u5171\u8d62\u987b\u5b88\u89c4\u5219\u5efa\u673a\u5236',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097675.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097560.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:38',
 'title': u'\u9686\u946b\u901a\u7528\uff1a\u62df4111\u4e07\u6b27\u5143\u83b7\u53d6\u610f\u5927\u5229\u516c\u53f8\u7ea667%\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097560.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:10',
 'title': u'\u5b9d\u80fd\u7cfb\u5168\u9762\u63a5\u7ba1\u5357\u73bb \u76d1\u7ba1\u90e8\u95e8\u8fde\u53d1\u5173\u6ce8\u51fd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097696.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:31',
 'title': u'\u76d1\u7ba1\u201c\u96f7\u9706\u51fa\u51fb\u201d\u623f\u5730\u4ea7\u4fe1\u6258\u4e1a\u52a1 \u4fe1\u6258\u516c\u53f8\u4e25\u9635\u4ee5\u5f85',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097746.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:30',
 'title': u'\u4e07\u79d1\u80a1\u6743\u4e89\u593a\u6218\u6108\u6f14\u6108\u70c8\uff1a\u6052\u5927\u7ee7\u7eed\u589e\u6301 \u4e2d\u7b56\u5bcc\u6c47\u201c\u62cd\u9a6c\u6740\u5165\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097733.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097565.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 21:39',
 'title': u'\u79cb\u6797\u96c6\u56e2\uff1a\u62df1\u4ebf\u5143\u5728\u6df1\u5733\u8bbe\u7acb\u73e0\u5b9d\u7ecf\u8425\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097565.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097678.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:04',
 'title': u'\u56fd\u4f01\u6539\u9769\u201c\u94c1\u5854\u6a21\u5f0f\u201d\uff1a\u201c\u91cd\u7ec4 \u4e0a\u5e02\u201d\u76d8\u6d3b\u56fd\u6709\u8d44\u672c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097678.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:07',
 'title': u'\u5357\u73bbA\u539f\u9ad8\u7ba1\u4e0e\u524d\u6d77\u4eba\u5bff\u7ea0\u7eb7\u518d\u8d77\u6ce2\u6f9c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097689.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:32',
 'title': u'\u5b89\u90a6\u51fa\u624b\uff01113\u4ebf\u5143\u4e3e\u724c\u4e2d\u56fd\u5efa\u7b51\u56fe\u4e2a\u5565\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097745.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:09',
 'title': u'\u524d\u6d77\u4eba\u5bff\u5357\u73bbA\u8f9e\u804c\u9ad8\u7ba1\u5404\u6267\u4e00\u8bcd \u5b9d\u80fd\u7cfb\u9762\u4e34\u7ba1\u7406\u6311\u6218',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097694.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:10',
 'title': u'\u5230\u5e95\u662f\u8c01"\u73bb\u7483\u5fc3"\uff1f\u5357\u73bbA\u98ce\u6ce2\u591a\u65b9\u5404\u6267\u4e00\u8bcd\u6ce2\u6f9c\u4e0d\u65ad',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097713.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:32',
 'title': u'\u4e0a\u5e02\u516c\u53f8\u5e76\u8d2d\u95ef\u5173\u5b58\u4e09\u5927\u6740\u5668 \u63a7\u5236\u6743\u6210\u5173\u6ce8\u65b0\u7126\u70b9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097742.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:45',
 'title': u'\u957f\u57ce\u52a8\u6f2b\u51fa\u552e\u5723\u8fbe\u7126\u5316 \u62d6\u6b20\u8d27\u6b3e\u5ba2\u6237\u5fb7\u80dc\u96c6\u56e28\u6298\u63a5\u76d8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097774.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:47',
 'title': u'A\u80a1\u201c\u58f3\u201d\u751f\u610f\u98ce\u751f\u6c34\u8d77 \u5404\u8def\u8d44\u672c\u7eb7\u5230\u201c\u7897\u91cc\u6765\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097783.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:56',
 'title': u'\u906d\u9047\u5f3a\u52bf\u5165\u4e3b\u201c\u540e\u9057\u75c7\u201d \u5916\u6765\u8d44\u672c\u5982\u4f55\u6597\u800c\u4e0d\u7834',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097796.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:43',
 'title': u'\u91cd\u6574\u8ba1\u5212\u83b7\u901a\u8fc7 *ST\u4e91\u7ef4\u4fdd\u58f3\u9700\u626d\u8f6c\u8fd110\u4ebf\u5de8\u4e8f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097767.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_4.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:43',
 'title': u'\u6b4c\u534e\u6709\u7ebf\u4e09\u5b63\u5ea6\u4e1a\u7ee9\u8868\u73b0\u62a2\u773c \u65b0\u5a92\u4f53\u8f6c\u578b\u6548\u679c\u663e\u73b0',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097768.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 11:01',
 'title': u'\u5357\u73bbA\u79bb\u804c\u9ad8\u7ba1\u79f0\u96c6\u4f53\u8df3\u69fd\u81f3\u65d7\u6ee8\u96c6\u56e2\u662f\u8c23\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5096992.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:42',
 'title': u'\u529b\u5e06\u80a1\u4efd\u62df1.7\u4ebf\u5143\u5265\u79bb\u5b89\u8bda\u4fdd\u9669\u80a1\u4efd \u6536\u7f29\u91d1\u878d\u201c\u8f93\u8840\u201d\u65b0\u80fd\u6e90',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097765.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:44',
 'title': u'28\u4ebf\u9a70\u63f4\u8d5b\u7ef4\u7834\u4ea7\u91cd\u6574 \u6613\u6210\u65b0\u80fd\u8bd5\u6c34\u53e6\u7c7b\u201c\u503a\u8f6c\u80a1\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097775.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:47',
 'title': u'\u201c\u95e8\u53e3\u7684\u91ce\u86ee\u4eba\u201d\u518d\u73b0 \u6da8\u505c\u7684\u5357\u73bbA\u80cc\u540e\u8fd8\u6709\u8fd9\u4e9b\u65e0\u4e3b\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097776.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u5b9c\u660c\u4ea4\u8fd0\u5b50\u516c\u53f8\u4e24\u5904\u623f\u5c4b\u5c06\u88ab\u5f81\u6536',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097051.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:57',
 'title': u'\u6d77\u6da6\u5149\u4f0f\u4ea7\u4e1a\u57fa\u91d1\u8ba1\u5212\u88ab\u95ee\u8be2 \u671d\u4ee4\u5915\u6539\u4e14\u8bbe\u4e0d\u5e73\u7b49\u6761\u7ea6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097797.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 08:57',
 'title': u'\u9996\u6279\u5e74\u62a5\u201c\u9884\u9001\u8f6c\u201d\u65b9\u6848\u51fa\u7089',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097798.html'}
2016-11-21 14:42:19 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:19 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 16:26',
 'title': u'11\u670818\u65e5\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a\uff1a\u4e07\u79d1A\uff1a\u6536\u8d2d\u524d\u6d77\u56fd\u9645\u65b9\u6848\u5c1a\u672a\u8fbe\u6210\u5171\u8bc6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097247.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u6d59\u5bcc\u63a7\u80a1\u4e2d\u68071.3\u4ebf\u5143\u8001\u631d\u6c34\u7535\u7ad9\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097050.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:51',
 'title': u'\u795e\u601d\u7535\u5b50\u80a1\u4e1c\u51cf\u6301330\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097386.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:42',
 'title': u'\u5317\u5df4\u4f20\u5a92\u3001\u4f17\u4e1a\u8fbe\u7ec8\u6b62\u53c2\u4e0e\u683c\u529b\u7535\u5668\u5b9a\u589e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097049.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:41',
 'title': u'11\u670817\u65e5\u5348\u95f4\u516c\u544a:\u5317\u5df4\u4f20\u5a92\u3001\u4f17\u4e1a\u8fbe\u7ec8\u6b62\u53c2\u4e0e\u683c\u529b\u7535\u5668\u5b9a\u589e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097060.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 12:45',
 'title': u'\u6b65\u68ee\u80a1\u4efd\u7ec8\u6b62\u670d\u88c5\u52df\u6295\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097059.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:43',
 'title': u'\u6d77\u8fbe\u80a1\u4efd\u80a1\u4e1c\u51cf\u6301390\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097376.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:48',
 'title': u'\u534e\u8679\u8ba1\u901a\u80a1\u4e1c\u8ba1\u5212\u51cf\u6301\u4e0d\u8d85\u8fc7168\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097381.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:48',
 'title': u'\u534e\u529b\u521b\u901a\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301590\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097382.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:54',
 'title': u'\u529b\u6e90\u4fe1\u606f\u83b7\u5f97\u653f\u5e9c\u8865\u52a9900\u4e07\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097387.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:01',
 'title': u'\u7279\u9510\u5fb7\u5168\u8d44\u5b50\u516c\u53f8\u7b7e\u8ba23.52\u4ebf\u5143EPC\u603b\u627f\u5305\u5408\u540c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097397.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:56',
 'title': u'\u6052\u6cf0\u827e\u666e\u62df\u6295\u8d448000\u4e07\u5143\u8bbe\u7acb\u5317\u4eac\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097390.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:55',
 'title': u'\u96f7\u66fc\u80a1\u4efd\u5b9e\u9645\u63a7\u5236\u4eba\u51cf\u6301600\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097389.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:00',
 'title': u'\u56db\u65b9\u7cbe\u521b\u6301\u80a15%\u4ee5\u4e0a\u80a1\u4e1c\u51cf\u6301150\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097395.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:56',
 'title': u'\u6d77\u8054\u8baf\u7b2c\u4e8c\u5927\u80a1\u4e1c\u51cf\u6301500\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097392.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:54',
 'title': u'\u56fd\u796f\u73af\u4fdd\u4e2d\u68072.6\u4ebf\u5143\u8499\u57ce\u6c61\u6c34\u5382\u7f51\u4e00\u4f53\u5316PPP\u9879\u76ee',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097388.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 18:39',
 'title': u'\u4e1c\u963f\u963f\u80f6\u4e09\u5927\u4e3b\u5bfc\u4ea7\u54c1\u5168\u7ebf\u63d0\u4ef7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097394.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:04',
 'title': u'11\u670818\u65e5\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1\uff1a\u6c49\u9f0e\u5b87\u4f51\u62df13\u4ebf\u5143\u5e76\u8d2d\u6e38\u620f\u516c\u53f8 \u5e03\u5c40\u5a31\u4e50\u4ea7\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097403.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:10',
 'title': u'\u6a2a\u6cb3\u6a21\u5177\u6536\u5230796\u4e07\u5143\u653f\u5e9c\u8865\u8d34',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097406.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:15',
 'title': u'11\u670818\u65e5\u91cd\u8981\u589e\u51cf\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097413.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:10',
 'title': u'\u5bcc\u745e\u7279\u88c5\u7b7e\u7f72\u5408\u4f5c\u5f00\u53d1\u6846\u67b6\u534f\u8bae',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097407.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:05',
 'title': u'\u4e09\u529b\u58eb\u667a\u80fd\u88c5\u5907\u5c55\u4f1a\u60ca\u8273\u4eae\u76f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097452.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:03',
 'title': u'\u65b0\u6e56\u4e2d\u5b9d\uff1a\u62df7.1\u4ebf\u5143\u8f6c\u8ba9\u5b50\u516c\u53f8\u65b0\u6e56\u5b9d\u534e65%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097457.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:13',
 'title': u'11\u670818\u65e5\u9ad8\u9001\u8f6c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097412.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:13',
 'title': u'11\u670818\u65e5\u91cd\u5927\u505c\u590d\u724c\uff1a\u534e\u5cf0\u8d85\u7ea4\u5468\u4e94\u590d\u724c \u62df18\u4ebf\u5143\u6536\u8d2d\u79fb\u52a8\u652f\u4ed8\u670d\u52a1\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097411.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:00',
 'title': u'\u5168\u7b51\u80a1\u4efd\uff1a\u7b7e\u8ba2\u5408\u4f5c\u6846\u67b6\u534f\u8bae \u6709\u671b\u83b7\u5f97\u5927\u91cf\u8ba2\u5355',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097454.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:09',
 'title': u'\u4e2d\u79d1\u66d9\u5149\u80a1\u4e1c\u5929\u5bcc\u521b\u6295\u51cf\u63013.11%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097459.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:09',
 'title': u'\u6885\u6cf0\u8bfa\u62df3000\u4e07\u5143\u6536\u8d2d\u9f0e\u5143\u4fe1\u5e7f49%\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097461.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:51',
 'title': u'\u6b65\u68ee\u80a1\u4efd\uff1a\u8fdb\u519b\u91d1\u878d\u79d1\u6280  \u6398\u91d1\u4e07\u4ebf\u5e02\u573a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097455.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 19:45',
 'title': u'\u4e2d\u56fd\u8fdc\u6d0b\uff1a\u62df7234\u4e07\u5143\u6536\u8d2d\u4e2d\u8fdc\u5e0c\u814a\u7b49\u5883\u5916\u516c\u53f8\u90e8\u5206\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097446.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u7ef4\u5c14\u522918\u65e5\u590d\u724c \u62df\u6536\u8d2d\u4e24\u5bb6\u73af\u4fdd\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097473.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_5.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:10',
 'title': u'\u5927\u5bcc\u79d1\u62802016\u5e74\u5ea6\u4e1a\u7ee9\u9884\u589e108%-137%',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097464.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u7528\u53cb\u7f51\u7edc\u62df\u53d1\u8d77\u8bbe\u7acb\u5317\u4eac\u4e2d\u5173\u6751\u94f6\u884c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097471.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u4e07\u79d1A\uff1a\u6536\u8d2d\u524d\u6d77\u56fd\u9645\u65b9\u6848\u5c1a\u672a\u8fbe\u6210\u5171\u8bc6',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097474.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u6613\u6210\u65b0\u80fd\u62df\u903e28\u4ebf\u5143\u6536\u8d2d\u6c5f\u897f\u8d5b\u7ef4\u53ca\u65b0\u4f59\u8d5b\u7ef4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097470.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:10',
 'title': u'\u65b0\u6e56\u4e2d\u5b9d\u5168\u8d44\u5b50\u516c\u53f8\u65a5\u903e14.69\u4ebf\u5143\u62ff\u5730',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097463.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5b87\u901a\u5ba2\u8f66\u56de\u5e94\u4e0b\u8dcc\u539f\u56e0\uff1a\u6216\u56e0\u65b0\u80fd\u6e90\u6c7d\u8f66\u8865\u8d34\u53d6\u6d88\u4f20\u95fb',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097943.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-17 20:14',
 'title': u'\u70bc\u77f3\u6709\u8272\u505c\u724c\u7b79\u5212\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161117_5097472.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:26',
 'title': u'\u4e0a\u6d77\u94f6\u884c\u4e0a\u5e02\u7b2c\u4e09\u5929\u5373\u6253\u5f00\u6da8\u505c\u677f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097937.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5eb7\u5f97\u65b0\uff1a\u4e0e\u534e\u4e3a\u7ec8\u6b62\u88f8\u773c3D\u4e1a\u52a1\u5408\u4f5c\u4f20\u95fb\u4e0d\u5b9e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097948.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:57',
 'title': u'\u4e0d\u6b62\u5357\u73bbA \u8fd9\u4e9b\u516c\u53f8\u540c\u6837\u7206\u53d1\u4eba\u4e8b\u5730\u9707\uff01(\u9644\u540d\u5355)',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097988.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u6c49\u738b\u79d1\u6280\u62df\u63a8\u80a1\u6743\u6fc0\u52b1\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098023.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u4e07\u79d1A\u5927\u6da86% \u518d\u521b\u5386\u53f2\u65b0\u9ad8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097947.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098101.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098211.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098342.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:23',
 'title': u'\u817e\u90a6\u65c5\u6e38\u96c6\u56e2\u4e1a\u52a1\u5c06\u5411\u76ee\u7684\u5730\u5ef6\u4f38',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098066.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098346.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:22',
 'title': u'\u6842\u53d1\u7965\u4eca\u65e5\u4e0a\u5e02',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098067.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5fb7\u8d5b\u7535\u6c60\u900f\u9732\u516c\u53f8\u4e3a\u534e\u4e3aMate9\u63d0\u4f9b\u7535\u6c60\u4ea7\u54c1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097964.html'}
2016-11-21 14:42:20 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 09:23',
 'title': u'11\u670818\u65e5\u5348\u95f4\u516c\u544a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098025.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098358.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098348.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098347.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u5409\u827e\u79d1\u6280\u7ec8\u6b62\u6536\u8d2d\u54c8\u8428\u514b\u65af\u5766\u70bc\u5316\u4f01\u4e1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098024.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u5c71\u4e1c\u534e\u9e4f\u91cd\u5927\u4e8b\u9879\u4e34\u505c \u9ad8\u9001\u8f6c\u662f\u8bef\u4f20',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097966.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:05',
 'title': u'\u666e\u90a6\u80a1\u4efd\u4e2d\u6807\u90d1\u5ddePPP\u9879\u76ee \u6709\u671b\u63d0\u632f\u5341\u5e74\u4e1a\u7ee9',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098022.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098101.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 14:19',
 'title': u'\u4e09\u5927\u80fd\u6e90\u96c6\u56e2\u5171\u8bbe\u6295\u8d44\u5e73\u53f0 \u52a9\u63a8\u80fd\u6e90\u8f6c\u578b\u5347\u7ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098101.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098211.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 16:16',
 'title': u'\u4e0b\u5468\u53ef\u80fd\u5f71\u54cd\u80a1\u4ef7\u91cd\u8981\u516c\u544a\uff1a\u5339\u51f8\u5339\u8463\u4e8b\u957f\u56e0\u5de5\u4f5c\u539f\u56e0\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098211.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098068.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098365.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098342.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 17:50',
 'title': u'\u51af\u5c0f\u521a\u624b\u6495\u738b\u5065\u6797\u80cc\u540e\uff1a\u5929\u4ef7\u7968\u8865\u4e0e\u4e07\u8fbe\u5784\u65ad',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098342.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098345.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 10:27',
 'title': u'\u8fce\u5b89\u90a6\u4e3e\u724c \u4e2d\u56fd\u5efa\u7b51\u4eca\u65e5\u9ad8\u5f00\u4f4e\u8d70',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5097968.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098346.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:00',
 'title': u'\u4e1c\u963f\u963f\u80f6\u9a74\u813e\u6c14\u96be\u6539 \u65d7\u4e0b\u4ea7\u54c16\u5e74\u6da8\u4ef712\u6b21',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098346.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098367.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098358.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 17:59',
 'title': u'\u4e0b\u5468\u5229\u597d\u516c\u544a\u8fce\u7206\u53d1\uff1a\u4e1c\u65b9\u7f51\u7edc\u62df35\u4ebf\u6536\u8d2d\u4e09\u5f71\u89c6\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098358.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098349.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098348.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:14',
 'title': u'\u4e0b\u5468\u91cd\u5927\u505c\u590d\u724c\uff1a\u8363\u4e4b\u8054\u7ec8\u6b62\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4 \u80a1\u7968\u590d\u724c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098348.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098347.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:00',
 'title': u'\u9664\u683c\u529b\u5916132\u5bb6\u4e0a\u5e02\u516c\u53f8\u91cd\u7ec4\u5931\u8d25\uff1a\u9ad8\u6ea2\u4ef7\u548c\u9ad8\u4e1a\u7ee9\u627f\u8bfa\u662f\u96f7\u533a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098347.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098368.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098369.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098068.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 13:37',
 'title': u'\u56db\u7ef4\u56fe\u65b0:\u4e0e\u5b9d\u9a6c\u5728\u5bfc\u822a\u9886\u57df\u5408\u4f5c\u5bc6\u5207',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098068.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098364.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098365.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:04',
 'title': u'\u4e2d\u822a\u5149\u7535\u80a1\u4e1c\u4e2d\u56fd\u7a7a\u7a7a\u5bfc\u5f39\u7814\u7a76\u9662\u62df\u51cf\u6301\u4e0d\u903e300\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098365.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098345.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 17:59',
 'title': u'\u6052\u5927\u501f\u58f3\u6df1\u6df1\u623f\u8fdb\u5165\u5b9e\u65bd\u9636\u6bb5 \u91cd\u7ec4\u4e2d\u4ecb\u673a\u6784\u5df2\u9009\u5b9a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098345.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098367.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:53',
 'title': u'\u4e2d\u5929\u79d1\u6280\u201c\u6c34\u4e0b\u4e92\u8054\u7f51\u201d\u4eae\u76f8\u4e16\u754c\u4e92\u8054\u7f51\u5927\u4f1a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098367.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098349.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:14',
 'title': u'11\u670818\u65e5\u665a\u95f4\u91cd\u8981\u589e\u51cf\u6301',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098349.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098368.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:47',
 'title': u'\u5e7f\u6c7d\u96c6\u56e2\u4eae\u76f82016\u7b2c14\u5c4a\u4e2d\u56fd\uff08\u5e7f\u5dde\uff09\u56fd\u9645\u6c7d\u8f66\u5c55',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098368.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098371.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098369.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:06',
 'title': u'\u5411\u65e5\u84755.2\u4ebf\u5143\u6536\u8d2d\u5965\u80fd\u7535\u6e90',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098369.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098373.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098364.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:00',
 'title': u'\u4eca\u4e16\u7f18\u526f\u603b\u7ecf\u7406\u8fdd\u89c4\u51cf\u6301\u906d\u5904\u7f5a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098364.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098375.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098372.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098374.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098376.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098371.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u5929\u76ee\u836f\u4e1a\u906d\u80a1\u4e1c\u51cf\u6301348\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098371.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098373.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u542f\u8fea\u53e4\u6c49\u5b9a\u589e\u7533\u8bf7\u83b7\u8bc1\u76d1\u4f1a\u5ba1\u6838\u901a\u8fc7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098373.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098420.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098366.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098377.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098375.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u5b9d\u94a2\u80a1\u4efd\u63a7\u80a1\u80a1\u4e1c\u66f4\u540d\u201c\u5b9d\u6b66\u94a2\u94c1\u96c6\u56e2\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098375.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098372.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u4e1c\u65b9\u7f51\u7edc\u62df35\u4ebf\u5143\u6536\u8d2d\u5f71\u89c6\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098372.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098374.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u4e1c\u5317\u5236\u836f\u906d\u80a1\u4e1c\u7d2f\u8ba1\u51cf\u6301470\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098374.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098376.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u4e00\u6c7d\u8f7f\u8f66\u62df4.28\u4ebf\u5143\u8f6c\u8ba9\u7ea2\u65d7\u54c1\u724c\u8d44\u4ea7',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098376.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098378.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098426.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098420.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:50',
 'title': u'\u5feb\u4e50\u8d2d\u4e24\u80a1\u4e1c\u8ba1\u5212\u51cf\u6301\u4e0d\u8d85\u8fc74530\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098420.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098366.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:05',
 'title': u'\u83ab\u9ad8\u80a1\u4efd\uff1a\u5b81\u6ce2\u5b8f\u521b\u53ca\u6c38\u65b0\u534e\u97f5\u6d89\u5acc\u8d85\u6bd4\u4f8b\u6301\u6709\u516c\u53f8\u80a1\u4efd\u672a\u516c\u544a\u906d\u7acb\u6848\u8c03\u67e5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098366.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098377.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u9e3f\u8def\u94a2\u6784\u9ad8\u7ba1\u51cf\u6301\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098377.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098419.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098504.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098423.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098378.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u8363\u4e4b\u8054\u7ec8\u6b62\u7b79\u5212\u91cd\u5927\u8d44\u4ea7\u91cd\u7ec4',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098378.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098425.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098379.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098426.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u5b9d\u5149\u80a1\u4efd\u80a1\u4e1c\u589e\u6301223.9\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098426.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098424.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098427.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098419.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:49',
 'title': u'\u5929\u76ee\u836f\u4e1a\u80a1\u4e1c\u5e73\u5b89\u5927\u534e\u6c47\u901a\u4ee3\u8868\u521b\u76c8 4 \u53f7\u51cf\u6301347.91\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098419.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098504.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 21:28',
 'title': u'\u6881\u5149\u4f1f\u5bf9\u6df1\u5733\u534e\u5f3a\u53d1\u8d77\u8981\u7ea6\u6536\u8d2d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098504.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098423.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u96f7\u67cf\u79d1\u6280\u63a7\u80a1\u80a1\u4e1c\u51cf\u63012.6%\u516c\u53f8\u80a1\u4efd',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098423.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098425.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u5929\u539f\u96c6\u56e2\u505c\u724c\u7b79\u5212\u975e\u516c\u5f00\u53d1\u884c\u80a1\u7968\u4e8b\u9879',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098425.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098379.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:09',
 'title': u'\u660e\u6cf0\u94dd\u4e1a\u906d\u80a1\u4e1c\u7d2f\u8ba1\u51cf\u6301200\u4e07\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098379.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098514.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098424.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u5339\u51f8\u5339\u8463\u4e8b\u957f\u56e0\u5de5\u4f5c\u539f\u56e0\u8f9e\u804c',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098424.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098594.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098595.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098427.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 19:54',
 'title': u'\u534e\u5f55\u767e\u7eb3\u80a1\u4e1c\u62ab\u9732\u51cf\u6301\u8ba1\u5212',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098427.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098644.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098645.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098649.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098514.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 21:31',
 'title': u'\u731b\u72ee\u79d1\u6280\uff1a\u62df1\u4ebf\u5143\u5728\u8944\u9633\u8bbe\u7acb\u65b0\u80fd\u6e90\u516c\u53f8',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098514.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098594.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:26',
 'title': u'\u8bc1\u76d1\u4f1a\u5bf9\u516d\u5b97\u6848\u4ef6\u4f5c\u51fa\u884c\u653f\u5904\u7f5a',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098594.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098647.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098595.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:28',
 'title': u'\u5b89\u90a6\u8d44\u4ea7\u62df\u589e\u6301\u4e2d\u56fd\u5efa\u7b511\u4ebf\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098595.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098644.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:48',
 'title': u'\u662f\u8d2a\u5a6a\u8fd8\u662f\u65e0\u77e5\uff1f\u4eca\u4e16\u7f18\u9ad8\u7ba1\u8fdd\u89c4\u51cf\u6301\u906d\u901a\u62a5',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098644.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098696.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098645.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:49',
 'title': u'\u5bb6\u4e50\u798f\u4e1a\u7ee9\u8fde\u4e8f\u80a1\u6743\u518d\u906d\u629b\u552e \u8fbd\u5b81\u6210\u59274.2\u4ebf\u51fa\u552e\u5176\u5168\u90e8\u80a1\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098645.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098649.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:51',
 'title': u'\u5b89\u90a6\u6301A\u80a1\u5df2\u8d851700\u4ebf\uff01\u4e3e\u724c\u4e2d\u56fd\u5efa\u7b51\u6d6e\u76c814.49\u4ebf \u8fd8\u5c06\u589e\u63011\u4ebf\u80a1',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098649.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098656.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098685.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098657.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098706.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098707.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098647.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:51',
 'title': u'\u91cd\u8981\u4ea4\u6613\u4fe1\u62ab\u4e0d\u5230\u4f4d \u5168\u7b51\u80a1\u4efd\u9686\u946b\u901a\u7528\u906d\u95ee\u8be2',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098647.html'}
2016-11-21 14:42:21 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098696.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:13',
 'title': u'\u5357\u73bb\u4e1a\u7ee9\u5982\u4f55\u4fdd\u969c\uff1f\u804c\u4e1a\u7ecf\u7406\u4eba\u7d20\u8d28\u4f55\u5728\uff1f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098696.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098672.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098656.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 08:59',
 'title': u'\u795e\u5f00\u80a1\u4efd\u80a1\u6743\u4e4b\u4e89\u672a\u6b47 \u80a1\u4e1c\u4f1a\u73b0\u573a\u8463\u4e8b\u201c\u53d1\u96be\u201d\u8d28\u7591\u8bae\u6848\u5408\u89c4\u6027',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098656.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098685.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:08',
 'title': u'\u5357\u73bbA\u9a7b\u4eac\u201c\u5e73\u6c11\u201d\uff1a\u6ce2\u53ca\u4e0e\u5426\u9700\u89c2\u671b\u5929\u6d25\u5de5\u5382',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098685.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098657.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:00',
 'title': u'\u6b65\u68ee\u53eb\u505c\u670d\u88c5\u52df\u6295\u73a9\u91d1\u670d \u4e1a\u7ee9\u4e00\u8def\u4e0b\u6ed1\u4e09\u6b21\u91cd\u7ec4\u5931\u8d25',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098657.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098706.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:16',
 'title': u'\u4e4c\u9547\u996d\u5c40\u201c\u5403\u51fa\u754c\u201d',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098706.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098707.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:17',
 'title': u'\u5ef6\u534e\u667a\u80fd\u5927\u80a1\u4e1c\u62df\u8f6c\u8ba9\u6295\u7968\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098707.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098740.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098741.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098742.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098672.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:03',
 'title': u'\u51af\u5c0f\u521a\u53eb\u677f\u4e07\u8fbe\uff1a\u738b\u601d\u806a\u9694\u7a7a\u5e94\u6218 \u534e\u8c0a\u8eba\u67aa\u7684\u80cc\u540e',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098672.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098686.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_1.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098740.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:41',
 'title': u'\u963f\u91cc\u5df4\u5df4\u7a7a\u964d\u4e09\u6c5f\u8d2d\u7269 \u4e0a\u4ea4\u6240\u95ee\u5176\u662f\u5426\u610f\u5728\u63a7\u5236\u6743',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098740.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Crawled (200) <GET http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098363.html> (referer: http://www.cs.com.cn/ssgs/gsxw/index_2.html)
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098741.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:41',
 'title': u'\u946b\u79d1\u6750\u659923\u4ebf\u542f\u52a8\u53cc\u8de8\u5e76\u8d2d \u8de8\u56fd\u8de8\u754c\u201c\u8c6a\u8d4c\u201d\u8f6c\u578b\u9762\u4e34\u6311\u6218',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098741.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098742.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:41',
 'title': u'A\u80a1118\u5bb6\u516c\u53f8\u624e\u5806\u7535\u52a8\u8f66 \u52a8\u529b\u7535\u6c60\u884c\u4e1a\u5c06\u73b0\u4e95\u55b7\u5f0f\u589e\u957f',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098742.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098686.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-19 09:07',
 'title': u'\u63a2\u8bbf\u5357\u73bbA\u6210\u90fd\u5206\u516c\u53f8\uff1a\u8463\u4e8b\u957f\u6302\u9774\u603b\u7ecf\u7406\u4ecd\u5728\u5c97',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161119_5098686.html'}
2016-11-21 14:42:22 [scrapy] DEBUG: Scraped from <200 http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098363.html>
{'content': u'<div class="Dte',
 'time': u'2016-11-18 18:59',
 'title': u'\u7231\u5c14\u773c\u79d1\u62df\u5bf9\u5e7f\u5dde\u7231\u5c14\u589e\u8d443800\u4e07\u5143',
 'url': 'http://www.cs.com.cn/ssgs/gsxw/201611/t20161118_5098363.html'}
2016-11-21 14:42:22 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:42:22 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 109133,
 'downloader/request_count': 363,
 'downloader/request_method_count/GET': 363,
 'downloader/response_bytes': 3282849,
 'downloader/response_count': 363,
 'downloader/response_status_count/200': 363,
 'dupefilter/filtered': 37,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 42, 22, 324490),
 'item_scraped_count': 352,
 'log_count/DEBUG': 717,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 363,
 'scheduler/dequeued': 363,
 'scheduler/dequeued/memory': 363,
 'scheduler/enqueued': 363,
 'scheduler/enqueued/memory': 363,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 6, 42, 14, 264851)}
2016-11-21 14:42:22 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:50:01 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:50:01 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:50:01 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:50:01 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:50:01 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:50:01 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:50:01 [scrapy] INFO: Spider opened
2016-11-21 14:50:01 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:50:10 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/gszt/20150325_61642/> (referer: http://www.cs.com.cn/ssgs/hyzx/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 46, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:50:11 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:50:11 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 117982,
 'downloader/request_count': 392,
 'downloader/request_method_count/GET': 392,
 'downloader/response_bytes': 3689843,
 'downloader/response_count': 392,
 'downloader/response_status_count/200': 392,
 'dupefilter/filtered': 8,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 50, 11, 470547),
 'item_scraped_count': 381,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 392,
 'scheduler/dequeued': 392,
 'scheduler/dequeued/memory': 392,
 'scheduler/enqueued': 392,
 'scheduler/enqueued/memory': 392,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 6, 50, 1, 375334)}
2016-11-21 14:50:11 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:54:48 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:54:48 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:54:48 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:54:48 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:54:48 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:54:48 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:54:48 [scrapy] INFO: Spider opened
2016-11-21 14:54:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:54:57 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:54:57 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 99134,
 'downloader/request_count': 330,
 'downloader/request_method_count/GET': 330,
 'downloader/response_bytes': 2969745,
 'downloader/response_count': 330,
 'downloader/response_status_count/200': 330,
 'dupefilter/filtered': 70,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 54, 57, 93600),
 'item_scraped_count': 320,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 330,
 'scheduler/dequeued': 330,
 'scheduler/dequeued/memory': 330,
 'scheduler/enqueued': 330,
 'scheduler/enqueued/memory': 330,
 'start_time': datetime.datetime(2016, 11, 21, 6, 54, 48, 877041)}
2016-11-21 14:54:57 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:56:48 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:56:48 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:56:48 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:56:48 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:56:48 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:56:48 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:56:48 [scrapy] INFO: Spider opened
2016-11-21 14:56:48 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098844.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098845.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098846.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099007.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098847.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099013.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099006.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099093.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099098.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099095.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099099.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099133.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099137.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099200.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099130.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:48 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099132.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090734.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090602.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090731.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090686.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090663.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090618.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090779.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090730.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090406.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090503.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090516.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090515.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090577.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090526.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090510.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090584.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090504.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090519.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091008.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090553.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090927.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091009.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090912.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091012.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5090833.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091007.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091015.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091064.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091264.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091023.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091078.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091187.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091355.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091360.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091283.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091583.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091426.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091642.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091363.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091705.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091431.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091579.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091609.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091704.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091710.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091890.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091707.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091904.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091708.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091747.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091757.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:49 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091907.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092043.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091981.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092034.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091914.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092050.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091968.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091365.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092285.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092046.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092336.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092457.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092060.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092220.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092325.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092070.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092080.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092396.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092310.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092531.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092401.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092522.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092405.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092570.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092518.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092574.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092512.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092513.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5096981.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092532.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092534.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097008.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092536.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097010.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092547.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5096980.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097011.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097089.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097009.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097007.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097180.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097186.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097185.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097189.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097194.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097193.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097207.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094273.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097192.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097181.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097188.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094340.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094293.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094378.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097210.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094296.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094391.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094402.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094393.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094299.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094434.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094342.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094351.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094392.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094555.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094554.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094394.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:50 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094608.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094430.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094644.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094741.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094911.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094472.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094736.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094616.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095148.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094905.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095135.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095136.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094982.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095152.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095124.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095013.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095183.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095134.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095012.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095147.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094975.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092937.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092939.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092995.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092960.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092941.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092963.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092996.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093108.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093111.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093159.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093105.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093155.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093101.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5093022.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093339.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5093032.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093154.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092961.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093453.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093309.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093278.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093427.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093451.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093509.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093510.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093507.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093454.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093457.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093512.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093511.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093517.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093508.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093518.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093520.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093514.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093513.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:51 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093516.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093515.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095347.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095330.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095244.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095389.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095195.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095294.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095316.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095374.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095354.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095356.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095455.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095431.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095355.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095357.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095621.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095363.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095436.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095605.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095862.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095622.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095625.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096039.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095929.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095856.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095863.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095613.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095936.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095958.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096233.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096280.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096192.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096092.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093521.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092575.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096286.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092609.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096142.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096287.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093519.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092576.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092631.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092524.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092530.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092537.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092684.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092249.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092533.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092670.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092535.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092568.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092786.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092726.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092915.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092727.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092728.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092801.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092914.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:52 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092917.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092926.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092741.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092930.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097639.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098836.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098835.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098838.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098837.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092925.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098839.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092931.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092935.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098840.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098841.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098843.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098842.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093522.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093525.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093523.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093524.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093580.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093569.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093649.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092916.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093526.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093527.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093728.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093867.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093598.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093650.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093869.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093870.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093623.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093871.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093872.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093616.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093873.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093885.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093733.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093986.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093874.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093987.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094073.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093921.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094121.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093939.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094114.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094188.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094141.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097337.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097320.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094116.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097409.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099202.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094139.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099195.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099248.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099289.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094184.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099290.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097401.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/cj/201611/t20161117_5097400.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099307.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099308.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099351.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099370.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099382.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099387.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099440.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099413.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099075.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097425.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097424.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097561.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097635.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097638.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097722.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097815.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097885.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097559.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097987.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097456.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097582.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097627.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097994.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098000.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097996.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098085.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:53 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098108.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097729.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097991.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098111.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098139.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098112.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098115.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097997.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098110.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098114.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098169.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098212.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098117.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098159.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:54 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098152.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:55 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093868.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:56:55 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:56:55 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 111348,
 'downloader/request_count': 375,
 'downloader/request_method_count/GET': 375,
 'downloader/response_bytes': 3357685,
 'downloader/response_count': 375,
 'downloader/response_status_count/200': 375,
 'dupefilter/filtered': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 56, 55, 461563),
 'item_scraped_count': 1,
 'log_count/ERROR': 364,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 375,
 'scheduler/dequeued': 375,
 'scheduler/dequeued/memory': 375,
 'scheduler/enqueued': 375,
 'scheduler/enqueued/memory': 375,
 'spider_exceptions/TypeError': 364,
 'start_time': datetime.datetime(2016, 11, 21, 6, 56, 48, 445407)}
2016-11-21 14:56:55 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:57:38 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:57:38 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:57:38 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:57:38 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:57:38 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:57:38 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:57:38 [scrapy] INFO: Spider opened
2016-11-21 14:57:38 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095347.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095294.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095389.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095316.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095354.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095330.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095356.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:38 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095363.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095436.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095455.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095355.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095357.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095374.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095431.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098845.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093526.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098846.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097185.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092728.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092727.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092741.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097089.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097181.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097189.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097425.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097011.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097180.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097188.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092786.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097193.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097192.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097210.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097186.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097194.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097207.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097320.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097337.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098837.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098835.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098838.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098836.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/cj/201611/t20161117_5097400.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097401.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098839.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098840.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097409.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097639.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098841.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097424.html> (referer: http://www.cs.com.cn/xwzx/hg/index_2.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098842.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098843.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091008.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098844.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091023.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091187.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091015.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091012.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091078.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091009.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161109_5091064.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091264.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091355.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091283.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091360.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098847.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092070.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091426.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091431.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091365.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099007.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099006.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091363.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099013.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099093.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099095.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099098.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099130.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099099.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099132.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:39 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099133.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092080.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099137.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099200.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099195.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099307.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099248.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099289.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099290.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099202.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099308.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099351.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099075.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099413.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099382.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099387.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099440.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099370.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091609.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091704.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091642.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091579.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091708.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091583.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091747.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091705.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091707.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091710.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091757.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091890.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091907.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091904.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091968.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091914.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092060.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5091981.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092043.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092046.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092034.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092285.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161110_5092050.html> (referer: http://www.cs.com.cn/xwzx/hg/index_9.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092457.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092220.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092310.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092531.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092512.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092325.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092401.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092336.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092396.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092405.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092513.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092518.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092574.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092522.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092532.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092534.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092575.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092570.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092536.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092547.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092576.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092530.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092609.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092631.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:40 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092684.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092524.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092537.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092670.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092535.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092726.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097559.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097561.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092249.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097456.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097627.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161117_5097582.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097635.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092801.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097638.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097722.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092914.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092916.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092917.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092915.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092925.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092926.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092931.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092930.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092937.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092935.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092939.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092941.html> (referer: http://www.cs.com.cn/xwzx/hg/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093525.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093519.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093523.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093521.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093522.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093524.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092961.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092960.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092963.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093105.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093108.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092995.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093159.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5093032.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092996.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5093022.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093155.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093339.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093154.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093111.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093101.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093352.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093309.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161112_5093278.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093454.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093427.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093453.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093451.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093457.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093508.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093507.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093510.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093509.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093513.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093514.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093515.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093511.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093517.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093512.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097729.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093516.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097885.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093520.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092533.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093518.html> (referer: http://www.cs.com.cn/xwzx/hg/index_6.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094608.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098000.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097815.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097987.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098085.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098108.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097994.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097991.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097997.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5097996.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161111_5092568.html> (referer: http://www.cs.com.cn/xwzx/hg/index_8.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098114.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098159.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098139.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098152.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098354.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098110.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098111.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098397.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098112.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/sylm/jsbd/201611/t20161118_5098393.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098115.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098400.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098402.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098169.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098404.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098117.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161119_5098667.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161119_5098653.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093580.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093569.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161118_5098212.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161113_5093527.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093598.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093650.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093616.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093869.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093871.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093728.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093623.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093872.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093733.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093885.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093867.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093870.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:42 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093649.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093874.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093921.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093873.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094073.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093939.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094114.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093868.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093986.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5093987.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094121.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094139.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094184.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094116.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094141.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094741.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094736.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094644.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094616.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094615.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161114_5094188.html> (referer: http://www.cs.com.cn/xwzx/hg/index_5.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094908.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095013.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095124.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094905.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095135.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095148.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094911.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095136.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094975.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5094982.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095012.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095183.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095244.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095134.html> (referer: http://www.cs.com.cn/xwzx/hg/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095147.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095152.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095621.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095613.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095862.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095605.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161115_5095195.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095929.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095856.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095622.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095625.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096039.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095863.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096286.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096192.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096092.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095936.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096280.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096142.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096233.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5095958.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161116_5096287.html> (referer: http://www.cs.com.cn/xwzx/hg/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 65, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:57:43 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:57:43 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 99650,
 'downloader/request_count': 336,
 'downloader/request_method_count/GET': 336,
 'downloader/response_bytes': 3053676,
 'downloader/response_count': 336,
 'downloader/response_status_count/200': 336,
 'dupefilter/filtered': 64,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 57, 43, 849075),
 'item_scraped_count': 1,
 'log_count/ERROR': 325,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 336,
 'scheduler/dequeued': 336,
 'scheduler/dequeued/memory': 336,
 'scheduler/enqueued': 336,
 'scheduler/enqueued/memory': 336,
 'spider_exceptions/TypeError': 325,
 'start_time': datetime.datetime(2016, 11, 21, 6, 57, 38, 537626)}
2016-11-21 14:57:43 [scrapy] INFO: Spider closed (finished)
2016-11-21 14:59:29 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 14:59:29 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 14:59:29 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 14:59:29 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 14:59:29 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 14:59:29 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 14:59:29 [scrapy] INFO: Spider opened
2016-11-21 14:59:29 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099007.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098842.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098847.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099006.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098844.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098846.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098845.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098843.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099013.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099093.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098835.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099099.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099130.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099132.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099133.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099098.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099095.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098836.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098837.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099137.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098839.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098841.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099200.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098838.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161120_5098840.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099202.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099195.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099248.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099289.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099290.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099307.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099308.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099351.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099370.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099382.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099387.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099413.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099440.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/hg/201611/t20161121_5099075.html> (referer: http://www.cs.com.cn/xwzx/hg/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 66, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 14:59:33 [scrapy] INFO: Closing spider (finished)
2016-11-21 14:59:33 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 11451,
 'downloader/request_count': 40,
 'downloader/request_method_count/GET': 40,
 'downloader/response_bytes': 397265,
 'downloader/response_count': 40,
 'downloader/response_status_count/200': 40,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 6, 59, 33, 135931),
 'log_count/ERROR': 39,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 40,
 'scheduler/dequeued': 40,
 'scheduler/dequeued/memory': 40,
 'scheduler/enqueued': 40,
 'scheduler/enqueued/memory': 40,
 'spider_exceptions/TypeError': 39,
 'start_time': datetime.datetime(2016, 11, 21, 6, 59, 29, 410827)}
2016-11-21 14:59:33 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:07:40 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:07:40 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:07:40 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:07:41 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:07:41 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:07:41 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:07:41 [scrapy] INFO: Spider opened
2016-11-21 15:07:41 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:07:49 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:07:49 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 111348,
 'downloader/request_count': 375,
 'downloader/request_method_count/GET': 375,
 'downloader/response_bytes': 3278690,
 'downloader/response_count': 375,
 'downloader/response_status_count/200': 375,
 'dupefilter/filtered': 25,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 7, 49, 692374),
 'item_scraped_count': 365,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 375,
 'scheduler/dequeued': 375,
 'scheduler/dequeued/memory': 375,
 'scheduler/enqueued': 375,
 'scheduler/enqueued/memory': 375,
 'start_time': datetime.datetime(2016, 11, 21, 7, 7, 41, 32587)}
2016-11-21 15:07:49 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:10:12 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:10:12 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:10:12 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:10:12 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:10:12 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:10:12 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:10:12 [scrapy] INFO: Spider opened
2016-11-21 15:10:12 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:10:29 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:10:29 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 119450,
 'downloader/request_count': 397,
 'downloader/request_method_count/GET': 397,
 'downloader/response_bytes': 3230831,
 'downloader/response_count': 397,
 'downloader/response_status_count/200': 397,
 'dupefilter/filtered': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 10, 29, 874537),
 'item_scraped_count': 387,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 397,
 'scheduler/dequeued': 397,
 'scheduler/dequeued/memory': 397,
 'scheduler/enqueued': 397,
 'scheduler/enqueued/memory': 397,
 'start_time': datetime.datetime(2016, 11, 21, 7, 10, 12, 962586)}
2016-11-21 15:10:29 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:15:03 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:15:03 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:15:03 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:15:03 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:15:03 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:15:03 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:15:03 [scrapy] INFO: Spider opened
2016-11-21 15:15:03 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:15:13 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/gszt/20150325_61642/> (referer: http://www.cs.com.cn/ssgs/hyzx/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 60, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:15:20 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 60, in parse_item
    item['content'] = response.xpath('//div[@class="Dtext z_content"]').extract_first()[:15]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:15:22 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:15:22 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 322297,
 'downloader/request_count': 1072,
 'downloader/request_method_count/GET': 1072,
 'downloader/response_bytes': 9704106,
 'downloader/response_count': 1072,
 'downloader/response_status_count/200': 1072,
 'dupefilter/filtered': 128,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 15, 22, 445991),
 'item_scraped_count': 1040,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1072,
 'scheduler/dequeued': 1072,
 'scheduler/dequeued/memory': 1072,
 'scheduler/enqueued': 1072,
 'scheduler/enqueued/memory': 1072,
 'spider_exceptions/TypeError': 2,
 'start_time': datetime.datetime(2016, 11, 21, 7, 15, 3, 590950)}
2016-11-21 15:15:22 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:15:23 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:15:23 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:15:23 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:15:23 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:15:23 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:15:23 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:15:23 [scrapy] INFO: Spider opened
2016-11-21 15:15:23 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:15:35 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:15:35 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 210316,
 'downloader/request_count': 704,
 'downloader/request_method_count/GET': 704,
 'downloader/response_bytes': 5945417,
 'downloader/response_count': 704,
 'downloader/response_status_count/200': 704,
 'dupefilter/filtered': 96,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 15, 35, 194409),
 'item_scraped_count': 684,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 704,
 'scheduler/dequeued': 704,
 'scheduler/dequeued/memory': 704,
 'scheduler/enqueued': 704,
 'scheduler/enqueued/memory': 704,
 'start_time': datetime.datetime(2016, 11, 21, 7, 15, 23, 577881)}
2016-11-21 15:15:35 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:19:35 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:19:35 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:19:35 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:19:35 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:19:35 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:19:35 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:19:35 [scrapy] INFO: Spider opened
2016-11-21 15:19:35 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:20:35 [scrapy] INFO: Crawled 2564 pages (at 2564 pages/min), scraped 2430 items (at 2430 items/min)
2016-11-21 15:21:35 [scrapy] INFO: Crawled 5143 pages (at 2579 pages/min), scraped 4899 items (at 2469 items/min)
2016-11-21 15:22:35 [scrapy] INFO: Crawled 7912 pages (at 2769 pages/min), scraped 7602 items (at 2703 items/min)
2016-11-21 15:23:35 [scrapy] INFO: Crawled 10194 pages (at 2282 pages/min), scraped 9786 items (at 2184 items/min)
2016-11-21 15:24:02 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:24:02 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3620421,
 'downloader/request_count': 11402,
 'downloader/request_method_count/GET': 11402,
 'downloader/response_bytes': 103095965,
 'downloader/response_count': 11402,
 'downloader/response_status_count/200': 11388,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/404': 13,
 'dupefilter/filtered': 911,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 24, 2, 565757),
 'item_scraped_count': 10990,
 'log_count/INFO': 11,
 'request_depth_max': 203,
 'response_received_count': 11399,
 'scheduler/dequeued': 11402,
 'scheduler/dequeued/memory': 11402,
 'scheduler/enqueued': 11402,
 'scheduler/enqueued/memory': 11402,
 'start_time': datetime.datetime(2016, 11, 21, 7, 19, 35, 494930)}
2016-11-21 15:24:02 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:48:51 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:48:51 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:48:51 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:48:51 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:48:51 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:48:51 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:48:51 [scrapy] INFO: Spider opened
2016-11-21 15:48:51 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:49:22 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:49:22 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 233874,
 'downloader/request_count': 820,
 'downloader/request_method_count/GET': 820,
 'downloader/response_bytes': 11835371,
 'downloader/response_count': 820,
 'downloader/response_status_count/200': 820,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 49, 22, 580942),
 'item_scraped_count': 800,
 'log_count/INFO': 7,
 'request_depth_max': 20,
 'response_received_count': 820,
 'scheduler/dequeued': 820,
 'scheduler/dequeued/memory': 820,
 'scheduler/enqueued': 820,
 'scheduler/enqueued/memory': 820,
 'start_time': datetime.datetime(2016, 11, 21, 7, 48, 51, 946011)}
2016-11-21 15:49:22 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:55:07 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:55:07 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:55:07 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:55:07 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:55:07 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:55:07 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:55:07 [scrapy] INFO: Spider opened
2016-11-21 15:55:07 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:55:17 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/gszt/20150325_61642/04/201611/t20161111_5092519.html> (referer: http://www.cs.com.cn/ssgs/hyzx/index_4.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 60, in parse_item
    item['time'] = response.xpath('//span[@class="ctime01"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:55:17 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/gszt/20150325_61642/> (referer: http://www.cs.com.cn/ssgs/hyzx/index_3.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 60, in parse_item
    item['time'] = response.xpath('//span[@class="ctime01"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:55:18 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/404error/index.html> (referer: http://www.cs.com.cn/ssgs/hyzx/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 60, in parse_item
    item['time'] = response.xpath('//span[@class="ctime01"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:55:23 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/ssgs/wysg/> (referer: http://www.cs.com.cn/ssgs/gsxw/)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 61, in parse_item
    item['time'] = datetime.datetime.strptime(item['time'], '%Y-%m-%d')
  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/_strptime.py", line 325, in _strptime
    (data_string, format))
ValueError: time data '(11-21 09:' does not match format '%Y-%m-%d'
2016-11-21 15:55:34 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:55:34 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 326823,
 'downloader/request_count': 1087,
 'downloader/request_method_count/GET': 1087,
 'downloader/response_bytes': 9729499,
 'downloader/response_count': 1087,
 'downloader/response_status_count/200': 1085,
 'downloader/response_status_count/302': 2,
 'dupefilter/filtered': 115,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 55, 34, 523021),
 'item_scraped_count': 1051,
 'log_count/ERROR': 4,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 1085,
 'scheduler/dequeued': 1087,
 'scheduler/dequeued/memory': 1087,
 'scheduler/enqueued': 1087,
 'scheduler/enqueued/memory': 1087,
 'spider_exceptions/TypeError': 3,
 'spider_exceptions/ValueError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 7, 55, 7, 153371)}
2016-11-21 15:55:34 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:56:26 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:56:26 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:56:26 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:56:26 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:56:26 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:56:26 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:56:26 [scrapy] INFO: Spider opened
2016-11-21 15:56:26 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:56:32 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/tzjj/00/201611/t20161109_5090199.html> (referer: http://www.cs.com.cn/xwzx/hwxx/index_7.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 106, in parse_item
    item['time'] = response.xpath('//span[@class="ctime01"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:56:41 [scrapy] ERROR: Spider error processing <GET http://www.cs.com.cn/xwzx/xwzt/151215_61693/01/201611/t20161117_5097402.html> (referer: http://www.cs.com.cn/xwzx/hg/index_1.html)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/twisted/internet/defer.py", line 649, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/yuweifu/news/news/spiders/cs.py", line 106, in parse_item
    item['time'] = response.xpath('//span[@class="ctime01"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:56:47 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:56:47 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 225318,
 'downloader/request_count': 754,
 'downloader/request_method_count/GET': 754,
 'downloader/response_bytes': 6484418,
 'downloader/response_count': 754,
 'downloader/response_status_count/200': 754,
 'dupefilter/filtered': 46,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 56, 47, 333302),
 'item_scraped_count': 732,
 'log_count/ERROR': 2,
 'log_count/INFO': 7,
 'request_depth_max': 1,
 'response_received_count': 754,
 'scheduler/dequeued': 754,
 'scheduler/dequeued/memory': 754,
 'scheduler/enqueued': 754,
 'scheduler/enqueued/memory': 754,
 'spider_exceptions/TypeError': 2,
 'start_time': datetime.datetime(2016, 11, 21, 7, 56, 26, 725685)}
2016-11-21 15:56:47 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:57:47 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:57:47 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:57:47 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:57:47 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:57:47 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:57:47 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:57:47 [scrapy] INFO: Spider opened
2016-11-21 15:57:47 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:58:22 [scrapy] INFO: Closing spider (finished)
2016-11-21 15:58:22 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 233874,
 'downloader/request_count': 820,
 'downloader/request_method_count/GET': 820,
 'downloader/response_bytes': 11835371,
 'downloader/response_count': 820,
 'downloader/response_status_count/200': 820,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 7, 58, 22, 71747),
 'item_scraped_count': 800,
 'log_count/INFO': 7,
 'request_depth_max': 20,
 'response_received_count': 820,
 'scheduler/dequeued': 820,
 'scheduler/dequeued/memory': 820,
 'scheduler/enqueued': 820,
 'scheduler/enqueued/memory': 820,
 'start_time': datetime.datetime(2016, 11, 21, 7, 57, 47, 579749)}
2016-11-21 15:58:22 [scrapy] INFO: Spider closed (finished)
2016-11-21 15:59:28 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 15:59:28 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 15:59:28 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 15:59:28 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 15:59:28 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 15:59:28 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 15:59:28 [scrapy] INFO: Spider opened
2016-11-21 15:59:28 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 15:59:47 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/company/scp_ggjd/gszhuanti/2015bnb/> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/25)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 15:59:49 [scrapy] ERROR: Spider error processing <GET http://yjbg.cnstock.com/dyjm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/24)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:13 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3283082.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/75)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:13 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278178.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:13 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3280014.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:13 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3280033.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:13 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3281519.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:14 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3276450.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:14 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3276453.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:14 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3273173.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:14 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3274832.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:16 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201411/3258128.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/83)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:21 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201411/3246942.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/87)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:22 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3274684.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:22 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278159.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:22 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278232.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:00:28 [scrapy] INFO: Crawled 2187 pages (at 2187 pages/min), scraped 2063 items (at 2063 items/min)
2016-11-21 16:00:28 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3281562.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:01:02 [scrapy] ERROR: Spider error processing <GET http://yjbg.cnstock.com/> (referer: http://company.cnstock.com/company/scp_gsxw/58)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:01:14 [scrapy] ERROR: Spider error processing <GET http://www.cnstock.com> (referer: http://company.cnstock.com/company/scp_gsxw/65)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:01:28 [scrapy] INFO: Crawled 4407 pages (at 2220 pages/min), scraped 4199 items (at 2136 items/min)
2016-11-21 16:02:21 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/topics/xzqmsdwk15> (referer: http://company.cnstock.com/company/scp_gsxw/176)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:02:23 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/topics/xzqmsdwk15/201512/3663412.htm> (referer: http://company.cnstock.com/company/scp_gsxw/179)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:02:28 [scrapy] INFO: Crawled 6828 pages (at 2421 pages/min), scraped 6514 items (at 2315 items/min)
2016-11-21 16:02:37 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/company/scp_ggjd/gszhuanti/20153jb> (referer: http://company.cnstock.com/company/scp_gsxw/192)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:03:28 [scrapy] INFO: Crawled 9512 pages (at 2684 pages/min), scraped 9122 items (at 2608 items/min)
2016-11-21 16:03:42 [scrapy] INFO: Closing spider (finished)
2016-11-21 16:03:42 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 3198741,
 'downloader/request_count': 10070,
 'downloader/request_method_count/GET': 10070,
 'downloader/response_bytes': 92432604,
 'downloader/response_count': 10070,
 'downloader/response_status_count/200': 10056,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/404': 13,
 'dupefilter/filtered': 911,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 8, 3, 42, 911948),
 'item_scraped_count': 9678,
 'log_count/ERROR': 23,
 'log_count/INFO': 11,
 'request_depth_max': 203,
 'response_received_count': 10067,
 'scheduler/dequeued': 10070,
 'scheduler/dequeued/memory': 10070,
 'scheduler/enqueued': 10070,
 'scheduler/enqueued/memory': 10070,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 22,
 'start_time': datetime.datetime(2016, 11, 21, 7, 59, 28, 327326)}
2016-11-21 16:03:42 [scrapy] INFO: Spider closed (finished)
2016-11-21 16:05:42 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 16:05:42 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 16:05:42 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 16:05:42 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 16:05:42 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 16:05:42 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 16:05:42 [scrapy] INFO: Spider opened
2016-11-21 16:05:42 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 16:05:42 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/gglist/search/qmtbbdj/> (referer: None)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 27, in parse
    if words[-2] == u'下一页':
IndexError: list index out of range
2016-11-21 16:05:55 [scrapy] ERROR: Spider error processing <GET http://yjbg.cnstock.com/dyjm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/24)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:05:56 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/company/scp_ggjd/gszhuanti/2015bnb/> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/25)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3283082.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/75)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278178.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3280014.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3281519.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3280033.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:24 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3276450.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:25 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3276453.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:25 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3273173.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:25 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3274832.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:28 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201411/3258128.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/83)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item['time'] = response.xpath('//span[@class="timer"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:32 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201411/3246942.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/87)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:33 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3274684.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/78)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:34 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278159.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:34 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3278232.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/77)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:34 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/gszj/rdgs/201412/3281562.htm> (referer: http://company.cnstock.com/company/scp_dsy/tcsy_rdgs/76)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:42 [scrapy] INFO: Crawled 2695 pages (at 2695 pages/min), scraped 2566 items (at 2566 items/min)
2016-11-21 16:06:46 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000622898> (referer: http://company.cnstock.com/company/scp_gsxw/15)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:06:56 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000604000> (referer: http://company.cnstock.com/company/scp_gsxw/37)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:08 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000590883> (referer: http://company.cnstock.com/company/scp_gsxw/57)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:08 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000591138> (referer: http://company.cnstock.com/company/scp_gsxw/57)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:08 [scrapy] ERROR: Spider error processing <GET http://yjbg.cnstock.com/> (referer: http://company.cnstock.com/company/scp_gsxw/58)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:08 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000589597> (referer: http://company.cnstock.com/company/scp_gsxw/59)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:11 [scrapy] ERROR: Spider error processing <GET http://www.cnstock.com> (referer: http://company.cnstock.com/company/scp_gsxw/65)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:07:42 [scrapy] INFO: Crawled 5271 pages (at 2576 pages/min), scraped 5037 items (at 2471 items/min)
2016-11-21 16:08:20 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/topics/xzqmsdwk15> (referer: http://company.cnstock.com/company/scp_gsxw/176)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:08:21 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/topics/xzqmsdwk15/201512/3663412.htm> (referer: http://company.cnstock.com/company/scp_gsxw/179)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:08:40 [scrapy] ERROR: Spider error processing <GET http://company.cnstock.com/company/scp_ggjd/gszhuanti/20153jb> (referer: http://company.cnstock.com/company/scp_gsxw/192)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:08:42 [scrapy] INFO: Crawled 7522 pages (at 2251 pages/min), scraped 7192 items (at 2155 items/min)
2016-11-21 16:08:54 [scrapy] ERROR: Spider error processing <GET http://ggjd.cnstock.com/ggdetail/index/2000625404> (referer: http://company.cnstock.com/company/scp_gsxw/11)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/cnstock.py", line 34, in parse_item
    item = NewsItem()
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:08:56 [scrapy] INFO: Closing spider (finished)
2016-11-21 16:08:56 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 2610660,
 'downloader/request_count': 8210,
 'downloader/request_method_count/GET': 8210,
 'downloader/response_bytes': 77345540,
 'downloader/response_count': 8210,
 'downloader/response_status_count/200': 8196,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/404': 13,
 'dupefilter/filtered': 880,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 8, 8, 56, 583671),
 'item_scraped_count': 7873,
 'log_count/ERROR': 29,
 'log_count/INFO': 10,
 'request_depth_max': 203,
 'response_received_count': 8207,
 'scheduler/dequeued': 8210,
 'scheduler/dequeued/memory': 8210,
 'scheduler/enqueued': 8210,
 'scheduler/enqueued/memory': 8210,
 'spider_exceptions/IndexError': 1,
 'spider_exceptions/TypeError': 28,
 'start_time': datetime.datetime(2016, 11, 21, 8, 5, 42, 606376)}
2016-11-21 16:08:56 [scrapy] INFO: Spider closed (finished)
2016-11-21 16:10:24 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 16:10:24 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 16:10:24 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 16:10:25 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 16:10:25 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 16:10:25 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 16:10:25 [scrapy] INFO: Spider opened
2016-11-21 16:10:25 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 16:11:25 [scrapy] INFO: Crawled 817 pages (at 817 pages/min), scraped 797 items (at 797 items/min)
2016-11-21 16:11:40 [scrapy] INFO: Closing spider (finished)
2016-11-21 16:11:40 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 233121,
 'downloader/request_count': 820,
 'downloader/request_method_count/GET': 820,
 'downloader/response_bytes': 12391561,
 'downloader/response_count': 820,
 'downloader/response_status_count/200': 820,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 8, 11, 40, 612674),
 'item_scraped_count': 800,
 'log_count/INFO': 8,
 'request_depth_max': 20,
 'response_received_count': 820,
 'scheduler/dequeued': 820,
 'scheduler/dequeued/memory': 820,
 'scheduler/enqueued': 820,
 'scheduler/enqueued/memory': 820,
 'start_time': datetime.datetime(2016, 11, 21, 8, 10, 25, 65034)}
2016-11-21 16:11:40 [scrapy] INFO: Spider closed (finished)
2016-11-21 16:12:28 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 16:12:28 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 16:12:28 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 16:12:28 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 16:12:28 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 16:12:28 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 16:12:28 [scrapy] INFO: Spider opened
2016-11-21 16:12:28 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 16:12:40 [scrapy] INFO: Closing spider (finished)
2016-11-21 16:12:40 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 233121,
 'downloader/request_count': 820,
 'downloader/request_method_count/GET': 820,
 'downloader/response_bytes': 12391705,
 'downloader/response_count': 820,
 'downloader/response_status_count/200': 820,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 8, 12, 40, 566836),
 'item_scraped_count': 800,
 'log_count/INFO': 7,
 'request_depth_max': 20,
 'response_received_count': 820,
 'scheduler/dequeued': 820,
 'scheduler/dequeued/memory': 820,
 'scheduler/enqueued': 820,
 'scheduler/enqueued/memory': 820,
 'start_time': datetime.datetime(2016, 11, 21, 8, 12, 28, 393099)}
2016-11-21 16:12:40 [scrapy] INFO: Spider closed (finished)
2016-11-21 16:13:07 [scrapy] INFO: Scrapy 1.2.1 started (bot: news)
2016-11-21 16:13:07 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'news.spiders', 'SPIDER_MODULES': ['news.spiders'], 'LOG_FILE': 'scrapy.log', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'news'}
2016-11-21 16:13:07 [scrapy] INFO: Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
2016-11-21 16:13:07 [scrapy] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-11-21 16:13:07 [scrapy] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-11-21 16:13:07 [scrapy] INFO: Enabled item pipelines:
['news.pipelines.MongoDBPipeline']
2016-11-21 16:13:07 [scrapy] INFO: Spider opened
2016-11-21 16:13:07 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-11-21 16:13:18 [scrapy] ERROR: Spider error processing <GET http://www.stcn.com> (referer: http://company.stcn.com/cjnews/13.shtml)
Traceback (most recent call last):
  File "/Library/Python/2.7/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/offsite.py", line 29, in process_spider_output
    for x in result:
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/referer.py", line 22, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Library/Python/2.7/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/yuweifu/news/news/spiders/stcn.py", line 35, in parse_item
    item['time'] = response.xpath('//div[@class="intal_tit"]/div[@class="info"]/text()').extract_first()[:10]
TypeError: 'NoneType' object has no attribute '__getitem__'
2016-11-21 16:13:55 [scrapy] INFO: Closing spider (finished)
2016-11-21 16:13:55 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 465188,
 'downloader/request_count': 1633,
 'downloader/request_method_count/GET': 1633,
 'downloader/response_bytes': 24054484,
 'downloader/response_count': 1633,
 'downloader/response_status_count/200': 1626,
 'downloader/response_status_count/302': 7,
 'dupefilter/filtered': 14,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2016, 11, 21, 8, 13, 55, 32257),
 'item_scraped_count': 1585,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'request_depth_max': 20,
 'response_received_count': 1626,
 'scheduler/dequeued': 1633,
 'scheduler/dequeued/memory': 1633,
 'scheduler/enqueued': 1633,
 'scheduler/enqueued/memory': 1633,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2016, 11, 21, 8, 13, 7, 332718)}
2016-11-21 16:13:55 [scrapy] INFO: Spider closed (finished)
